<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SSD筆記 - 第六篇 結論</title>
      <link href="/ssd-notes-06/"/>
      <url>/ssd-notes-06/</url>
      
        <content type="html"><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>第六篇，這篇就是把五篇的重點做個摘錄。</p><h2 id="基礎"><a href="#基礎" class="headerlink" title="基礎"></a>基礎</h2><ol><li>SSD（solid state drive) 是基於 flash NAND memory 製作的儲存裝置。資料（Bits)儲存在不同種類的 cell 裡，當時有 SLC, MLC, TLC，分別代表一個 cell 裡面可存 1, 2, 3 個 bit(s)，並有不同的讀寫時間、壽命等特性。</li><li>每個 Cell 有 P&#x2F;E (Program&#x2F;Erase) cycles 次數限制，超過了該 Cell 就不能用了。意味著 SSD 裝置會隨著使用過程損耗、有“可預期”的使用年限。</li><li>效能評定 (Benchmarking) 很難做。原廠及第三方的報告都要多看多比較，別輕易相信他們的數字。可以的話自己買了做一次效能測試。並確定你了解效能指標的意義，且該數據有達到你的需求。</li></ol><h2 id="Pages-and-blocks"><a href="#Pages-and-blocks" class="headerlink" title="Pages and blocks"></a>Pages and blocks</h2><ol><li><p>鄰近的cell會再組成可被讀寫的最小單位 page, nand-flash 的 page&#x2F;分頁 大小 2, 4, 8, 16 KB 不等。 鄰近的 page 則會組成 block，通常是 128, 256 個 page 為一 block，因而 block 大小有 256 KB 到 4MB 不等。如 Sxxsung SSD 840 block &#x3D; 2048 KB, 由 256 個 8 KB page 組成。</p></li><li><p>即便你只在作業系統讀了一個 byte，SSD 的低消還是要讀一個 page。</p></li><li><p>寫入&#x2F;write 一個 page 也稱為 program，上面提到的為寫一點資料要寫一堆的現象也稱為 write amplification &#x2F; 寫入放大。</p></li><li><p>page 不能直接被複寫。nand-flash 只有在進入 “free” state 才能被寫。在我們寫入一筆資料的時候我們需要先讀出現有內容到暫存器&#x2F;register，然後再寫到其他的 free 的 page 裡，原先的 page 會被進入 “stale” state，並等待被清理，這種操作模式稱為 “copy-modify-write”</p><blockquote><p>zfs 以及一些作業系統也有類似的術語 <a href="https://en.wikipedia.org/wiki/Copy-on-write">copy-on-write</a>，沒什麼相關就是了。</p></blockquote></li><li><p>erase 必須以 block 為單位 (Erases are aligned on block size):<br>page stale 之後必須要清除&#x2F;erase 才能回到 free 狀態</p></li></ol><h2 id="SSD-控制器與其原理"><a href="#SSD-控制器與其原理" class="headerlink" title="SSD 控制器與其原理"></a>SSD 控制器與其原理</h2><ol><li><p>FTL Flash Translation layer<br>FTL 是 SSD controller 工作之一，負責把 host interface 的 Logical Block Addresses (LBA) 轉 Physical Block Addresses (PBA)。最近很多 controller 實作 hybrid log-block mapping，讓隨機寫入的行為像是 log-structured file systems ，寫入行為像是 循序寫入 (sequential write)。</p></li><li><p>internel parallelism<br>controller 內有同時寫入許多 block 到不同的 nand-flash 晶片的機制，此寫入機制&#x2F;單位 clustered block。</p></li><li><p>Wear leveling<br>FTL 的一個功能是讓各個 block 的 P&#x2F;E cycle 接近，大家約在同個時間壞掉。</p></li><li><p>GC &#x2F; Garbage collection 處理垃圾<br>controller 的 GC 流程會把 stale page 清除，回到 free state, 以備下次資料寫入。</p></li><li><p>background&#x2F; 背景作業的 GC 會影響前台 (foreground) 的寫入效能</p></li></ol><h2 id="建議的-SSD-操作姿勢"><a href="#建議的-SSD-操作姿勢" class="headerlink" title="建議的 SSD 操作姿勢"></a>建議的 SSD 操作姿勢</h2><ol><li><p>避免多次寫入小於 page size 的資料。避免 read-modify-write, write amplification. page size 愈大愈好</p></li><li><p>align write, 盡量寫入以 page size 為單位的資料</p></li><li><p>為提升 throughput 盡量把小的寫入 cache 到記憶體，在適當實際一次批次寫入。</p><blockquote><p>這個應該是設計資料庫或是有極端效能考量的系統時的需求</p></blockquote></li><li><p>讀取效率跟寫入行為有關，當我們批次寫入資料時 SSD controller 會把資料平行寫入、四散在各個 nand flash chip 之間。寫入資料時將日後可能會一起讀取的資料排在一起寫會有助於讀取效能</p><blockquote><p>感覺有點難，所以規劃架構的時候用 VM 來區分各個應用程式，如資料庫、web server 分離可以較有效運用到這點。 你說 docker, k8s container? 可能也有吧… 我不太確定(TODO)</p></blockquote></li><li><p>讀寫分離<br>當我們在 SSD 上進行大量小的讀寫穿插 (interleaved) 的操作時會讓 controller 內有的 cache, readahead 機制失效，效能低落。例如如果你有 1000 個 檔案需要讀寫，一個個讀寫跟一次讀 1000 個完了以後再寫，後者效能較好。 </p><blockquote><p>zfs 也有 zil, l2arc 讀寫 cache 分離的機制。  “the L2ARC for random reads, and the ZIL for writes.” [2](#zfs cache-l2arc)</p></blockquote></li><li><p>當你要刪資料的時候最好是批次、一次性刪，好讓 controller GC 有更多空間可以操作，降低 SSD 內部資料碎片化 fragmentation。</p></li><li><p>隨機寫入不一定比循序寫入慢<br>寫入的檔案小的時候會慢，但檔案跟 clustered block 同大時可以利用到 ssd 內部的平行機制，效能跟 sequential weite 差不多好</p></li><li><p>單執行緒、一次讀很多資料的操作比同時跑很多 thread 的讀取操作更能利用到 readahead 的機制。因為有可能 LBA 剛好都在同個 flash chip，還是要排隊才能拿到資料。很多時候反而單執行緒讀取可以更好的運用到 readahead buffer</p></li><li><p>寫入情況同上面一條，single threaded large write is better</p></li><li><p>如果大量小的資料沒辦法批次或是快取寫入的操作，那還是用多執行緒來寫</p></li><li><p>冷熱分離<br>常改的資料（熱的）放在一起，因為 read-modify-write 特性的關係，冷資料會跟熱的混在一塊，wear leveling 也會一起做，盡可能分開兩類資料能讓 GC 更好做事。</p></li><li><p>熱資料、常改的 metadata 最好有做緩存(buffered) cache 在記憶體裡，並避免寫到 SSD 裡。</p></li></ol><h2 id="系統最佳化"><a href="#系統最佳化" class="headerlink" title="系統最佳化"></a>系統最佳化</h2><ol><li><p>PCI Express, 企業級的 SAS 比 SATA 效能好，host interface 先天限制。</p><blockquote><p>可是最近 HPE SAS <a href="ttps://blocksandfiles.com/2019/11/25/hpe-issues-firmware-fix-to-to-stop-ssd-failure/">爆了一次</a></p></blockquote></li><li><p>Over-provisioning 分割硬碟的時候別把空間全用完，例如留 10~15% 給 GC 運作空間可以提升使用壽命，controller 還是會把那個空間拿來做 wear leveling 等事。如果有更大量寫入需求可以考慮拉大到 25 %</p></li><li><p>開啟 trim 指令，作業系統核心、檔案系統可以通知 SSD controller 某個 block 沒在用，讓 controller 進行 GC 作業。</p></li><li><p>align the partition<br>確定硬碟格式化時確定分割區與 實體 page 的位置有對齊很重要 <a href="https://tytso.livejournal.com/2009/02/20/">ref</a></p></li></ol><h1 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h1><p>想了解更多的話，作者建議可以再去看 2-5 的參考資料。另外 FAST conference（USENIX conference on file and storage) 也可以看看，了解時事動態。</p><h1 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h1><h4 id="coding-for-ssd-part6"><a href="#coding-for-ssd-part6" class="headerlink" title="coding for ssd part6"></a>coding for ssd part6</h4><p><a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-6-a-summary-what-every-programmer-should-know-about-solid-state-drives/">http://codecapsule.com/2014/02/12/coding-for-ssds-part-6-a-summary-what-every-programmer-should-know-about-solid-state-drives/</a></p><h4 id="zfs-cache-l2arc"><a href="#zfs-cache-l2arc" class="headerlink" title="zfs cache-l2arc"></a>zfs cache-l2arc</h4><p><a href="http://www.brendangregg.com/blog/2008-07-22/zfs-l2arc.html">http://www.brendangregg.com/blog/2008-07-22/zfs-l2arc.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD筆記 - 第五篇 access pattern, 系統配置</title>
      <link href="/ssd-notes-05/"/>
      <url>/ssd-notes-05/</url>
      
        <content type="html"><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>作者在介紹了 SSD 內部運作原理後，為何同時 (concurrent) 進行的讀寫行為會互相干涉，並介紹如何更好的 SSD 讀寫手法。此篇也涵蓋了一部分可改善效能的檔案系統最佳化手段。</p><h1 id="7-Access-Patterns"><a href="#7-Access-Patterns" class="headerlink" title="7 Access Patterns"></a>7 Access Patterns</h1><h2 id="7-1-定義循序及隨機-IO-操作"><a href="#7-1-定義循序及隨機-IO-操作" class="headerlink" title="7.1 定義循序及隨機 IO 操作"></a>7.1 定義循序及隨機 IO 操作</h2><p>Sequential&#x2F;循序：一個 IO 操作的 LBA &#x2F; Logical block address 開頭接著上一個操作 LBA 的結尾。除此之外皆視為隨機。<br>值得注意的是即便我們 LBA 是連續的，經過 FTL 之後實際存在 physical block 的資料還是可能會四散各處。</p><h2 id="7-2-寫"><a href="#7-2-寫" class="headerlink" title="7.2 寫"></a>7.2 寫</h2><p>效能評定報告及廠商規格通常會顯示循序寫入速度慢於隨機寫入。<br>但作者這類資料是用小於 clustered block size 的資料量（&lt; 32 MB)的測試，沒用到平行機制。如果大於還剛好是倍數，寫入的效能是可以比擬的。作者的解釋是 parallelism 跟 interleaving 同時上場，也就是寫入一個 clustered block 可以保證 SSD 完全用上了設計好的機制。下兩圖分別是從 <a href="#ref">[2, 8]</a> 擷取出來，隨機寫入效率跟循序寫入在寫入資料大小跟 clustered block 差不多大的時候是差不多的（大約是 16&#x2F;32 MB)。<br><img src="http://codecapsule.com/wp-content/uploads/2014/01/writes-random-01.jpg" alt="隨機跟循序寫入比較"><br><img src="http://codecapsule.com/wp-content/uploads/2014/01/writes-random-02.jpg"></p><p>然而當隨機寫入小於 page size 如 16 KB 的資料時，除了寫入放大，SSD 工作量也會變多，例如必須要把每一筆 LBA 對應的 PBA mapping 都記錄下來，而許多 FTL 用類似 tree 之類的資料結構來存，很多的小資料寫入會變成對 FTL RAM 的大量更新操作，而且這個 mapping table 必須在斷電後留存 &#x2F; persisted ，結果就是同時帶來對 nand block 的大量寫入 <a href="#ref">[1, 5]</a>。 循序寫入可以降低此類更新 metadata 的情形發生，減少對 flash 的寫入。</p><p>隨機大量小資還會造成大量 copy-erase-write 的現象發生。相較 循序寫入至少一個 block 大小的資料的情況，switch merge 等 happy path 更容易在後者發生。隨機大量小資還會讓 stale page 四散在各個 block，而不是集中在某個區域。這現象也稱為 internel fragmentation，造成 cleaning efficiency &#x2F; 清掃效率降低， GC 需要搬動更多次資料來清除一樣大的空間。</p><p>至於應用層關心的 concurrency ， 單執行緒的大筆資料寫入與多執行緒的同步多筆寫入差不多快，而且前者較能利用到 SSD 的平行處理機制，因此費工夫寫多執行緒的寫入並不會對 IO 的效能有幫助<a href="#ref">[1,5]</a>，反而有害<a href="#ref">[3, 26, 27]</a>。</p><blockquote><p>但是作者最後備註還是寫了如果你沒辦法把隨機大量小資做緩存批次寫入，還是用多執行緒會比較快。</p></blockquote><h2 id="7-3-讀"><a href="#7-3-讀" class="headerlink" title="7.3 讀"></a>7.3 讀</h2><p>總的來說，讀比寫快。但循序跟隨機讀取孰快孰慢，不一定。FTL 在寫入時動態的&#x2F; dynamically 將 LBA 寫到 PBA 去，其中更涉及上述的平行機制，資料切塊寫到各個 channel&#x2F;package 去，這個寫入模式也稱為 “write-order-based” <a href="#ref">[3]</a>。如果讀取的順序完全隨機，跟寫入模式無相關，則讀取時先前以平行機制寫入的資料不保證在讀取時有用。很多時候即便 LBA 是連續的，讀取效能也不一定好，甚至連續的 LBA 讀取被 mapping 到同一個 channel 上，還是要排隊等資料。作者提到 Acunu <a href="#ref">[47]</a> 有篇 blog 測試發現讀取資料的模式與寫入的模式有直接關聯。</p><blockquote><p>[47] 的 Acunu 網站已經掛了。[TODO]: 找替代方案</p></blockquote><p>讀取效能與寫入模式息息相關，作者建議相關聯的資料最好寫在同個 page &#x2F; block &#x2F; clustered block 裡，確保寫入時用到平行機制，相關聯資料放一起也較符合日後讀取需求與效能提升條件。</p><p>下圖是 2 channels, 4 chips, 1 plane&#x2F;chip 的參考配置圖。注意通常一個 chip 裡面不只有一個 plane，作者做了些簡化以便說明。大寫的英文字分別代表一筆 NAND-flash block 大小的資料。這裡我們寫入四筆連續的 LBA 資料 <code>[A, B, C, D]</code>，剛好也是 clustered block 的大小。利用 clustered block 平行機制(parallelism and interleaving)這四筆資料會被分開寫到四個 plane 去。即便他們在 logical address 是連續的，為了效能考量他們會被分到不同的 physical plane。</p><p>write-order-based FTL 在選擇寫入 clustered block 的時候不會要求在各 plane 的 PBN 要相同，所以圖例可以看到 結果寫到了 1, 23, 11, 51 這四個位置去。</p><blockquote><p>我不太確定作者提這個用意為何，先前他也沒有介紹 plane 的設計細節 XD</p></blockquote><p>當我們讀取 <code>[A, B, E, F]</code>, <code>[A, B, G, H]</code> 的時候，前者因為部分資料在同個 plane 裡，需要讀兩次，後者則可利用到平行機制加快讀取。</p><p><img src="http://codecapsule.com/wp-content/uploads/2014/02/ssd-exploiting-parallelism.jpg"></p><p>這會直接影響到內部平行機制對應用層讀取資料。因為資料可能剛好在同個 physical channel ，當用多執行緒進行讀取不一定能帶來效能提升。另外在 <a href="#ref">[3]</a> 也指出多執行緒的讀取會干擾 readahead (prefetchiing buffer) 的運行。</p><blockquote><p>類似 FTL 會先猜你接下來要讀的資料，先抓好放著。</p></blockquote><p>雖然 SSD 廠商通常不公開 page&#x2F;block&#x2F;clustered block 大小，但是透過基本的測試工具可以抓出個大概。<a href="#ref">[2, 3]</a>這些資訊可以用來作為最佳化讀&#x2F;寫暫存區的大小，並當作分割硬碟的參考依據。</p><h2 id="7-4-同時-concurrent-讀寫"><a href="#7-4-同時-concurrent-讀寫" class="headerlink" title="7.4 同時&#x2F;concurrent 讀寫"></a>7.4 同時&#x2F;concurrent 讀寫</h2><p><a href="#ref">[1, 3]</a> 提到交錯讀寫對效能的負面影響，主要是因為讀寫操作同時進行會競爭資源、妨礙 SSD 內部快取、readahead 的運作。<br>因此作者建議將讀寫活動分開，如果你有 1000 個檔案需要頻繁讀寫，建議一次讀完再一次寫入，而不是讀了又寫讀了又寫讀了又寫…</p><h1 id="8-系統最佳化"><a href="#8-系統最佳化" class="headerlink" title="8. 系統最佳化"></a>8. 系統最佳化</h1><h2 id="8-1-Partition-alignment"><a href="#8-1-Partition-alignment" class="headerlink" title="8.1 Partition alignment"></a>8.1 Partition alignment</h2><p>3.1 提到當除了寫入資料大小是 page 大小倍數之外，寫入位置也要對，否則還是會佔了兩個 physical page。<a href="#ref">[53]</a><br><img src="http://blog.nuclex-games.com/wp-content/uploads/2009/12/ssd-unaligned-write.png" alt="from [53]"><br>因此了解 SSD 的 NAND page 大小是很重要滴，想知道如何正確的分割硬碟，可以參考 <a href="#ref">[54,55]</a></p><blockquote><p>[54] 壞了<br>Google 搜尋也可以找到 SSD 型號的相關資料，即便找不到你也可以試著用逆向工程的做法來隔空抓藥<a href="#ref">[2,3]</a>。</p></blockquote><p>[[43]] 的結果顯示正確的分割磁區對效能有幫助。另外 <a href="#ref">[44]</a> 也指出跳過&#x2F;by-passing 檔案系統，直接對硬碟下指令對效能有些微幫助。</p><blockquote></blockquote><h2 id="8-2-檔案系統參數"><a href="#8-2-檔案系統參數" class="headerlink" title="8.2 檔案系統參數"></a>8.2 檔案系統參數</h2><p>5.1 及 <a href="#ref">[16]</a> 提到的 TRIM 需要從 <code>discard</code> 指令開啟。除此之外拿掉 <code>relatime</code>, 加入 <code>noatime, nodiratime</code> 可能也有幫助。 <a href="#ref">[40, 55, 56, 57]</a></p><h2 id="8-3-Operating-system-I-O-scheduler"><a href="#8-3-Operating-system-I-O-scheduler" class="headerlink" title="8.3 Operating system I&#x2F;O scheduler"></a>8.3 Operating system I&#x2F;O scheduler</h2><p>CFQ scheduler (Completely Fair Queuing) 是 linux 預設的 scheduler，他會把 LBA 相近的 IO 放在一起執行，降低 seek 操作的延遲。這種安排對沒有那些會動機構的 SSD 來說並非必要。<a href="#ref">[56, 58]</a> 以及其他許多的擁護者都建議從 CFQ 換成 NOOP 排程。但從 linux kernel 3.1 開始 CFQ 也有對 SSD 的一些最佳化 <a href="#ref">[59]</a>，另外許多效能評定也指出排程器&#x2F;scheduler 的效能與搭配的應用層負載及硬碟本身都有關係 <a href="#ref">[40, 60, 61, 62]</a>。<br>作者認為除非你的應用層模式固定、並且更改 scheduler 確定有幫助，否則建議還是用預設的 CFQ。</p><h2 id="8-4-Swap"><a href="#8-4-Swap" class="headerlink" title="8.4 Swap"></a>8.4 Swap</h2><p>swap 把虛擬記憶體 page 寫入硬碟時會帶來大量的 IO 請求，會大幅降低 SSD 壽命。 linux kernel 有個 <code>vm.swappiness</code> 可以設定寫入 swap 的頻率 0-100 由少到多。Ubuntu 的預設是 60，建議設 0 來避免不必要的 swap，提升 SSD 使用年限。另外也有人建議設成 1 ，作者認為基本上是一樣的。<a href="#ref">[56, 63, 57, 58]</a><br>另外也可以用 RAM disk 來做 swap，或是就別用 swap 了。</p><blockquote><p>有點不太懂拿 ramdisk 來做 swap 的意義…</p></blockquote><h2 id="8-5-Temporary-files"><a href="#8-5-Temporary-files" class="headerlink" title="8.5 Temporary files"></a>8.5 Temporary files</h2><p>暫存檔不需要被保存下來，寫到 SSD 去是浪費 P&#x2F;E cycle 建議可以用 tmpfs，保存在記憶體即可。 <a href="#ref">[56, 57, 58]</a></p><h1 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h1><h4 id="coding-for-ssd-part-5"><a href="#coding-for-ssd-part-5" class="headerlink" title="coding for ssd part 5"></a>coding for ssd part 5</h4><p><a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-5-access-patterns-and-system-optimizations/">http://codecapsule.com/2014/02/12/coding-for-ssds-part-5-access-patterns-and-system-optimizations/</a></p><p>其他有編號參考資料請至原文觀賞：<a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-3-pages-blocks-and-the-flash-translation-layer/#ref">link</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD筆記 - 第四篇 FTL 其他功能及平行機制</title>
      <link href="/ssd-notes-04/"/>
      <url>/ssd-notes-04/</url>
      
        <content type="html"><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>在了解 FTL 之後，這裡將對 TRIM, over-provisioning 作介紹，並探討 clustered block 以及 SSD 不同層級的平行機制。</p><h1 id="5-Advanced-functionalities"><a href="#5-Advanced-functionalities" class="headerlink" title="5 Advanced functionalities"></a>5 Advanced functionalities</h1><h2 id="5-1-TRIM"><a href="#5-1-TRIM" class="headerlink" title="5.1 TRIM"></a>5.1 TRIM</h2><p>依照 HDD 的慣例，檔案系統刪除資料時不一定要真的下抹除指令到硬碟去（真的要刪的時候只要直接複寫過去就好了）。造成可能有檔案系統回報硬碟是空的、裡面塞滿實質 stale 的資料但 controller 不知情的情況。這會造成 controller 沒法有效 GC，到了發現要複寫了才開始清出空間，最後導致效能低落。</p><p>另外一個問題是，controller 快樂的把那些 controller 應該知道要刪除的資料搬來搬去做 wear leveling，但是這些都是做白工，而且干擾了 foreground 的讀寫工作。</p><blockquote><p>有沒有跟職場環境有點像？</p></blockquote><p>對這個問題的一個解法是 TRIM 指令，由作業系統送出，告知 SSD controller 某些 page 已經被刪掉了，沒有留存在 logical space 的必要。有了這個資訊 SSD 就不用把那些 page 搬來搬去，並適時刪除。這個指令必須要在 SSD controller, 作業系統, 檔案系統都有支援的情況下才有用。</p><p>維基百科的 TRIM 頁面有列出支援的作業系統及檔案系統<a href="#ref">[16]</a>，</p><blockquote><p>關心 zfs 的人，freeBSD 9.2 及近期的 <a href="https://www.phoronix.com/scan.php?page=news_item&px=ZFS-On-Linux-TRIM-Lands">zfsOnLinux 8.0</a> 都有支援 TRIM，愈來愈適合裝在筆電上啦。</p></blockquote><p>5.2 Over-provisioning<br>透過提供更多備用的 physical block 來讓 SSD gc 更好做事、提升壽命。大部分的 SSD 都有將 7 ~ 25% 的空間做 over-provisioning<a href="#ref">[13]</a>。使用者也可以加碼在分割硬碟的時候留更多空間，例如 100 GB 的硬碟，切了 90 GB 來用，其他擺著，controller 一樣會把那些空間拿來做 GC 等用途。</p><p>AnandTech 的一篇關於 over-provisioning 的文章，建議除了製造商原有的之外可以做到 25% 來達到更好的 SSD 存取效能<a href="#ref">[34]</a>。另外一篇 Percona 的文章指出 Intel 320 SSD 在將滿時寫入效能低落的現象<a href="#ref">[38]</a>。</p><p>作者對這現象的解釋是如果 SSD controller 始終保持在忙碌狀態，就會找不到適當實際進行 GC，清出 free state 的 block，直到 free block 用完了才不得不做。在這時候 FTL 已經無法像先前那樣有效率的完成 foreground 讀寫操作，必須等 GC 清出空間才能做，這導致嚴重的效能下降。 over-provisioning 可以協助減緩此類現象的發生，讓 FTL 有更多的空間支應大量的寫入操作。至於需要多大的空間來做，作者建議如果需要因應尖峰時段大量隨機寫入，上看25%，不需要的話 10 ~ 15%即可。</p><h1 id="5-3-Secure-Erase"><a href="#5-3-Secure-Erase" class="headerlink" title="5.3 Secure Erase"></a>5.3 Secure Erase</h1><p>有部分型號提供 ATA Secure Erase 功能可以讓 SSD 所有 block 清為 free，清空各 FTL mapping table。這可以解決資訊安全及使 SSD 效能恢復至出廠狀態。不過 <a href="#ref">[11]</a> 提到很多大部分廠商的實作都有問題。 Stackoverflow 上面有對於資訊安全議題的相關討論，也可以看到如何更有效的把資料確實從 SSD 上抹除，也有一篇 <a href="https://www.usenix.org/legacy/events/fast11/tech/full_papers/Wei.pdf">paper</a> 在討論這件事，，原則上就是挑選有支援加密的型號，或是你直接用有加密的檔案系統。 <a href="#ref">[48, 49]</a></p><blockquote><p>還有把硬碟丟到調理機裡面</p></blockquote><h1 id="5-4-Native-Command-Queueing-NCQ"><a href="#5-4-Native-Command-Queueing-NCQ" class="headerlink" title="5.4 Native Command Queueing (NCQ)"></a>5.4 Native Command Queueing (NCQ)</h1><p>SATA 讓 SSD 可以批次接受多個指令，利用內部平行處理機制的功能<a href="#ref">[3]</a>。除了降低延遲之外，部分 controller 也提供此機制讓 host CPU 可以批次下指令，當 CPU 工作量大的時候有幫助 <a href="#ref">[39]</a></p><h2 id="5-5-斷電保護"><a href="#5-5-斷電保護" class="headerlink" title="5.5 斷電保護"></a>5.5 斷電保護</h2><p>部分實作利用 supercapacitor 來保持 SSD 在斷電之後仍有足夠能量完成 host bus 的指令。不過作者指出這個跟 Secure Erase 一樣，各家實作不同，也沒有統一規範。</p><p><a href="#ref">[72]</a> Zheng et al., 2013 在斷電壓力測試中測了 15 款 SSD，沒透露廠家，但掉資料、系統損毀的比例 13&#x2F;15。另外一位 Luke Kenneth Casson Leighton 也拿了四款 SSD 來做測試，只有 Intel 沒掉資料 <a href="#ref">[73]</a>。</p><blockquote><p>如果是資訊機房的話還是要牢記備份 321 原則，還有上 UPS 跟自動關機機制。</p></blockquote><h1 id="6-SSD-內部平行處理機制"><a href="#6-SSD-內部平行處理機制" class="headerlink" title="6. SSD 內部平行處理機制"></a>6. SSD 內部平行處理機制</h1><h2 id="6-1-有限的-IO-頻寬"><a href="#6-1-有限的-IO-頻寬" class="headerlink" title="6.1 有限的 IO 頻寬"></a>6.1 有限的 IO 頻寬</h2><p>因 nand flash 物理限制，單一 package 的 io 頻寬極限是在 32-40 MB <a href="#ref">[5]</a>。因此能提升存取效能的方法就是 parallelized&#x2F;平行化 或是 interleaved 解釋可見 <a href="http://csl.skku.edu/papers/CS-TR-2010-329.pdf">[2]</a>的 2.2。</p><blockquote><p>interleved 類似 pipelined </p></blockquote><p>藉由結合不同層級的內部平行處理機制，SSD 可以同時 simutaneously  存取多個 block，又稱 clustered block. 作者建議想了解細節的人去看 <a href="#ref">[2, 3]</a>，進階指令如 copyback, inter-plane transfer 可參考 <a href="#ref">[5]</a>。</p><h2 id="6-2-不同層級的平行機制"><a href="#6-2-不同層級的平行機制" class="headerlink" title="6.2 不同層級的平行機制"></a>6.2 不同層級的平行機制</h2><p><img src="http://codecapsule.com/wp-content/uploads/2014/02/ssd-package.jpg"><br>上圖為 nand flash 的內部結構，所謂的層級即是 channel, package, chip, plane, block, 到 page ，以離 controller 的距離來做分級。</p><ul><li>Channel-level parallelism.<br>controller 與 package 透過多個 channel 溝通，各channel 可被獨立運用，也可同步使用，各個 channgel 由多個 package 共用。</li><li>Package-level parallelism.<br>在同個 channel 上的 package 可以被同時存取，上面提到的 interleaving 可以用在同 channel 的 package 上。</li><li>Chip-level parallelism.<br>一個 package 裡有兩個以上的 die&#x2F;chip 可被平行存取。</li><li>Plane-level parallelism.<br>一個 chip 裡面有兩個以上的 plane, 同個指令（讀寫抹）可同時下在 chip 的各 plane 上。plane 裡面有 block, block 裡面有 page。plane 裡面還有一些暫存器（小的 RAM 緩存區），用來協助 plane 層級的操作。</li></ul><h1 id="6-3-Clustered-blocks"><a href="#6-3-Clustered-blocks" class="headerlink" title="6.3 Clustered blocks"></a>6.3 Clustered blocks</h1><p>對分布在不同 chip 的多個 block 的操作也稱為 clustered block <a href="#ref">[2]</a>. 跟 HDD raid 的 striping 概念有點像 <a href="#ref">[1, 5]</a>.</p><p>批次對 LBA 的存取會被視為 clustered 操作，並同時對不同的 flash package 做存取。多虧了 FTL 的 mapping 演算法&#x2F;資料結構，即便我們不是做循序的讀寫，一樣可以發揮 FTL 平行運算的超能力。分散 block 到各個 channel 去讓我們的讀寫抹都可以平行處理。意味著當我們 IO 的大小是 clustered block 的倍數，並有把 LBA 對齊，將可充分利用 SSD 內部各層級的平行運作機制。下一篇的 8.2 8.3 有更多介紹</p><h1 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h1><ol start="0"><li><a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-4-advanced-functionalities-and-internal-parallelism/">coding for ssd part 4</a></li></ol><p>其他有編號參考資料請至原文觀賞：<a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-3-pages-blocks-and-the-flash-translation-layer/#ref">link</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ssd_notes_03</title>
      <link href="/ssd-notes-03/"/>
      <url>/ssd-notes-03/</url>
      
        <content type="html"><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>這篇主要介紹 SSD 的讀寫單位如 page、 block，以及寫入放大 (write amplification) 、 wear leveling 等 SSD 問題及設計。除此之外， Flash Translation Layer (FTL) 及其兩個主要功能 logical block mapping, garbage collection (gc)。也以 hybrid log-block mapping 設計當例子介紹 FTL 如何實際進行一個 flash 的寫入操作。</p><blockquote><p>如果是我的筆記會像這樣加註在 info 欄位。</p></blockquote><h1 id="3-SSD-的基本操作"><a href="#3-SSD-的基本操作" class="headerlink" title="3. SSD 的基本操作"></a>3. SSD 的基本操作</h1><h2 id="3-1-讀、寫、抹"><a href="#3-1-讀、寫、抹" class="headerlink" title="3.1 讀、寫、抹"></a>3.1 讀、寫、抹</h2><p>因為 nand flash 的物理特性， flash memory 存取時必須要遵循特定規則，如果我們了解這些特性對我們在最佳化資料結構設計時會有幫助。</p><ul><li>SSD 讀取以分頁 (page) 為基本單位，即便你只是要讀一個 byte，還是會回傳一個 page。</li><li>寫入也以 page 為單位，即便你只有寫入小量資料，實際進行物理寫入時 SSD 還是要寫一個 page，此類現象也稱為寫入放大。 write, program 在 SSD 期刊上指的是同一件事。</li><li>copy-modify-write: 已有資料的 page 不能被直接複寫，當需要更改 page 資料時，要不是寫在該 blcok 空白&#x2F;free 的 page 裡，然後把該 page 標示為 stale，或是將整個 block 複製到 mem 修改，再寫到其他空的 block 去。 stale 的 block 必須在其他時機點清空。</li><li>資料抹除必須以 block 為單位。一般使用者在讀寫資料時 SSD 不會實際把 stale 資料物理上抹除，SSD也只有進行 read&#x2F;write 操作。SSD 只在 GC 判斷需要清出空間時對 nand flash 執行抹除&#x2F;erase 指令。</li></ul><h2 id="3-2-寫入範例"><a href="#3-2-寫入範例" class="headerlink" title="3.2 寫入範例"></a>3.2 寫入範例</h2><p>圖中得 2 看到我們在寫入 x’ 的時候不是複寫 x，而是 free 的 page 1000-3。<br>3 則是 GC 的操作，把 1000 清為 free，原有資料放到另一個 block，並清除 stale page。<br><img src="http://codecapsule.com/wp-content/uploads/2014/02/ssd-writing-data.jpg" alt="寫入操作範例"></p><blockquote><p>這裡可以猜測 SSD controller 需要很多儲存 lba -&gt; pba 的資料結構.</p></blockquote><h2 id="3-3-寫入放大"><a href="#3-3-寫入放大" class="headerlink" title="3.3 寫入放大"></a>3.3 寫入放大</h2><p>寫入小於 page size 的資料會造成 write amplification 的空間浪費([13])[#ref], 寫入 1 B 變成 16 KB。此類寫入放大也會在後續 GC, wear leveling 中持續傳遞。我們也可能寫入一個 page 的資料量但是 address mapping 結果沒有對在 page 開始處，最後要用到兩個 page，並可能觸發 read-modify-write，讓效能變差<a href="#ref">[2, 5]</a>。</p><p>作者給了幾個建議：永遠不要寫入小於一個 page size 的資料，寫入的資料兩大小與 page size 成倍數為原則、小的大量寫入先做緩存再批次寫入。</p><h2 id="3-4-wear-leveling"><a href="#3-4-wear-leveling" class="headerlink" title="3.4 wear leveling"></a>3.4 wear leveling</h2><p>因為 SSD cell 有 P&#x2F;E life cycle 限制，如果我們一直都讀寫同個 block， cell 掛了，SSD 容量會隨著使用一直變少。 wear leveling 就是要讓使用次數平均分配到各個 block 去<a href="#ref">[12, 14]</a>。<br>為了做到 wear leveling, controller 在寫入時需要依 page 寫入次數來選擇，必要時也有可能將各個 block 的資料做調動，也是一種 write amplification。 block 管理就是在 wear leveling 跟 write amplification 之間做取捨。<br>SSD 製造商想出了各種方法來解決這類問題，讓我們繼續看下去。</p><blockquote><p>有點像在整理房間一樣，各個原則都有好有壞 XD。</p></blockquote><h1 id="4-Flash-Translation-Layer-FTL"><a href="#4-Flash-Translation-Layer-FTL" class="headerlink" title="4 Flash Translation Layer (FTL)"></a>4 Flash Translation Layer (FTL)</h1><h2 id="4-1-FTL-的必要性"><a href="#4-1-FTL-的必要性" class="headerlink" title="4.1 FTL 的必要性"></a>4.1 FTL 的必要性</h2><p>SSD 可以很快導入是因為他走 HDD 的 Logical Block Addresses (LBA) ，上層軟體&#x2F;檔案系統不用因為 SSD 做調整。 上面提到SSD 不如 HDD 各個 sector&#x2F;page 可以直接被複寫，所以 FTL 橫空出世來解決這個問題，把 SSD 操作細節藏起來，讓 host interface 依然只需要對不同的 LBA 做存取，不用管 copy-modify-write, level wearing 等事。</p><blockquote><p>amd64 與 x86 的演進感覺也是類似的關係，向後相容非常重要。誰跟你換個硬體&#x2F;架構就軟體全部重寫啊 XD。</p></blockquote><h2 id="4-2-LBA-to-PBA"><a href="#4-2-LBA-to-PBA" class="headerlink" title="4.2 LBA to PBA"></a>4.2 LBA to PBA</h2><p>controller 工作其一就是把 host interface 的 logical block address 轉physical address。這類資料結構通常是存成一個 table ，為了存取效率，這類資料會快取在 controller 的 memory 裡，並提供斷電保護。<a href="#ref">[1,5]</a></p><p>實作方法有 </p><ol><li><p>page level mapping，最有彈性。每個 page 都對應到各自的 physical page，缺點是需要更大的 ram 來存 mapping table，太貴。</p></li><li><p>為解決上述問題，block level mapping 節省 mapping table 的 ram。整個做法大幅降低了 mapping table ram 用量，但是每次寫入都需要寫入一個 block，面對大量小資寫入放大豈不崩潰<a href="#ref">[1,2]</a>。</p></li><li><p>上面的 page vs black 的戰爭就是空間換取時間之間的取捨。有些人開始說這樣不行，我全都要：有了混合的 log-block mapping ，面對小資寫入會先寫到 log 緩存區，log存到一定量再合併成 block 寫下去<a href="#ref">[9, 10]</a>。</p></li></ol><p>下圖是一個簡化版本的 hybrid log-glock FTL 實作。寫了四個 full page 大小的資料，Logical page # 5, 9 都對應到 logicl block number(LBN) 1，此時關聯到一個空的 physical block #1000。<br>一開始 log-block page mapping table 1、 block #1000 是空的，隨著寫入資料到 block 1000 的過程 physical page offset 會新增&#x2F;更新其對應位置。 #1000 也稱為 log block。</p><p><img src="http://codecapsule.com/wp-content/uploads/2014/02/ssd-hybrid-ftl.jpg" alt="又一張不知出處的圖"></p><p>當 log block #1000 寫滿了之後， controller 會將原有的 data block #3000 與 log block #1000 合併，寫到空的 data block #9000，此時 #9000 成了 data block。</p><p>值得注意的是這個方法消除了四個寫入 b’, d’, b’’, d’’ 可能帶來的寫入放大，而且合併 block 的時候，新的 block #9000 拿到的是新的資料  b’’, d’’.</p><p>最後讀取指令會看現在資料是在 log block 來回傳資料，若否則去查 data-block mapping table（圖左下方）</p><p>log-block 在我們剛好寫入完整的 block 的時候也可以直接省去跟 data block<br>合併的功夫，直接更改 data-block mapping table 的 metadata，並把原有的 data block 清空，更新為 log block。這類最佳化手段也稱為 switch-merge, swap-merge。</p><p>目前對 log-block 的研究很多：FAST (Fully Associative Sector Translation), superblock mapping, flexible group mapping <a href="#ref">[10]</a>。其他的 mapping 手法也有如 Mitsubishi algorithm, SSR <a href="#ref">[9]</a>。</p><blockquote><p>這類 hybrid log-block 的作法，作者說很像 <a href="https://en.wikipedia.org/wiki/Log-structured_file_system">log-sructured</a> 檔案系統，一個例子是 zfs。 自從 proxmox ve 開始接觸 zfs ，覺得他真的很好用… 從 ubuntu 19.10 開始可以直接把 rootfs 用 zfs 裝喔。</p></blockquote><h1 id="4-3-2014-業界狀況"><a href="#4-3-2014-業界狀況" class="headerlink" title="4.3 2014 業界狀況"></a>4.3 2014 業界狀況</h1><p>當時 wikipedia 有 70 間 SSD 廠商， 11 間有能力做 controller， 其中4 間有自有品牌（Intel, Samsung, …)，另外 7 間專做 controller 的公司佔了 90% 的市場銷量<a href="#ref">[64, 65]</a>。</p><blockquote><p>wiki 上 2019 年變成 12 家，而自有品牌的有 5 間 WD、Toshiba、Samsung、Intel、威盛電子。<br>台灣的 controller 廠商有 phison, slicon motion, VIA tech, realtek, jmicron</p></blockquote><p>作者不確定是哪幾間公司吃下這個市場，但他以 Pareto（80&#x2F;20） 法則猜應該是其中的兩三家，所以從除了自有品牌的 SSD，用同一個 controller 大概行為都會差不多。FTL 的實作對效能影響甚鉅，但是各家廠商也不會公開自己用了哪些方法實作。</p><p>作者對於了解或是逆向工程<a href="#ref">[3]</a> mapping schema 的實作對提升應用層程式的效能保持保留態度。畢竟市面上的 controller 廠商大多沒有開放實作細節，就算針對某個 policy 去調整程式設定更甚至你拿到原始碼，這套系統在其他 schema 或是其他廠牌下也不一定有更好的結果。唯一的例外可能是你在開發嵌入式系統，已經確定會用某廠商的晶片。</p><p>作者建議大抵上知道許多的 controller FTL 是實作 hybrid log block policy 就好了。然後盡量一次寫入至少一個 block size 的資料，通常會得到較好的結果。</p><h2 id="4-4-Garbage-Collection"><a href="#4-4-Garbage-Collection" class="headerlink" title="4.4 Garbage Collection"></a>4.4 Garbage Collection</h2><p>因為將 page 清為 free 的抹除(erase)指令 latency 較高(1500-3500 μs, 寫入 250-1500 μs)，大部分的 controller 會在閒暇時做 housekeeping，讓之後寫入作業變快<a href="#ref">[1]</a>，也有一些實作是在寫入時平行進行<a href="#ref">[13]</a>。</p><p>時常會遇到 foregound 頻繁小檔寫入影響 background，導致找不到時間做  GC 的情況。這時候TRIM command, over-provisioning 可以幫得上忙（下一篇會介紹）。</p><p>flash 還有一個特性是 read disturb，常讀一個 block 的資料會造成 flash 狀態改變，所以讀了一定次數以後也需要搬動 block <a href="#ref">[14]</a></p><p>另外當一個 page 裡面有不常改的 cold&#x2F;static data 及 hot&#x2F;dynamic data，hot data 的更動會讓他們一起被搬動，分開冷熱資料可以改善這類情況。（不過冷的資料放久了還是會因 wear leveling 被動）。 另外也因為資料冷熱是應用層的事，SSD 不會知道，改善 SSD 效能的一個方法便是冷熱分開在不同的 page裡，讓 GC 好做事。</p><p>作者也建議 “非常熱” 的資料可以先 cache 起來再寫到硬碟。以及當有資料不再被需要、或是需要刪除的時候可以批次進行，這讓 GC 可以一次得到比較多的空間操作，降低空間碎片化。</p><blockquote><p>所以很多人提倡電腦換了要把硬碟丟到調理機裡面，這點在 SSD 也不例外 XD</p></blockquote><h1 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h1><ol start="0"><li><a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-3-pages-blocks-and-the-flash-translation-layer/">coding for ssd part 3</a></li></ol><p>其他有編號參考資料請至原文觀賞：<a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-3-pages-blocks-and-the-flash-translation-layer/#ref">link</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD笔记-第二篇SSD结构与性能评估概述</title>
      <link href="/ssd-notes-02/"/>
      <url>/ssd-notes-02/</url>
      
        <content type="html"><![CDATA[<p>本文转载自：<a href="https://www.owlfox.org/blog/2019-11-25-coding-for-SSD-part-2/">https://www.owlfox.org/blog/2019-11-25-coding-for-SSD-part-2/</a></p><h1 id="緣由"><a href="#緣由" class="headerlink" title="緣由"></a>緣由</h1><p>這篇主要談論 Nand flash 的不同 cell type，基本的 SSD 系統架構，及如何做 SSD 效能評定（Benchmarking）。作者是在 booking.com 上班的軟體工程師。有用過應該就知道這是很大的旅遊訂房行程規劃服務網站，在這類工作環境可能需要對底層的效能有深入解快，才能解決工作上的實務問題。我覺得這類軟體從業人員提供的觀點對自己來說幫助很大，所以翻譯&#x2F;兼做做筆記。</p><h1 id="SSD-？"><a href="#SSD-？" class="headerlink" title="SSD ？"></a>SSD ？</h1><p>Solid state drives，顧名思義 SSD 設計裡去除了傳統硬碟裡不 solid，會動的部分，改善了噪音、震動、讀寫速度慢、易損壞及資料分散時需要硬碟重組來改善讀取時間等缺點。<br>SSD 作為儲存裝置：</p><ul><li>優點：<ul><li>隨機存取快、且存取時間固定，HDD 的 seek time ？ 沒這毛病！</li><li>體積小，看看這些愈來愈小的筆記型電腦、移動裝置、SD卡</li><li>少了傳統硬碟機械故障、硬碟重組等煩惱。</li></ul></li><li>缺點：<ul><li>Cell 有讀寫次數限制(wearing off&#x2F;wear-out)<blockquote><p>但對於 IT 人員來說， HDD 也是有看人品、需買高階型號跟擺乖乖才能保證資料安全的問題。 <img src="https://comet.noonspace.com/w61NoonSpace/kuai/MsgInfo/LogoKuai.png" alt="乖乖 LOGO, kuai.com.tw"></p></blockquote></li><li>bit&#x2F;$ 較高, (TODO)</li></ul></li></ul><h2 id="NAND-flash-種類"><a href="#NAND-flash-種類" class="headerlink" title="NAND flash 種類"></a>NAND flash 種類</h2><p>依各個 block 可儲存的資料多寡，可分為：<br>SLC, MLC, eMLC, TLC, QLC, 3D NAND, see this <a href="https://searchstorage.techtarget.com/definition/flash-memory">link</a> for ref</p><p>關於製程資訊（floating gate, charge trap) 見 <a href="#%E5%BF%AB%E9%96%83%E8%A8%98%E6%86%B6%E9%AB%94%E7%9A%84%E8%B7%AF%E7%B7%9A%E4%B9%8B%E7%88%AD">3</a></p><blockquote><p>關於 IC &#x2F; PCB &#x2F; SMT 的製程可能要補文章（TODO）</p></blockquote><h2 id="存取介面"><a href="#存取介面" class="headerlink" title="存取介面"></a>存取介面</h2><p>目前看到的 SSD 架構 &#x3D;  SSD controller 晶片 + RAM + NAND flash<br>controll 支援多種不同的 host interface 指令格式</p><ul><li>Serial ATA (SATA), 3.0 ~ 6GBit&#x2F;s</li><li>PCI Express (PCIe), 3.0 ~ 8Gbit&#x2F;s per lane, 4 lanes max</li><li><a href="https://nvmexpress.org/">nvme</a></li><li>Serial Attached SCSI interface (SAS), ~ 12 Gbit&#x2F;s</li></ul><blockquote><p>也有看到 open channel SSD 將主控權交給作業系統，詳情可見 [2](#lightnvm, linux implementation of open channel SSD)。我覺得有點像是 zfs 捨棄 raid 卡讓檔案系統透過 HBA 卡接管硬碟所有資訊的作法。我覺的軟體定義的方式應該是終端用戶最後的選擇，畢竟免了 vendor lock in 的問題。</p></blockquote><p>controller 把 NAND flash 的 block, page size, GC(garbage collection) 等細節藏起來，讓 host interface 及其上層作業系統有跟 HDD 一樣的存取介面。</p><h2 id="效能評定-Benchmarking"><a href="#效能評定-Benchmarking" class="headerlink" title="效能評定 Benchmarking"></a>效能評定 Benchmarking</h2><p>原文作者有發現當時的 SSD 效能報告<a href="http://blog.zorinaq.com/many-ssd-benchmark-reviews-contain-flaws/">亂象</a>，例如不同的 <a href="https://gerardnico.com/io/drive/lba">LBA</a>, 過於簡單的 <a href="https://www.userbenchmark.com/Faq/What-is-queue-depth/41">queue size</a> 測試情節。文中也提到 SSD 的讀寫測試其實要在寫入一定的隨機資料<a href="https://searchstorage.techtarget.com/feature/The-truth-about-SSD-performance-benchmarks">pre-conditioning, warm up</a>才有測出 controller GC 能力並具參考價值。而非當時很多資料是拿了新的 SSD 測了 happy path 很開心就把資料放出來這樣，文中舉的比較好的範例是這篇關於 samsung 840 pro 做的<a href="https://www.storagereview.com/samsung_ssd_840_pro_review">評測</a>，可以很明顯看到讀寫效能(IOPS, Read&#x2F;Write at different sizes&#x2F;order)在一定的讀寫後明顯下降，文中也對其拿實際的應用案例如資料庫、網頁伺服器做了分析，並得到其在前述企業應用環境效能較差的結論。</p><blockquote><p><code>圖一堆，真是很有心 XD</code></p></blockquote><p>目前不確定儲存裝置是否有個明確的效能評定規範（針對不同應用情境、不同裝置、不同 host interface）。但作者提出一套他的原則（2.3內容）：</p><ul><li>workload type ，確定你的應用環境是哪種讀寫操作居多</li><li>percentage of read &#x2F; write, 設定同步進行的讀寫操作比例，如 30% 讀 70% 寫</li><li>queue length，你有多少同步執行的執行緒(thread)在對儲存裝置下指令</li><li>size of data chunk, 你的應用環境的檔案讀寫大小（4KB, 8KB 之類的)</li></ul><blockquote><p>最後一點不太確定怎麼定義，如果你是跑 postgresql, mysql 那要怎麼知道大小？</p></blockquote><p>以及需要觀測的指標：</p><ul><li>Throughput: KB&#x2F;s, MB&#x2F;s 資料轉換的效率，一般是 sequential 的評定會看</li><li>IOPS: 每秒可完成的 Input&#x2F;Output（IO） 操作，這是以作業系統的觀點來看，通常是拿 4KB 的寫入來測，用來評定隨機操作的效能。<blockquote><p>應該是因為 4KB 是大部分作業系統 virtual memory 預設的 page size, 這也要因應使用情節而調整。</p></blockquote></li><li>latency:  下指令到完成指令回傳結果需要的時間 μs, ms</li></ul><p>IOPS 也可以換算成 throughput, 如 1000 IOPS 在 4KB 檔案大小下 就是 4 MB&#x2F;s. 作者也舉了個可能的 logging 系統案例， 10k IOPS, log 檔四散各地，可能的 throughput 會是 20 MB&#x2F;s </p><p>另外 throughput 不等同於效能，假設你有個伺服器裝了個 10G 網卡，偏偏你的系統每次作業要跟 25 個 Database 拿資料，每個連線要花 20 ms 好死不死你還寫成 single blocked thread，每次處理一個網頁頁面至少都要多花 500 ms，這個就偏人的問題，而非系統效能瓶頸。</p><blockquote><p>所以我想一般都是在系統發展到一定規模，要做大、或是遇上應用程式端無法解決瓶頸時才會多考慮底層儲存系統選擇與設定。</p></blockquote><p>在確保自己的系統架構不會對儲存系統造成不必要的負擔之後，這三項指標（一起）是系統管理員、軟體工程師在評估自己的硬體是否符合需求時的常用指標。</p><h1 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h1><h2 id="coding-for-ssd-part2"><a href="#coding-for-ssd-part2" class="headerlink" title="coding for ssd part2"></a>coding for ssd part2</h2><p><a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-2-architecture-of-an-ssd-and-benchmarking/">link</a></p><h2 id="lightnvm-linux-implementation-of-open-channel-SSD"><a href="#lightnvm-linux-implementation-of-open-channel-SSD" class="headerlink" title="lightnvm, linux implementation of open channel SSD"></a>lightnvm, linux implementation of open channel SSD</h2><p>links: </p><ul><li><a href="http://lightnvm.io/">http://lightnvm.io/</a></li><li><a href="https://openchannelssd.readthedocs.io/en/latest/">https://openchannelssd.readthedocs.io/en/latest/</a></li><li><a href="https://www.usenix.org/conference/fast17/technical-sessions/presentation/bjorling">https://www.usenix.org/conference/fast17/technical-sessions/presentation/bjorling</a></li><li><a href="https://www.ithome.com.tw/news/122307">https://www.ithome.com.tw/news/122307</a></li></ul><h2 id="The-Myth-of-HDD-Endurance"><a href="#The-Myth-of-HDD-Endurance" class="headerlink" title="The Myth of HDD Endurance"></a>The Myth of HDD Endurance</h2><p><a href="https://www.micron.com/about/blog/2016/february/the-myth-of-hdd-endurance">https://www.micron.com/about/blog/2016/february/the-myth-of-hdd-endurance</a></p><h2 id="快閃記憶體的路線之爭"><a href="#快閃記憶體的路線之爭" class="headerlink" title="快閃記憶體的路線之爭"></a>快閃記憶體的路線之爭</h2><p><a href="https://www.digitimes.com.tw/col/article.asp?id=717">https://www.digitimes.com.tw/col/article.asp?id=717</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD笔记- 第一篇引言</title>
      <link href="/ssd-notes-01/"/>
      <url>/ssd-notes-01/</url>
      
        <content type="html"><![CDATA[<p>本文转载自：<a href="https://www.owlfox.org/blog/2019-11-25-coding-for-SSD-part-1/">https://www.owlfox.org/blog/2019-11-25-coding-for-SSD-part-1/</a></p><h2 id="缘由"><a href="#缘由" class="headerlink" title="缘由"></a>缘由</h2><p>Emmanuel Goossaert 是booking.com的工程师，他因为想拿SSD 做自己的 key-value store专案的储存方案，开始学习SSD 相关知识。这六篇文是他在2014 年写下，里面很多的参考资讯可能都找不到了，但是我刚好在准备SSD 相关工作面试，想想还是有参考价值，所以做了简单翻译，跟一些笔记，再加一些(个人意见)。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>作者对这系列文章的结论可以看<img src="/" alt="第六篇"></p><h2 id="我的结论"><a href="#我的结论" class="headerlink" title="我的结论"></a>我的结论</h2><p>我自己是觉得这系列文章对于入门了解SSD 还不错。如果要做到这种程度的最佳化必须要很多人一起投入。</p><ol><li>sysadmin 必须确认档案系统、SSD 型号、作业系统配置。</li><li>developer 应用层的程式必须要注意错误的写入&#x2F;读取的资料大小&#x2F;频率可能对SSD 造成的过大压力。</li></ol><p><strong>难维护。</strong></p><p>就目前我的认知，需要做到对效能斤斤计较又很重要的系统瓶颈在File system，需要对SSD 特性客制化应用层程式的机会很小。 解决方案？选个好文件系统？</p><h2 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h2><p>zfs既有的档案系统阶层在效能上可能已经很优秀了，还送copy-on-write，容错机制，snapshot。</p><img src="/ssd-notes-01/image.png" class="" title="alt text"><p> <a href="http://www.brendangregg.com/blog/2008-07-22/zfs-l2arc.html">http://www.brendangregg.com/blog/2008-07-22/zfs-l2arc.html</a></p><p>zfs 也开始可以在linux 上面使用，Ubuntu 19.10 也有直接把zfs 装成roo FS 的选项。 如果是一般server、文书、游戏使用我会以后装个zfs 就好了。</p><p>至于更高端的选择.. 可能是建cepf cluster，或是open-channel SSD 等特殊解法？</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fp16 的累加误差有多大</title>
      <link href="/fp16-err/"/>
      <url>/fp16-err/</url>
      
        <content type="html"><![CDATA[<p>最近在项目中需要实现fp16的数据类型做FFN的计算，算子实现的同学反馈误差与x86上得到的golden数据有比较大误差。开始以为是x86侧做数值模拟仿真的问题。后面也实现了对比了一下，发现误差累计确实挺大。</p><h2 id="实测结果对比"><a href="#实测结果对比" class="headerlink" title="实测结果对比"></a>实测结果对比</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Seed with a real random value, if available</span></span><br><span class="line">    std::random_device rd;</span><br><span class="line">    <span class="function">std::mt19937 <span class="title">gen</span><span class="params">(rd())</span></span>;</span><br><span class="line">    std::uniform_real_distribution&lt;&gt; <span class="built_in">dist</span>(<span class="number">0</span>, <span class="number">0.01</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="type">float16_t</span> lhs[<span class="number">4096</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">float16_t</span> rhs[<span class="number">4096</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4096</span>; i++) &#123;</span><br><span class="line">        lhs[i] =  <span class="built_in">dist</span>(gen);</span><br><span class="line">        rhs[i] =  <span class="built_in">dist</span>(gen);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">float16_t</span> res_fp16 = <span class="number">0</span>;</span><br><span class="line">    <span class="type">float</span> res_fp32 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4096</span>; i++) &#123;</span><br><span class="line">        res_fp16 += lhs[i] * rhs[i];</span><br><span class="line">        res_fp32 += lhs[i] * rhs[i];</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;fp16 &quot;</span> &lt;&lt; res_fp16 &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;fp32 &quot;</span> &lt;&lt; res_fp32 &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">wirte2file</span>(<span class="string">&quot;/data/local/tmp/lhs&quot;</span>, <span class="built_in">reinterpret_cast</span>&lt;<span class="type">char</span>*&gt;(lhs), <span class="number">8192</span>);</span><br><span class="line">    <span class="built_in">wirte2file</span>(<span class="string">&quot;/data/local/tmp/rhs&quot;</span>, <span class="built_in">reinterpret_cast</span>&lt;<span class="type">char</span>*&gt;(rhs), <span class="number">8192</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fp16 0.0942383</span><br><span class="line">fp32 0.103176</span><br></pre></td></tr></table></figure><p>相对误差到8.1%了。难怪反馈有问题。</p><table><thead><tr><th>dim</th><th>绝对误差</th></tr></thead><tbody><tr><td>100</td><td>1.63913e-07</td></tr><tr><td>1000</td><td>-0.00033829</td></tr><tr><td>2000</td><td>-0.000909835</td></tr><tr><td>4000</td><td>-0.00924221</td></tr></tbody></table><h2 id="golden-数据误差从何而来"><a href="#golden-数据误差从何而来" class="headerlink" title="golden 数据误差从何而来"></a>golden 数据误差从何而来</h2><p>实际生成golden数据的时候，也考虑了数值类型差异的影响，那为什么还存在误差呢？</p><blockquote><p>对比了一下dot的视线与直接累加结果</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">lhs = np.fromfile(<span class="string">&quot;lhs&quot;</span>,dtype=np.float16)</span><br><span class="line">rhs = np.fromfile(<span class="string">&quot;rhs&quot;</span>,dtype=np.float16)</span><br><span class="line"></span><br><span class="line">lhs = torch.from_numpy(lhs)</span><br><span class="line">rhs = torch.from_numpy(rhs)</span><br><span class="line"></span><br><span class="line">res = torch.Tensor([<span class="number">1</span>]).half()</span><br><span class="line">res[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4096</span>):</span><br><span class="line">    res += lhs[i:i+<span class="number">1</span>] * rhs[i:i+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(res)</span><br><span class="line"><span class="built_in">print</span>(torch.dot(lhs, rhs))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.0942], dtype=torch.float16)</span><br><span class="line">tensor(0.1041, dtype=torch.float16)</span><br></pre></td></tr></table></figure><p>结果对得上了。torch 的 dot实现的时候很可能用了更高数值类型做累加。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL </tag>
            
            <tag> 数值精度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cuda_mode_1</title>
      <link href="/cuda-mode-1/"/>
      <url>/cuda-mode-1/</url>
      
        <content type="html"><![CDATA[<h1 id="hello-load-inline"><a href="#hello-load-inline" class="headerlink" title="hello load inline"></a>hello load inline</h1><p>这个是torch加载C++扩展的简单demo。代码比较简单</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load_inline</span><br><span class="line"></span><br><span class="line">cpp_source = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">std::string hello() &#123;</span></span><br><span class="line"><span class="string">  return &quot;Hello World!&quot;;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">my_module = load_inline(</span><br><span class="line">    name=<span class="string">&#x27;my_module&#x27;</span>,</span><br><span class="line">    cpp_sources=[cpp_source],</span><br><span class="line">    functions=[<span class="string">&#x27;hello&#x27;</span>],</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">    build_directory=<span class="string">&#x27;./tmp&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(my_module.hello())</span><br></pre></td></tr></table></figure><p>执行输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Emitting ninja build file ./tmp/build.ninja...</span><br><span class="line">Building extension module my_module...</span><br><span class="line">Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)</span><br><span class="line">ninja: warning: build log version is too old; starting over</span><br><span class="line">[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=my_module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\&quot;_gcc\&quot; -DPYBIND11_STDLIB=\&quot;_libstdcpp\&quot; -DPYBIND11_BUILD_ABI=\&quot;_cxxabi1011\&quot; -isystem /home/anaconda3/lib/python3.12/site-packages/torch/include -isystem /home/anaconda3/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/anaconda3/lib/python3.12/site-packages/torch/include/TH -isystem /home/anaconda3/lib/python3.12/site-packages/torch/include/THC -isystem /home/anaconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /mnt/e/cuda_mode_notes/lecture_001/tmp/main.cpp -o main.o </span><br><span class="line">[2/2] c++ main.o -shared -L/home/anaconda3/lib/python3.12/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o my_module.so</span><br><span class="line">Loading extension module my_module...</span><br><span class="line">Hello World!</span><br></pre></td></tr></table></figure><p>这个需要创建好tmp文件夹。然后创建编译、加载</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-rwxrwxrwx  .ninja_deps</span><br><span class="line">-rwxrwxrwx  .ninja_log</span><br><span class="line">-rwxrwxrwx  build.ninja</span><br><span class="line">-rwxrwxrwx  main.cpp</span><br><span class="line">-rwxrwxrwx  main.o</span><br><span class="line">-rwxrwxrwx  my_module.so</span><br></pre></td></tr></table></figure><p>这里完成了完整的编译流程，把C++代码放在main.cpp中，编译到.o文件，链接到so。然后在python侧加载运行。<br>需要注意的是，这里底层依赖的是pybind11的库。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::string <span class="title">hello</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="string">&quot;Hello World!&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(TORCH_EXTENSION_NAME, m) &#123;</span><br><span class="line">m.<span class="built_in">def</span>(<span class="string">&quot;hello&quot;</span>, torch::<span class="built_in">wrap_pybind_function</span>(hello), <span class="string">&quot;hello&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="load-inline-py"><a href="#load-inline-py" class="headerlink" title="load_inline.py"></a>load_inline.py</h1><p>这里是上面同样的操作，不同的是这里实现的是CUDA代码，定义了一个平方运算的CUDA程序 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.cpp_extension <span class="keyword">import</span> load_inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the CUDA kernel and C++ wrapper</span></span><br><span class="line">cuda_source = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) &#123;</span></span><br><span class="line"><span class="string">    int row = blockIdx.y * blockDim.y + threadIdx.y;</span></span><br><span class="line"><span class="string">    int col = blockIdx.x * blockDim.x + threadIdx.x;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (row &lt; height &amp;&amp; col &lt; width) &#123;</span></span><br><span class="line"><span class="string">        int idx = row * width + col;</span></span><br><span class="line"><span class="string">        result[idx] = matrix[idx] * matrix[idx];</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#include &lt;sys/types.h&gt;</span></span><br><span class="line"><span class="string">#include &lt;unistd.h&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">torch::Tensor square_matrix(torch::Tensor matrix) &#123;</span></span><br><span class="line"><span class="string">    const auto height = matrix.size(0);</span></span><br><span class="line"><span class="string">    const auto width = matrix.size(1);</span></span><br><span class="line"><span class="string">    pid_t pid = getpid();</span></span><br><span class="line"><span class="string">    printf(&quot;pid %d &quot; , pid);</span></span><br><span class="line"><span class="string">    auto result = torch::empty_like(matrix);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    dim3 threads_per_block(16, 16);</span></span><br><span class="line"><span class="string">    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,</span></span><br><span class="line"><span class="string">                          (height + threads_per_block.y - 1) / threads_per_block.y);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    square_matrix_kernel&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;(</span></span><br><span class="line"><span class="string">        matrix.data_ptr&lt;float&gt;(), result.data_ptr&lt;float&gt;(), width, height);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    return result;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">cpp_source = <span class="string">&quot;torch::Tensor square_matrix(torch::Tensor matrix);&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the CUDA kernel as a PyTorch extension</span></span><br><span class="line">square_matrix_extension = load_inline(</span><br><span class="line">    name=<span class="string">&#x27;square_matrix_extension&#x27;</span>,</span><br><span class="line">    cpp_sources=cpp_source,</span><br><span class="line">    cuda_sources=cuda_source,</span><br><span class="line">    functions=[<span class="string">&#x27;square_matrix&#x27;</span>],</span><br><span class="line">    with_cuda=<span class="literal">True</span>,</span><br><span class="line">    extra_cuda_cflags=[<span class="string">&quot;-O2&quot;</span>],</span><br><span class="line">    build_directory=<span class="string">&#x27;./load_inline_cuda&#x27;</span>,</span><br><span class="line">    <span class="comment"># extra_cuda_cflags=[&#x27;--expt-relaxed-constexpr&#x27;]</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]], device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(square_matrix_extension.square_matrix(a))</span><br></pre></td></tr></table></figure><p>运行下来可以正常运行，没啥问题。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">python load_inline.py</span></span><br><span class="line">/home/wanger/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. </span><br><span class="line">If this is not desired, please set os.environ[&#x27;TORCH_CUDA_ARCH_LIST&#x27;].</span><br><span class="line">  warnings.warn(</span><br><span class="line">tensor([[ 1.,  4.,  9.],</span><br><span class="line">        [16., 25., 36.]], device=&#x27;cuda:0&#x27;)</span><br><span class="line">pid 17133 </span><br></pre></td></tr></table></figure><p><code>ncu python load_inline.py </code> 这个倒没有跟代码一样报错。不过也没任何输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ncu python load_inline.py</span> </span><br><span class="line">==PROF== Connected to process 17416 (/home/wanger/anaconda3/bin/python3.12)</span><br><span class="line">/home/wanger/anaconda3/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. </span><br><span class="line">If this is not desired, please set os.environ[&#x27;TORCH_CUDA_ARCH_LIST&#x27;].</span><br><span class="line">  warnings.warn(</span><br><span class="line">==ERROR== Unknown Error on device 0.</span><br><span class="line">tensor([[ 1.,  4.,  9.],</span><br><span class="line">        [16., 25., 36.]], device=&#x27;cuda:0&#x27;)</span><br><span class="line">pid 17416 </span><br><span class="line">==PROF== Disconnected from process 17416</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cuda </tag>
            
            <tag> lecture </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>端侧 LLM 的PD分离技术之稀疏性</title>
      <link href="/llm-sparse/"/>
      <url>/llm-sparse/</url>
      
        <content type="html"><![CDATA[<img src="/llm-sparse/image.png" class="" title="alt text"><blockquote><p>Attention是时空力场（LLM只有时，Version存在空间力场），而FFN则是空间结构，并且代表了基于Knowledge的静态高纬字典，也有人用图书馆来比喻。   </p></blockquote><p>首先回顾一下transformer block的结构，Attention +FFN，前者中的 kvcache 随着序列的增长占用越来越高，FFN则在权重中占了极大比例。</p><ul><li>#Attention 部分完成的是时间序列建模，完成的<strong>当前token与历史信息（kv cache）</strong>的相关性查找。</li><li>#FFN 层则沉淀了训练集中的固化的“知识”，从Gate、Up的运算来看，其计算也有很强的查找属性。FFN权重的尺寸占比比较高，从直觉上讲计算中也是一部分发挥作用，这中间存在一个筛选的机制。从计算过程也可以反映出，Up、Gate的计算接近于查找得到一个mask，Down部分对应value，最终得到结果传递给下一层。</li></ul><p>既然是查找过程，就存在选择，也就有了稀疏性。有了稀疏性指引，那很多工作就可以优化了。<br>比如：Attention部分kvcache 走以查代算、DRAM到GPU HBM的动态加卸载等等</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nvidia 的快速反量化操作</title>
      <link href="/fast-dequant/"/>
      <url>/fast-dequant/</url>
      
        <content type="html"><![CDATA[<p>主要记录了论文Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production中关于反量化的快速操作。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>NN模型通常为了降低RAM、FLASH占用，提高计算吞吐率，会引入量化操作。比如INT8、INT4、INT3等等。如果是weight only的量化方法情况，输入的部分还是浮点，在GPU、NPU、arm上float16用的比较多。这样一来，就需要在计算的时候，需要把权重反量化到浮点再进行计算。</p><p>论文在实验中发现，使用原生的native Int8ToFloat16做数值类型转换，性能并不符合预期。于是决定，将native Int8ToFloat16（吞吐低，底层应该是走 PTX cvt指令-&gt;SASS，关于ARM上计算，这里回头查一下）替换成一系列高吞吐的ALU和FP16运算指令，来巧妙地完成Int8-&gt;Float16的数值转换。</p><h2 id="INT8-Float16"><a href="#INT8-Float16" class="headerlink" title="INT8 -&gt; Float16"></a>INT8 -&gt; Float16</h2><ul><li>【观察一】 对于任意 FP16 数 X，其中 1024 ≤ X &lt;2048, 1024 将准确地表示在指数位，而 int(X − 1024) 部分将直接存储在尾数中（二进制level原封不动地存储）。</li><li>【观察二】对于任何整数 0 ≤ Y &lt; 1024，我们可以构造 Y + 1024 的 FP16 表示，将指数设置为 1024 并存储 Y在 FP16 尾数中。</li></ul><p><a href="https://evanw.github.io/float-toy/">浮点表示</a></p><p>首先，回忆一下浮点的表示方法。下图表示了float32的表示。</p><img src="/fast-dequant/image.png" class="" title="alt text"><p>很明显可以看出来，右侧尾数表示的范围与左侧指数的表示范围是强相关的，左边指数范围越小，那么精度越高；反之，指数越大，精度越低。下面我们看下float16。</p><img src="/fast-dequant/image-1.png" class="" title="alt text"><p>我们先忽略符号位部分，中间5bit表示指数，右边10bit表示小数。回到转10进制的表示，<br>$$<br> sign \times 2^{exp-15} \times (1+\frac{frac}{1024})<br>$$</p><p>其中fraction表示尾数部分的10进制数值，公式中之所以是(fraction&#x2F;1024)，是因为FP16尾数部分是10位，而2^10&#x3D;1024。<br>来看一个FP16二进制转实数的列子，比如<code>0b0110010000000011</code>这个FP16，其中组成为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0b 0 11001 0000000011</span><br><span class="line"># 符号位：0</span><br><span class="line"># 指数位：11001 -&gt; 25 -&gt; 25 - 15 = 10, 2^10=1024</span><br><span class="line"># 尾数部分：0000000011 -&gt; fraction为3（尾数部分的10进制数值）</span><br></pre></td></tr></table></figure><p>转换为实数表达为：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 * 2^10 * (1 + 3/1024) = 1027 = 1024 + 3 &lt;-&gt; 2^10 * 1 + 2^10 * 3/1024 </span><br></pre></td></tr></table></figure><p>观察一，是<strong>针对FP16数值</strong>而言的，它描述的其实是，FP16中落在$[1024, 2048)$ 范围中的数值，<strong>在16个bit位上的分布规律</strong>。如果转float32。同样道理算一下就可以。</p><p>现在从观察一可以得到一个结论：<br>$[1024, 2048)$ 范围内的float16整数x，在尾数部分的值恰好等于 x-1024<br>现在结合观察二，就得到量化后int4数值速转float16方法。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = 0x6400 | (x &amp;0b1111)</span><br><span class="line">x = x - 1024</span><br></pre></td></tr></table></figure><p>上面可以看出，这个操作仅支持正数，那么int8是有符号的怎么处理的呢？</p><blockquote><p>量化存储的int值是加上了128转为u8，在反量化时需要减去这个128即可</p></blockquote><p>code: <a href="https://github.com/Wanger-SJTU/recipes/tree/master/fast_dequant">https://github.com/Wanger-SJTU/recipes/tree/master/fast_dequant</a><br><strong>参考链接：</strong></p><ol><li><a href="https://zhuanlan.zhihu.com/p/657072856">https://zhuanlan.zhihu.com/p/657072856</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> llm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Turbo Sparse</title>
      <link href="/TurboSparse/"/>
      <url>/TurboSparse/</url>
      
        <content type="html"><![CDATA[<h2 id="关于llama稀疏性的观察"><a href="#关于llama稀疏性的观察" class="headerlink" title="关于llama稀疏性的观察"></a>关于llama稀疏性的观察</h2><p>llama原始模型的FFN计算过程为：</p><p>$$<br>f(x) &#x3D; \text{silu}(xW_{Gate}) \odot xW_{UP} \times W_{Down}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.w2(F.silu(self.w1(x)) * self.w3(x))</span><br></pre></td></tr></table></figure><table><thead><tr><th>Model</th><th>Sparisty</th></tr></thead><tbody><tr><td>Llama-2-7B</td><td>40%</td></tr><tr><td>ReLULlama-7B</td><td>67%</td></tr><tr><td>ShiftedReLULlama-7B</td><td>71%</td></tr></tbody></table><p>论文统计首层transformer block FFN层的稀疏性质，原生FFN的稀疏性仅有40%，激活函数由silu替换为Relu后可以达到67%，而ShiftedReLU可进一步提高到71%。<br>从FFN层的计算上来看，表面上是Gate部分作为门控控制了计算的稀疏性，<strong>实际上Up、Gate共同控制了计算的稀疏性</strong>，所以很自然的就引出了<strong>drelu</strong>的方案</p><p>$$<br>\text{Combined dReLU} (x) :&#x3D; max(0, xW_{gate} ) \odot max(0, xW_{up} )<br>$$</p><img src="/TurboSparse/image-2.png" class="" title="alt text"><p>从训练过程上来看，替换以后收敛性没有影响，结果的评价指标上也没有太大影响。</p><p>下一步就是进一步评价下修改以后得稀疏度了。这里没有直接用两个mask的交集，而是按照topk的方法做了评测</p><p>$$<br>\text{Mask}(x) :&#x3D; Top_k(|\text{Combined}(x)|)<br>$$</p><p>$$<br>        \text{Gated-MLP}(x) :&#x3D; (\text{Combined}(x) ∗ \text{Mask}(x))W_{down}<br>$$</p><img src="/TurboSparse/image-1.png" class="" title="alt text"><p>显然效果显著。不影响模型表现的情况下，稀疏到达到了80%，而牺牲一定精度的条件下可以到达<strong>90%</strong></p><h2 id="Sparsity-of-Sparsifi-ed-Models"><a href="#Sparsity-of-Sparsifi-ed-Models" class="headerlink" title="Sparsity of Sparsifi ed Models"></a>Sparsity of Sparsifi ed Models</h2><img src="/TurboSparse/image.png" class="" title="alt text">]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kimi 长上下文 case</title>
      <link href="/kimi-case-0/"/>
      <url>/kimi-case-0/</url>
      
        <content type="html"><![CDATA[<p>今天用kimi看论文的时候，想让他总结翻译一下论文某一节的内容。结果不是很理想，看来这个也是一种形式的“捞针实验”了吧<br>后续再其他平台也测试一下。</p><img src="/kimi-case-0/image.png" class="" title="alt text">]]></content>
      
      
      <categories>
          
          <category> 记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nivdia向量数据库图检索最新标杆——CAGRA</title>
      <link href="/CARGA/"/>
      <url>/CARGA/</url>
      
        <content type="html"><![CDATA[<p><a href="https://docs.rapids.ai/api/raft/nightly/pylibraft_api/neighbors/#cagra">CAGRA</a> 是 N社在RAFT项目中 最新的 ANN 向量索引。这是一种高性能的、 GPU 加速的、基于图的方法，尤其是针对<strong>小批量情况进行了优化</strong>，其中每次查找只包含一个或几个查询向量。</p><p>与其他像HNSW、SONG等这类基于图的方法相似，CAGRA在索引训练阶段构建了一个经过优化的 k-最近邻（k-NN）图。这个图具备多种优良特性，能够在保持合理召回率的同时实现高效的搜索。与NSW、HNSW算法不同的是，CARGA算法是单层的图，为了适用GPU计算加速，在构建和查询阶段做了特殊的优化。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> raft::neighbors;</span><br><span class="line"><span class="comment">// use default index parameters based on shape of the dataset</span></span><br><span class="line">ivf_pq::index_params build_params =   ivf_pq::index_params::<span class="built_in">from_dataset</span>(dataset);</span><br><span class="line">ivf_pq::search_params search_params;</span><br><span class="line"><span class="keyword">auto</span> knn_graph      = raft::<span class="built_in">make_host_matrix</span>&lt;IdxT, IdxT&gt;(dataset.<span class="built_in">extent</span>(<span class="number">0</span>), <span class="number">128</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// create knn graph</span></span><br><span class="line">cagra::<span class="built_in">build_knn_graph</span>(res, dataset, knn_graph.<span class="built_in">view</span>(), <span class="number">2</span>, build_params, search_params);</span><br><span class="line"><span class="keyword">auto</span> optimized_gaph = raft::<span class="built_in">make_host_matrix</span>&lt;IdxT, IdxT&gt;(dataset.<span class="built_in">extent</span>(<span class="number">0</span>), <span class="number">64</span>);</span><br><span class="line">cagra::<span class="built_in">optimize</span>(res, dataset, knn_graph.<span class="built_in">view</span>(), optimized_graph.<span class="built_in">view</span>());</span><br><span class="line"><span class="comment">// Construct an index from dataset and optimized knn_graph</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> index = cagra::<span class="built_in">index</span>&lt;T, IdxT&gt;(res, build_params.<span class="built_in">metric</span>(), dataset,</span><br><span class="line">                                   optimized_graph.<span class="built_in">view</span>());</span><br></pre></td></tr></table></figure><p>CAGRA构建的图有几个不同之处：</p><ul><li>每个节点有固定的出度</li><li>构建的图是一个有向图</li><li>不同于HNSW，CAGRA构建的图是单层的</li></ul><h2 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h2><p>为了满足GPU加速的要求，并行度要高、且召回率也要准确，构建的图得满足：</p><ol><li><p><strong>任意节点间的遍历能力</strong>：这是为了确保图中的所有节点都是相互可达的。如果一个图中存在某些节点无法从其他节点访问，那么这些孤立的节点在搜索过程中将永远不会被考虑，这可能导致搜索结果的不完整或不准确。确保所有节点都是相互可达的，有助于提高搜索算法的覆盖率和准确性。</p></li><li><p><strong>指定遍历次数内的节点访问数量</strong>：这个指标用来衡量从任一节点出发，在有限的步骤内能够探索到的节点的多样性和数量。在ANNS中，通常希望在较少的遍历步骤内能够访问到更多的节点，这样可以更快地找到可能的最近邻。如果一个节点在几步之内能够访问到很多其他节点，那么搜索算法的效率和召回率（即找到真正最近邻的概率）可能会更高。</p></li></ol><p>所以就涉及到了图构建过程中的优化目标：</p><ul><li><p><strong>强连通分量（Strong Connected Components, CC）</strong> 的个数<br>  通过计算图中的强连通分量数量来评估图中任意节点是否能够到达其他任意节点。强连通分量是图中的子图，其中每个节点都可以直接或间接地到达子图中的任何其他节点。</p><blockquote><p>A smaller number of strong CC are preferred because a larger number of CC can lead to more unreachable nodes starting from a search start node.  </p></blockquote></li><li><p><strong>平均 2 跳节点数（Average 2-hop Node Count）</strong>：<br>  这个指标衡量的是从任一节点在两次遍历内能够到达的节点数量，用以评估在特定搜索迭代步骤中可以探索的节点数量。</p></li></ul><h3 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h3><img src="/CARGA/image.png" class="" title="alt text"><p>CAGRA算法的构建训练过程，先初始化一个knn graph，然后优化其中的边关系。</p><ol><li>初始knn-graph创建：比较简单，这里实际上可以理论上依赖任何一种已有的算法，但在实现上选了IVF-PQ、和NN-Descent算法。这里就不过多展开了<blockquote><p>步骤一结束后，每个节点都有k个邻居节点，并且通常按距离排序</p></blockquote></li><li>基于<strong>rank</strong>的重排序：这里每个节点出边按照初始rank重新排序，并且过滤掉一些边 <img src="/CARGA/reorder.png" class="" title="alt text"><ul><li><p>左侧：来自节点X及其他相关边的初始排名。</p></li><li><p>中间：可能的两跳路径（XAB、XBC、XCD、XAC、XDC），根据方程3被分类为可绕路和不可绕路的。我们使用排名代替距离。<br>  $$<br>  (eX→Z, eZ→Y ) s.t. \max(wX→Z, wZ→Y ) &lt; wX→Y<br> $$</p><blockquote><p> 直接边就不是最优路径，可以被视为可绕路的</p></blockquote></li><li><p>右侧：连接到节点X的每个节点的可绕路路径数量。根据可绕路路径数量，从列表末尾开始丢弃边。</p></li></ul></li><li><strong>构建反向图</strong><br> 同样的思路构建反向图。 </li><li>融合两张图</li></ol><h2 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h2><p>之前的HNSW一类算法之所以不能满足GPU计算主要原因就是并行度不够，很难去发挥GPU多线程计算的优势。CAGRA不同之处在于在构图的时候尽可能保证了任意两点的可达性，在查找的时候放弃了按照最近路径找到目标节点的优化思路，而是通过提高吞吐量来尽可能覆盖尽可能多的点来提高召回率和GPU利用率。</p><p>这里需要特别提一点就是这里的buffer。其实是两部分的，前半部分top-M的，我猜测是有序的，后半部是候选访问区，不必一定保证有序。</p><img src="/CARGA/query.png" class="" title="alt text"><p>计算过程：</p><ol><li>随机选取E个节点，计算他们与 query 的距离，并存在 candidate buffer 中</li><li>在 top-M buffer（这里应该是上一轮的结果，初始阶段为空） 和 candidate buffer 中选取 top M 个结果存储在 Top-M buffer中  </li><li>在Top-M buffer中选取一个还没有被 traverse 的离 query 最近的节点  </li><li>选取与 Step 3 中选择的节点近邻的E个没有访问的节点，并计算他们与query的距离，然后存储在 Candidate buffer 中<br>一直计算到收敛（topM buffer全部是已访问状态）</li></ol><p>参考：</p><ol><li><a href="https://github.dev/facebookresearch/faiss">https://github.dev/facebookresearch/faiss</a></li><li><a href="https://arxiv.org/pdf/2308.15136">https://arxiv.org/pdf/2308.15136</a></li><li>kimi_chat</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ANNs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RAG系统构建_技术文档中句子嵌入的挑战</title>
      <link href="/RAG%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA-%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3%E4%B8%AD%E5%8F%A5%E5%AD%90%E5%B5%8C%E5%85%A5%E7%9A%84%E6%8C%91%E6%88%98/"/>
      <url>/RAG%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA-%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3%E4%B8%AD%E5%8F%A5%E5%AD%90%E5%B5%8C%E5%85%A5%E7%9A%84%E6%8C%91%E6%88%98/</url>
      
        <content type="html"><![CDATA[<p>这个是爱立信对RAG pipeline中 retrival阶段的一个实验报告。并得到的一些初步的实验结论。</p><ol><li>sentence embedding 计算的相似度随着文本切分长度增加逐渐变得不可信。</li></ol><p>  他们选取了 10,970条句子，计算了相互之间的余弦相似度。最终形成了下面的Kernel Density Estimate (KDE) 图。</p><p>  从图中可以看到不同句子长度的余弦相似性得分的分布。</p>  <img src="/RAG%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA-%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3%E4%B8%AD%E5%8F%A5%E5%AD%90%E5%B5%8C%E5%85%A5%E7%9A%84%E6%8C%91%E6%88%98/image.png" class="" title="The distribution of similarities across 10974 documents of various sizes split by number  of words in the document"><ol start="2"><li>Table 1 从论文中提供了对实验假设和观察结果的总结。这些假设和观察结果是基于对技术文档进行的检索增强型生成（RAG）系统实验。以下是对Table 1内容的介绍：</li></ol><table><thead><tr><th>Hyp Hypothesis (假设)</th><th>Observation (观察)</th><th>Support (支持样本数)</th></tr></thead><tbody><tr><td>H1 分割定义和定义词有助于查询</td><td>对于定义，使用定义词和定义分别进行检索可以获得更好的性能</td><td>22 of 30 queries (ID 2, 3)</td></tr><tr><td>H2 不应使用相似度分数来比较检索结果</td><td>观察到不同方法之间的相似度分数不可比，且正确答案的绝对值通常很小</td><td>24 of 30 queries (ID 2, 3)</td></tr><tr><td>H3 关键词的位置影响结果</td><td>关键词越靠近句首，检索准确度越高</td><td>25 of 30 queries (ID 1, 4, 5, 6)</td></tr><tr><td>H4 基于句子的相似度更好</td><td>基于句子和不同段落的相似度检索可以为生成器提供更详细的上下文</td><td>ID F1 - Table 2 (8 of 10 queries)</td></tr><tr><td>H5 生成器对基于句子的相似度</td><td>使用基于句子的相似度和基于段落的检索生成的答案更好</td><td>8 of 10 queries (App. Table 3 - ID F1)</td></tr><tr><td>H6 包含缩写词的定义表现不佳</td><td>生成的答案常常只是展开或提供缩写词，这并不有用</td><td>15 of 16 queries (App. Table 3 - ID F2, F3)</td></tr><tr><td>H7 检索段落的顺序对生成器结果的影响</td><td>实验中我们没有观察到检索段落的顺序对生成器结果产生影响</td><td>NA</td></tr></tbody></table><p>  这个表格展示了作者们在实验中提出的七个假设以及通过实验得到的观察结果。每个假设后面都列出了支持该假设的样本查询数量和具体ID。例如，假设H1表明，如果将定义和定义词分开进行检索，可以提高查询的性能，这一点在30个查询中的22个得到了验证（具体查询ID为2和3）。</p><p>  这些观察结果对于理解技术文档RAG系统的设计和改进至关重要，因为它们揭示了在实际应用中可能遇到的问题和有效的解决策略。</p><p>参考链接：</p><ol><li><a href="https://arxiv.org/pdf/2404.00657">https://arxiv.org/pdf/2404.00657</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bi-encoder vs Cross encoder? When to use which one?</title>
      <link href="/encoder-cross-bi/"/>
      <url>/encoder-cross-bi/</url>
      
        <content type="html"><![CDATA[<p>Bi-encoder和Cross-encoder是在自然语言理解任务模型的两种不同方法，在信息检索和相似性搜索二者的使用更为广泛。在LLM大火的今天，RAG的pipe line中这两个模块作为提升检索精度的模块更是备受瞩目。</p><img src="/encoder-cross-bi/compare.png" class="" title="Bi_vs_Cross-Encoder"><table><thead><tr><th></th><th>Bi-encoder</th><th>Cross-encoder</th></tr></thead><tbody><tr><td>架构</td><td>有两个<strong>独立</strong>的编码器 —— 一个用于编码输入的查询，另一个用于编码候选文档。这些编码器独立工作，为查询和每个文档生成嵌入表示。</td><td>查询和文档一起在<strong>单个</strong>编码器中处理。这意味着模型将查询和文档作为输入，并产生联合表示</td></tr><tr><td>训练方式</td><td>在训练期间，模型被训练以最大化查询与相关文档之间的相似性，同时最小化查询与不相关文档之间的相似性。训练通常使用对比损失函数进行。</td><td>与Bi-encoder类似，Cross-encoder被训练以最大化相关查询-文档对之间的相似性。但是，由于它们同时处理查询和文档，因此它们捕获了两者之间的交互。</td></tr><tr><td>使用</td><td>在推理时，模型独立计算查询与每个文档之间的相似性得分。相似性得分最高的文档被认为是最相关的。</td><td>Cross-encoder为每个查询-文档对生成单一的相似性得分，考虑了查询和文档嵌入之间的交互。得分最高的文档被认为是最相关的。</td></tr></tbody></table><h2 id="使用哪个："><a href="#使用哪个：" class="headerlink" title="使用哪个："></a>使用哪个：</h2><ul><li><p>Bi-encoder：当您拥有大规模数据集和计算资源时，使用Bi-encoder。由于相似性得分可以独立计算，它们在推理期间通常更快。它们适用于<strong>捕获查询和文档之间复杂交互不太关键</strong>的任务。</p></li><li><p>Cross-encoder：当捕获<strong>查询和文档之间的交互对于您的任务至关重要</strong>时，请选择Cross-encoder。它们在<strong>计算上更为密集</strong>，但可以在理解查询和文档之间的上下文或关系至关重要的场景中提供更好的性能。</p></li></ul><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><ol><li><p>为什么embedding 在RAG中常常失真？还需要rerank模型来做二次处理。</p><blockquote><p>虽然训练上二者都是度量学习的思路，但在推理判断阶段，还是有很大区别的。</p><ul><li>embedding 模型在获取embeding向量时，仅仅考虑了当前的文本，rerank模型则是把query跟相关的文档信息一起做了比较。</li><li>sentence embedding vector 本质上还是word的建模，无论pooling方式如何，重合的关键词越多，一般来说最终也越相近。rerank模型则是在最后学习一个映射，输出的就是0-1的相似性判断。简单来说就是rerank功能更明确，效果也更好。</li></ul></blockquote></li><li><p>embedding 模型和rerank模型是否只有训练架构上的区别？在模型结构上有没有偏好？</p><blockquote><p>当前绝大多数embeding模型都是BERT架构的，rerank模型多数时XLMRobertaForSequenceClassification</p></blockquote></li><li><p>如果一定要rerank模型，是不是可以在一定范围内牺牲embeding的召回精度？选用较小的embedding 模型，但是提高topK。</p></li><li><p>这两类模型的训练细节有哪些？</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【转载】一种编译期Map的实现</title>
      <link href="/compile-time-map/"/>
      <url>/compile-time-map/</url>
      
        <content type="html"><![CDATA[<p>本文来自：<a href="https://xuhuisun.com/post/c++-weekly-2-constexpr-map/">c++ weekly</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++ </tag>
            
            <tag> constexpr </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【转载】爬虫如何通过二维码登录知乎</title>
      <link href="/zhihu-login-qr-code/"/>
      <url>/zhihu-login-qr-code/</url>
      
        <content type="html"><![CDATA[<h1 id="登录过程分析"><a href="#登录过程分析" class="headerlink" title="登录过程分析"></a>登录过程分析</h1><p>先来到知乎提供二维码登陆的界面，利用F12开发工具，可查看请求这个二维码图片需要那些数据。</p><img src="/zhihu-login-qr-code/image.png" class="" title="alt text"><img src="/zhihu-login-qr-code/image-1.png" class="" title="alt text"><p>能看到是get请求，headers也很寻常，但多次刷新可发现请求的url地址有一部分在改变。寻找前面的文件，能找到这部分动态改变的值 </p><img src="/zhihu-login-qr-code/image-2.png" class="" title="alt text"><p>为了方便阐述，那就把image称为A文件，qrcode称为B文件。</p><p>这里就有了一个思路，先请求B文件，拿到token值以后，拼接成目的url，再去请求A文件</p><h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>于是我们从A迁移到了B</p><img src="/zhihu-login-qr-code/image-3.png" class="" title="alt text"><p>可见请求B文件的时候，headers字段是真的很多，但绝对不会所有都必要，这只能排除法了</p><p>首先看清楚了，是<strong>POST</strong>请求<br> <img src="/zhihu-login-qr-code/image-4.png" class="" title="alt text"></p><p>复制了所有headers，做一次post的请求，再看看状态码是不是201</p><blockquote><p>（为了避免请求被重定向，建议打印请求内容，或者关闭重定向，后面皆以打印内充处理不再单独提示）</p></blockquote><img src="/zhihu-login-qr-code/image-5.png" class="" title="alt text"><p>可以说很OK，然后就开始排除法，首先去掉的是最常用不到的。通过几轮排除下来，发现Cookie和User-Agent是必要的，既然需要用到cookie，我们就得维持会话，所以要实例化一个session对象了，实现如下： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session = requests.session()</span><br></pre></td></tr></table></figure><p>顺便也把Cookie分解了，看看需要哪些内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> headers[<span class="string">&quot;Cookie&quot;</span>].split(<span class="string">&quot;;&quot;</span>):</span><br><span class="line">    <span class="built_in">print</span>(item.split(<span class="string">&quot;=&quot;</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure> <img src="/zhihu-login-qr-code/image-6.png" class="" title="alt text"><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>那么如何让session对象持有完整的cookie呢？<br> <img src="/zhihu-login-qr-code/image-7.png" class="" title="alt text"></p><p>打开开发者工具，刷新页面，然后点击二维码登陆</p><img src="/zhihu-login-qr-code/image-8.png" class="" title="alt text"><p>我们可以看到第一次请求登陆界面的时候，请求是不带cookies的；而请求之后，按照响应体的要求，会设置对应的<code>xsrf</code>，<code>_zap</code>，<code>tgw_17</code>。前面我们知道需要6个，这里才三个肯定是不够的，所以继续找signin后面的文件，看看到底有什么猫腻在里头</p><p>于是在<code>udid</code>这个文件中，你会发现响应体要求设置<code>q_c1</code>，<code>d_c0</code>；也就是说，在成功请求这个文件之后，Cookie就包含这两个部分了<br> <img src="/zhihu-login-qr-code/image-9.png" class="" title="alt text"></p><p>照例复制下完整的headers，找到请求的url，以及请求方式（注意了！这里也是post），最后排除法，找到必要的部分</p><img src="/zhihu-login-qr-code/image-10.png" class="" title="alt text"><p>仍然是Cookie以及User-Agent</p><h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题 3"></a>问题 3</h2><p>还不够，现在我们的cookies还差<code>capsion_ticket</code>部分，所以继续撸</p><p>于是找到了<code>captcha?lang=cn</code>文件，它的响应体告诉浏览器，可以设置<code>capsion_ticket</code>了<br> <img src="/zhihu-login-qr-code/image-11.png" class="" title="alt text"></p><img src="/zhihu-login-qr-code/image-12.png" class="" title="alt text"><h2 id="开始代码"><a href="#开始代码" class="headerlink" title="开始代码"></a>开始代码</h2><p>有了这波分析，我们就可以开始动手敲代码了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">session = requests.session()</span><br><span class="line"></span><br><span class="line">HEADERS = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 \ (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一次请求，为了Cookie(_xsrf，_zap，tgw_17)</span></span><br><span class="line">session.get(url=<span class="string">&quot;https://www.zhihu.com/signin&quot;</span>, headers=HEADERS)</span><br><span class="line"><span class="comment"># 第二次请求，为了Cookie(q_c1，d_c0)</span></span><br><span class="line">session.post(url=<span class="string">&quot;https://www.zhihu.com/udid&quot;</span>, headers=HEADERS)</span><br><span class="line"><span class="comment"># 第三次请求，为了Cookie(capsion_ticket)</span></span><br><span class="line">session.get(url=<span class="string">&quot;https://www.zhihu.com/api/v3/oauth/captcha?lang=cn&quot;</span>, headers=HEADERS)</span><br><span class="line"><span class="comment"># 第四次请求，为了token，用于构建二维码图片的请求链接</span></span><br><span class="line">response = session.post(url=<span class="string">&quot;https://www.zhihu.com/api/v3/account/api/login/qrcode&quot;</span>, headers=HEADERS)</span><br><span class="line"><span class="comment"># print(response.json())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第五次请求，为了二维码图片</span></span><br><span class="line">url4QR = <span class="string">&quot;https://www.zhihu.com/api/v3/account/api/login/qrcode/&#123;0&#125;/image&quot;</span>.<span class="built_in">format</span>(response.json().get(<span class="string">&quot;token&quot;</span>))</span><br><span class="line"></span><br><span class="line">response = session.get(url=url4QR, headers=HEADERS)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;qr.jpg&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(response.content)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;【保存二维码成功】&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;【请求二维码图片错误】&quot;</span>)</span><br></pre></td></tr></table></figure><p>运行结果如下 </p><img src="/zhihu-login-qr-code/image-13.png" class="" title="alt text"><p>这时候以为扫描二维码就登陆成功了吗？然而没有<br> 我们扫描一下网页的二维码登陆一下试试，会发现在手机上点击确认登陆以后，请求知乎<a href="http://www.zhihu.com网页的时候,cookie又多了一个`z_c0/">www.zhihu.com网页的时候，Cookie又多了一个`z_c0</a> <code>  &#123;% asset_img "image-14.png" "alt text" %&#125; 晕！但是扶住墙，老规矩。可以看到距离知乎首页文件最近的一个</code>scan_info<code>文件，说了要设置</code>z_c0 &#96;<br> <img src="/zhihu-login-qr-code/image-15.png" class="" title="alt text"></p><p>于是在我们扫描二维码之后，应该先请求这个文件，再请求首页文件；查看请求的url，也能发现，这个文件也有一部分是动态的，而且正是之前获取的token<br> <img src="/zhihu-login-qr-code/image-17.png" class="" title="alt text"></p><p>为了确保我们成功登陆，可测试编辑页面，因为这个页面只有在登陆成功后可以访问，不然就会被重定向到登陆页面去<br> <img src="/zhihu-login-qr-code/image-18.png" class="" title="alt text"></p><p>添加代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 阻塞程序，给予用户扫描二维码的时间</span></span><br><span class="line"><span class="built_in">input</span>(<span class="string">&quot;请随便输入后回车&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求scan_info文件，并打印状态码</span></span><br><span class="line"><span class="built_in">print</span>(session.get(<span class="string">&quot;https://www.zhihu.com/api/v3/account/api/login/qrcode/&#123;0&#125;/scan_info&quot;</span>.<span class="built_in">format</span>(token), headers=HEADERS).status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求编辑页面</span></span><br><span class="line">response = session.get(<span class="string">&quot;https://www.zhihu.com/people/edit&quot;</span>, headers=HEADERS, allow_redirects=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;登陆成功&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(response.text[:<span class="number">10000</span>])</span><br></pre></td></tr></table></figure><p>成功</p>]]></content>
      
      
      <categories>
          
          <category> 转载 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TK 学习法</title>
      <link href="/tk-learn-method/"/>
      <url>/tk-learn-method/</url>
      
        <content type="html"><![CDATA[<p>这是TK分享的学习方法，这里收藏备份一下。重要的是<strong>思路</strong>。</p><blockquote><p>这两天论坛上又有人开始抱怨世风日下，大家都现实了，都不开放了，不交流了。对这种“月经贴”，我基本上已经习惯了，不过因为吃了粉皮炖鸡，心情比较好，于是就说了两句。</p></blockquote><blockquote><p>三四年前，当时我对人性的看法还不像现在这样。有几个人加了我的QQ，说想学Windows，我居然就好为人师起来，自不量力地教人学Windows。我很天真地把自己的经验告诉他们</p></blockquote><blockquote><p>一、先把Windows的帮助文件从头到尾看一遍。</p></blockquote><blockquote><p>二、在Windows目录下搜索*.txt、*.htm?、*.log、*.ini，把每一个文件内容都看一遍。</p></blockquote><blockquote><p>三、把注册表浏览一遍。</p></blockquote><blockquote><p>没有诀窍，也不用花钱买书。任何人把这三步做完之后，只要不是傻子，在Windows应用方面都可以非常熟练。并且如果想进一步学，也自然知道应该去看什么了。</p></blockquote><blockquote><p>结果甚至没有一个人能看完Windows帮助文件，看完三分之一的都没有，都说看不下去。我很奇怪，我看Windows的帮助文件就像看金庸小说一样愉快，怎么会有人觉得辛苦？</p></blockquote><blockquote><p>后来我想明白了：因为我爱她，而他们不爱她，只是想占有她而已。</p></blockquote><blockquote><p>他们要的不是交流，不是开放，甚至也不是想找个人“拜师”，他们想要的不是郭靖遇到的洪七公，而是虚竹遇到的无涯子。</p></blockquote><blockquote><p>再后来，一个偶然的机会，我看到了小四同学写的那篇《你尽力了么？》，才知道原来这不只是我一个人的看法。</p></blockquote><blockquote><p>这两天在家，在笔记本上折腾Linux，遇到了很多问题，我就把内核每一个编译选项的说明都细细看了一遍，反复编译上二十多遍——然后，所有问题的答案都找到了。显然，学Linux和学Windows的方法并没有什么不同。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 闲聊 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>端侧需要向量数据库吗</title>
      <link href="/vec-ondevice/"/>
      <url>/vec-ondevice/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近参与的向量数据的技术分析项目什么的基本告一段落了，简单总结一下，在Edge侧实现向量数据的应用场景以及一些问题挑战。</p></blockquote><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>问题：</p><ul><li>手机上需要不需要一个向量数据？</li><li>如果需要，需要什么样的数据库？支持哪些算法，有哪些约束？</li></ul><h2 id="是不是需要？"><a href="#是不是需要？" class="headerlink" title="是不是需要？"></a>是不是需要？</h2><p>对于第一个问题显然是成立的。近年来各大手机手机厂商在AI能力上的探索越来越深入，AI算法所涉及的业务范围也从最开始的camera场景的计算摄影逐渐扩散到更多的业务。从技术上讲主要有下面几个场景：</p><ul><li>多模态搜索</li><li>LLM大模型</li></ul><h3 id="多模搜索"><a href="#多模搜索" class="headerlink" title="多模搜索"></a>多模搜索</h3><p>多模搜索（图文搜索）业务源于CLIP模型提出和发展。CLIP模型如何去做的，这里就不展开了。通过CLIP模型，可以把图片和文本在同一语义空间内对齐，这就使得通过描述搜图变成可能了。相较于原始的基于Tag搜索，就极大地增强了可能性。</p><blockquote><p> 这里扩展一下，目前看起来，文档搜索反而在多模搜索以后得到应用，就有点奇奇怪怪了。</p></blockquote><p>考虑到未来算法演进，以及手机侧信息的数字化管理的需求，后面肯定是更多模态数据可以相互搜索。向量数据库在其中肯定也要发挥更大的作用。</p><h3 id="LLM大模型"><a href="#LLM大模型" class="headerlink" title="LLM大模型"></a>LLM大模型</h3><p>LLM模型大热，确实吸引了不少人的注意力。但是如果仅仅只能在云测跑，受限于隐私安全的考虑，个性化的需求就很难满足，而模型又很难在本地通过训练来更新知识信息。这就引入了端侧RAG框架的需求。根据当前RAG的技术演进程度来看，向量检索是Retriaval阶段必不可少的一环，特别是未来LLM到LMM演进上，更是如此。因为传统数据库并不是为了多模态数据设计的。</p><blockquote><p>至于其他场景，以后遇到再补充了。</p></blockquote><h2 id="需要什么样的向量数据库"><a href="#需要什么样的向量数据库" class="headerlink" title="需要什么样的向量数据库"></a>需要什么样的向量数据库</h2><p>手机侧不同于云侧，有下面的几个特点。</p><ul><li>ROM空间有限且珍贵，哪个应用（小而美）用着用着占用越来愈大了还容易引发舆情信息</li><li>RAM空间就更金贵了，不能随便说只服务你一个业务就可以</li><li>功耗电量消耗也是需要考虑的</li><li>还有上面业务场景决定，召回率、准确度的要求也是越高越好</li><li>删改实时性有要求，不能一直不加、也不能一直不删</li></ul><p><strong>简单说就是：资源要节省、准确度要高</strong>、时延倒是重要但没那么重要的部分了</p><h3 id="算法怎么选"><a href="#算法怎么选" class="headerlink" title="算法怎么选"></a>算法怎么选</h3><ul><li>从上面的约束可以看出，云侧用的多的NSW、HNSW等基于图的算法就不是很能满足要求了，构建、删除成本高、内存占用大。</li><li>PQ这类量化算法内存占用虽然低了，如果bit数选取太低了，掉点严重，能不用尽量别用</li><li>LSH、kNN-Tree这些，效果也不咋地</li><li>IVF-算法，牺牲一点时间，降低计算量和内存。可以考虑</li><li>NN降维、PCA降维等算法，可以考虑组合使用。</li></ul><blockquote><p>理论上来说，这里量化策略选择接近与NN模型的量化，底库向量等价于权重。但是还没有实验验证了。</p></blockquote><h2 id="向量数据库计算"><a href="#向量数据库计算" class="headerlink" title="向量数据库计算"></a>向量数据库计算</h2><p>既然需要了，那就可以来实现一个向量数据库了。下面就看下，向量数据计算的特点吧。</p><h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><p>传统意义上来看，检索就是一个向量和一个矩阵做一个相似度计算。是一个<code>sgemv</code> 的计算过程。属于一种计算密度比较低的计算场景，适合vector计算单元发挥作用。</p><p>在LLM交互的场景结果略有不同，从用户对话的角度来看，很多时候用户一次的问题是片面的，仅仅凭借这一个query向量，召回的相关信息不不太足的。这就涉及到了query rewrite，经过rewrite以后就成了一个multi-query 场景了，这样计算密度就大大增加。变成一个比较适合NPU计算的场景了。</p><blockquote><p>这里参考一下，langchain 的<a href="http://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever">MultiQueryRetriever</a> </p></blockquote><h3 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h3><p>这里把构建过程单独拿出来说，主要是构建算法会涉及到k-means聚类，这时候参与计算的向量就多了，计算密度也大了。</p><h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p>降维算法包括非NN类的，比如SVD、PCA这种，也有NN类的，比如VAE这类。</p><h3 id="向量计算库-需要什么样的ASIC"><a href="#向量计算库-需要什么样的ASIC" class="headerlink" title="向量计算库 需要什么样的ASIC"></a>向量计算库 需要什么样的ASIC</h3><ul><li>上面可以看出来，当前大多数调用频繁的场景来看，实际的计算密度并不高，gemv的计算走CPU的neon也足够了，只有在大计算量的情况下，走专用的计算单元才有优势。</li><li>不可否认的一点就是，专用计算单元在能效是有优势的</li><li>如果从计算时延上来，Android系统架构下，必须ION内存；从CPU到专用的计算单元之间也会引入额外的时延开销；这就要求走ASIC的话，需要在这中间做好文章，才好有优势</li></ul><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      
      
      <categories>
          
          <category> 闲聊 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ANN </tag>
            
            <tag> vecdb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>国内环境配置pyppeteer</title>
      <link href="/pyppeteer-install/"/>
      <url>/pyppeteer-install/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>pyppeteer 是 puppeteer 的 python 版本，实现了大部分接口，因为使用了异步await等关键字，需要 python3.6+，具体作用自行百度。</p><p>因初次运行默认需要从国外下载 chromium 到指定路径，不适合国内，所以写了这篇文章方便小伙伴们在国内进行配置。</p><p>附上官方文档，英语好的小伙伴们可自行配置。</p><p>windows下的安装和配置</p><p>1、使用豆瓣源安装pyppeteer：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.douban.com/simple/ pyppeteer</span><br></pre></td></tr></table></figure><p>2、添加环境变量，更改下载 chromium 的来源网站和执行路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PYPPETEER_DOWNLOAD_HOST，对应值为http://npm.taobao.org/mirrors</span><br></pre></td></tr></table></figure><p>也可以在<code>import pyppeteer</code>之前在代码中设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;PYPPETEER_DOWNLOAD_HOST&quot;</span>] = <span class="string">&quot;http://npm.taobao.org/mirrors&quot;</span></span><br></pre></td></tr></table></figure><p>3、在cmd终端进入 python&#x2F;ipython 环境，执行以下代码查看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyppeteer</span><br><span class="line"><span class="comment"># chromium执行目录</span></span><br><span class="line">pyppeteer.chromium_downloader.chromiumExecutable.get(<span class="string">&#x27;win64&#x27;</span>)</span><br><span class="line"><span class="comment"># 下载chromium的url地址</span></span><br><span class="line">pyppeteer.chromium_downloader.downloadURLs.get(<span class="string">&#x27;win64&#x27;</span>)</span><br></pre></td></tr></table></figure><p>正常情况下这里会打印出执行目录的地址和下载地址。</p><p>用下面的代码测试的话也会自动下载对应的浏览器文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> pyppeteer <span class="keyword">import</span> launch</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    browser = <span class="keyword">await</span> launch()</span><br><span class="line">    page = <span class="keyword">await</span> browser.newPage()</span><br><span class="line">    <span class="keyword">await</span> page.goto(<span class="string">&#x27;https://www.baidu.com/&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> page.screenshot(&#123;<span class="string">&#x27;path&#x27;</span>: <span class="string">&#x27;baidu.png&#x27;</span>&#125;)</span><br><span class="line">    <span class="keyword">await</span> browser.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">asyncio.get_event_loop().run_until_complete(main())</span><br></pre></td></tr></table></figure><p>但是实际并非如此，由于python包中的版本与实际镜像的版本偶尔会出现不一致的情况，会下载失败。<br>这时候可以看下远程镜像的归档有哪些版本，自己在代码中改成对应的即可。</p><p><a href="https://registry.npmmirror.com/binary.html?path=chromium-browser-snapshots/Win_x64/">https://registry.npmmirror.com/binary.html?path=chromium-browser-snapshots/Win_x64/</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>post</title>
      <link href="/python_await/"/>
      <url>/python_await/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MLX 框架浅析</title>
      <link href="/mlx-apple/"/>
      <url>/mlx-apple/</url>
      
        <content type="html"><![CDATA[<p>最近Apple 新发布了一个MLX的DL框架，这是继 ML Compute 可用于在 Mac 上进行 TensorFlow 模型的训练，PyTorch 在 M1芯片后可使用 Metal Performance Shaders (MPS) 作为GPU 加速的 PyTorch 机器学习模型训练之后，进一步的尝试。</p><p>与MLX同时开源的还有数据读取的框架MLX-data，以及最近大模型相关的一些Example代码，MLX-examples</p><h2 id="MLX特性"><a href="#MLX特性" class="headerlink" title="MLX特性"></a>MLX特性</h2><ul><li><p><strong>Familiar APIs</strong>: MLX的python API 设计基本上与numpy和Pytorch对齐，基础的数据结构array设置可以隐式的转换为numpy的 Array。高层次的API mlx.nn 和mlx.optimizers则基本与pytorch对齐，使用方式也基本一致。C++ 的API与python基本一致。 这对算法开发人员来说上手的成本较低，历史代码也比较好迁移和继承。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mlx.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> mlx.core <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure></li><li><p><strong>Composable function transformations</strong>:  JAX </p></li><li><p>MLX has composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.</p></li><li><p><strong>Lazy computation</strong>: 延迟计算，每部分的计算结果都是按需求值，也包括内存申请。</p></li><li><p><strong>Dynamic graph construction</strong>: 动态图构建，函数输入shape发生变化时，并不会触发编译，debug也很简单符合直觉。</p></li><li><p><strong>Multi-device</strong>: 多IP计算支持。当前支持CPU、GPU。因为ANE在Apple内部处于闭源的工具链，在这一现状没变化时，不会支持ANE。ref：<a href="https://github.com/ml-explore/mlx/issues/18#issuecomment-1846492294">link</a></p></li><li><p><strong>Unified memory</strong>: 最大的特点在于统一的内存模型，MLX 中的在共享内存上分配，跨IP计算时无需拷贝移动数据。</p></li></ul><h2 id="为什么还需要一个MLX"><a href="#为什么还需要一个MLX" class="headerlink" title="为什么还需要一个MLX"></a>为什么还需要一个MLX</h2><ul><li>Apple silicon first</li><li>Alternative Design and API</li><li>Simple, Flexible, Baggage-Free</li><li>More Exploration, More Diversity</li></ul><p><a href="https://github.com/ml-explore/mlx/issues/12">https://github.com/ml-explore/mlx/issues/12</a></p><h2 id="缺少哪些部分"><a href="#缺少哪些部分" class="headerlink" title="缺少哪些部分"></a>缺少哪些部分</h2><ol><li>图优化几近于无</li><li>序列化、反序列化功能</li><li>JIT 编译</li><li>INT量化</li></ol><h2 id="性能以及现状"><a href="#性能以及现状" class="headerlink" title="性能以及现状"></a>性能以及现状</h2><p>M2 Max</p><p>FP16 10 token&#x2F;s</p><img src="/mlx-apple/20231213220124.png" class=""><img src="/mlx-apple/20231213220841.png" class=""><h2 id="可能的-Roadmap"><a href="#可能的-Roadmap" class="headerlink" title="可能的 Roadmap"></a>可能的 Roadmap</h2><ol><li>有微调模型诉求的非算法人员。 比如lora、大模型的</li><li>基础模型开源、lora微调以后，通过core ml导出</li></ol>]]></content>
      
      
      <categories>
          
          <category> 竞分 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 竞分 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从向量数据库到 ANN search</title>
      <link href="/ANN-algo/"/>
      <url>/ANN-algo/</url>
      
        <content type="html"><![CDATA[<p>LLM的模型的爆火，意外带动了向量数据库的热度。之前名不见经传的一些初创公司也突然备受追捧。最近在分析端侧LLM场景的时候也分析了相关的一些向量数据库的相关知识。</p><h1 id="GPT的缺陷"><a href="#GPT的缺陷" class="headerlink" title="GPT的缺陷"></a>GPT的缺陷</h1><p>chatgpt在对话过程中表现出的能力包括了一定的上下文检索能力。但这个能力是基于LLM本身的上下文理解能力完成的，但受限于多数模型是基于kv cache结构的记忆历史对话信息的，kv cache size是有限的，在长程记忆上就天然存在一些缺陷。另一方面，在跨对话的场景下，这些上下文信息也不能使用。如果在端侧作为一个数字助理的场景来看，这显然是不合格的。</p><p>不同模型对于 token 的限制也不同，gpt-4 是 32K tokens 的限制，而目前最大的 token 限制是 Claude 模型的 100K，这意味可以输入大约 75000 字的上下文给 GPT，这也意味着 GPT 直接理解一部《哈利波特》的所有内容并回答相关问题。</p><p>这时候就可能觉得，那我把上下文信息一起发给LLM模型不就可以了。这就到了向量数据库的场景范畴了。在处理用户输入的时候，先去通过向量查找得到一些相关信息，一起输入给LLM模型，这样就可以正确回答相关信息了。</p><img src="/ANN-algo/Embedding.png" class=""><h1 id="ANN-Search"><a href="#ANN-Search" class="headerlink" title="ANN Search"></a>ANN Search</h1><p>向量数据库说起来并不是一个新鲜的技术了，在统计机器学习时代，做KNN算法的时候就已经在研究相关的技术了。这里就简要的介绍一下原理和算法。</p><p>ANN搜索（Approximate nearest neighbor）, 本质上是在很多稠密向量中，迅速找到目标点的临近点，并认为这认为是相似的节点，主要用于图像检索、高维检索。这里隐含了一个假设，映射在同一向量空间且距离相近的点，具有相似的语义特征，距离越近越相关，反之关系越远。</p><p>当前 ANN 搜索的方法大都是对空间进行切分，可以迅速找到子空间，并与子空间的数据进行计算。方法主要有基于树的方法、哈希方法、矢量量化、基于图的方法。</p><h2 id="基于树的方法"><a href="#基于树的方法" class="headerlink" title="基于树的方法"></a>基于树的方法</h2><p>基于树的方法最经典的就是KD树了。</p><img src="/ANN-algo/kd-tree.png" class=""><p><strong>构建</strong><br>KD树构建的过程就是迭代二分空间的过程<br>经典算法：<br>选择方差最大的维度,计算中位数点，作为划分点，分为左右子树，迭代上述过程, 直到空间上的点小于阈值</p><p><strong>检索</strong><br>因为ANN这个任务并不像关系数据库中那样需要精准的结果，而是得到其中Top-K的候选结果返回。<br>KD树的检索过程其实就是一个二叉树的回溯搜索过程：</p><ol><li>根据目标p的坐标和kd树的结点向下进行搜索，如果树的结点root是以数据集的维度d以来切分的，那么如果p的维度d坐标值小于root，则走左子结点，否则走右子结点。</li><li>到达叶子结点时，将其标记为已访问。如果S中不足k个点，则将该结点加入到S中；否则如果S不空且当前结点与p点的距离小于S中最长的距离，则用当前结点替换S中离p最远的点。</li><li>如果当前结点不是根节点，执行（a）；否则，结束算法。<br>  a.  回退到当前结点的父结点，此时的结点为当前结点（回退之后的结点）。将当前结点标记为已访问，执行（b）和（c）；如果当前结点已经被访过，再次执行（a）。<br>  b. 如果此时S中不足k个点，则将当前结点加入到S中；如果S中已有k个点，且当前结点与p点的距离小于S中最长距离，则用当前结点替换S中距离最远的点。<br>  c. 计算p点和当前结点切分线的距离。如果该距离大于等于S中距离p最远的距离并且S中已有k个点，执行步骤3；如果该距离小于S中最远的距离或S中没有k个点，从当前结点的另一子节点开始执行步骤1；如果当前结点没有另一子结点，执行步骤3。</li></ol><h2 id="LSH"><a href="#LSH" class="headerlink" title="LSH"></a>LSH</h2><p>LSH即 local sensitive hash，局部敏感哈希。不同于sha256、MD5这种避免碰撞的函数，这里我们选取hash函数的时候希望语义相近的向量可以映射到同一个桶里。这里有一个前提在的：</p><blockquote><p>原始数据空间中的两个相邻数据点通过相同的映射或投影变换（projection）后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个桶的概率很小。</p></blockquote><img src="/ANN-algo/lsh.png" class=""><p><strong>构建</strong></p><ol><li>选取一组的LSH hash functions；</li><li>将所有数据经过 LSH hash function 哈希到相应的hash码，所有hash数据构成了一个hash table；</li></ol><p><strong>检索</strong></p><ol><li>将查询数据经过LSH hash function哈希得到相应的编码；</li><li>通过hamming 距离计算query数据与底库数据的距离，返回最近邻的数据</li></ol><p>当然也有其他的实现方案，这里不一一列举了。</p><h2 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h2><p>LSH这一类算法给了一个很好的加速方案，既然在原始向量空间内存在计算慢的问题，那么把向量数据映射到一个新的空间是不是就可以加速了。量化的算法就是这么想的，float型数据内存占用大，计算慢，那映射到整型数据就快了。</p><h3 id="PQ量化"><a href="#PQ量化" class="headerlink" title="PQ量化"></a>PQ量化</h3><p>PQ量化，即乘积量化，这里的乘积指的是笛卡尔积。<br>如图所示。我们有一个向量库，里面有N个向量，每个向量D维。简要介绍一下算法原理：</p><img src="/ANN-algo/PQ.png" class=""><p>PQ 量化一般分为三个步骤：</p><p><strong>Train</strong></p><ol><li>向量切分：将D维向量切分成M组子向量，每个子向量 $\frac{D}{M}$ 维。</li><li>聚类：分别在每一组子向量集合内，做Kmeans聚类，在每个子向量空间中，产生K个聚类中心。<ul><li>每个聚类中心就是一个 $\frac{D}{M}$ 维子向量，由一个id来表示，叫做clusterid。</li><li>一个子空间中所有的clusterid，构造了一个属于当前子空间的codebook。对于当前向量库，就有M个codebook。</li><li>这M个codebook所能表示的样本量级就是 $K^M$，也就是 M个codebook的笛卡尔积。</li></ul></li></ol><p><strong>建库</strong><br>对于子向量空间中的N个子向量样本，在完成Kmeans聚类之后，用这个聚类中心的clusterid来代表这个子向量。这就是构建底库的过程。</p><p>原本我们的向量库的大小为 $N\times D\times 32bit$，压缩后，clusterid按照8bit来算的话，那就是 $N\times M * 8bit $，相比压缩前少了很多。</p><p><strong>查找</strong><br>这里查找的过程存在两种方式：SDC和ADC</p><img src="/ANN-algo/SDC_ADC.png" class=""><p><strong>SDC</strong><br>S&#x3D;symmetric，对称的。如图symmetric case。图中x就是query检索向量，y就是向量库里面的向量(注意，y已经是量化过了的，就是上文中说的那个用数字id替代向量)。那么如何计算x与y的距离呢？</p><ul><li>首先，计算q(x)，拿到x对应的聚类中心；同样的，计算q(y)，拿到y对应的聚类中心。</li><li>q(x)和q(y)就是两个完整的子向量，我们计算这两个向量的距离，便是当前子空间下的距离。</li></ul><p>为什么名字叫symmetric呢？因为他俩都是用对应的聚类中心来计算距离，所以是对称的。<br>优点:</p><ul><li>两两聚类中心之间的距离，可以离线就计算好，在线直接查表，提升了在线query的效率。</li></ul><p>缺点：</p><ul><li>误差也比ADC来的大，因为有x和q(x)，y和q(y)两个量化误差。</li></ul><p><strong>ADC</strong><br>A&#x3D;asymmetric，不对称的。上文中讲了对称是因为SDC都用了对应的聚类中心。那么ADC，就只有向量库中的y使用了聚类中心，而query向量x没有。那么，计算距离的时候，计算的就是x和q(y)的距离了。ADC的精确度更高，因为只有y和q(y)这一个量化误差；当然必须要在线计算(x是用户请求带过来的)，计算速度不如SDC。</p><p><strong>计算过程</strong></p><p>将每一个子空间下的所有距离的平方相加再开根号，就是最终的X跟Y的距离了(就是使用每个子空间的向量距离进行了一次欧氏距离计算)。</p><h3 id="SQ量化"><a href="#SQ量化" class="headerlink" title="SQ量化"></a>SQ量化</h3><p>SQ量化，又叫标量量化。是按照向量维度统计min-max最值，然后将每一维向量归一化指定bit数整数的量化方式。</p><img src="/ANN-algo/SQ.jpg" class=""><p>基本原理如上图所示。</p><h2 id="IVF类方法"><a href="#IVF类方法" class="headerlink" title="IVF类方法"></a>IVF类方法</h2><p>上面讲的量化算法，仅仅并没有解决全库计算的问题，虽然数据上做了压缩，如果数据量一大，计算量还是很大。如果可以只计算最相关的一部分，是不是就可以进一步减少了呢。这就是IVF算法的思路。</p><img src="/ANN-algo/IVF.jpg" class=""><p>概括一下：<br>IVF主要利用倒排的思想保存每个聚类中心下的向量(id，vector)，每次查询向量的时候找到最近的几个中心，分别搜索这几个中心下的向量。通过减小搜索范围，大大提升搜索效率。</p><p>这里额外补充一点：</p><ul><li>IVF跟PQ结合的时候，IVF的聚类中心里面向量按照PQ量化的聚类时，我们将不会在样本上直接做PQ量化，而是对样本Y和聚类中心q(Y)的残差向量(向量减法，Y-q(Y))做PQ量化。</li></ul><h2 id="基于图的方法"><a href="#基于图的方法" class="headerlink" title="基于图的方法"></a>基于图的方法</h2><p>让我们重新回顾一下ANN这个任务：</p><img src="/ANN-algo/points.png" class=""><p>已有的向量数据库内容就是图中的点，ANN的任务就是对给定一个点找到距离最近的点。那么如果每个点都知道离自己近的点，那么是不是就可以沿着这个连接线找到相近的点了。这样就避免了与所有数据计算距离。这就是基于图算法出发点。</p><h3 id="NSW"><a href="#NSW" class="headerlink" title="NSW"></a>NSW</h3><p>NSW（navigate small world）,漫游小世界算法。对于每个新的传入元素，我们从结构中找到其最近邻居的集合（近似的 Delaunay 图， 就是上面的右图）。该集合连接到元素。随着越来越多的元素被插入到结构中，以前用作短距离边现在变成长距离边，形成可导航的小世界。</p><img src="/ANN-algo/NSW.png" class=""><p>圆（顶点）是度量空间中的数据，黑边是近似的 Delaunay 图，红边是用于对数缩放的长距离边。箭头显示从入口点到查询的贪心算法的示例路径（显示为绿色）。</p><p>图中的边有两个不同的目的：</p><ul><li>Short-range edges，用作贪婪搜索算法所需的近似 Delaunay 图。</li><li>Long-range edges，用于贪婪搜索的对数缩放。负责构造图形的可导航小世界（NSW）属性。</li></ul><p><strong>NSW查找步骤</strong></p><ol><li>随机选一个点作为初始进入点，建立空废弃表g和动态列表c，g是变长的列表，c是定长为s的列表（s&gt;m）,将初始点放入动态列表c（附上初始点和待查找q的距离信息），制作动态列表的影子列表c’。</li><li>对动态列表c中的所有点并行找出其“友点”，查看这些“友点”是否存储在废弃表g中，如果存在，则丢弃，如不存在，将这些 剩余“友点”记录在废弃列表g中（以免后续重复查找，走冤枉路）。</li><li>并行计算这些剩余“友点”距离待查找点q的距离，将这些点及其各自的距离信息放入c。</li><li>对动态列表c去重，然后按距离排序（升序），储存前s个点及其距离信息。</li><li>查看动态列表c和c’是否一样，如果一样，结束本次查找，返回动态列表中前m个结果。如果不一样，将c’的内容更新为c的 内容，执行第2步。</li></ol><p>NSW有什么问题呢：</p><ul><li>先插入的点构建的边，大都是长边；后插入的大都是短边。边的的连接关系不是很均衡。实际搜索的时候优化空间还比较大。</li></ul><h3 id="HNSW"><a href="#HNSW" class="headerlink" title="HNSW"></a>HNSW</h3><p>HNSW（Hierarchical Navigable Small World）是对 NSW 的一种改进。HNSW 借鉴了跳表的思想，根据连接的长度（距离）将连接划分为不同的层，然后就可以在多层图中进行搜索。在这种结构中，搜索从较长的连接（上层）开始，贪婪地遍历所有元素直到达到局部最小值，之后再切换到较短的连接（下层），然后重复该过程，如下图所示：</p><img src="/ANN-algo/HNSW.jpg" class=""><p>利用这种结构可以将原来 NSW 的多重对数（Polylogarithmic）计算复杂度降低至对数（Logarithmic）复杂度。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ANN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>L1 data 缓存为什么一般只有32K或者64K</title>
      <link href="/L1-cache-size/"/>
      <url>/L1-cache-size/</url>
      
        <content type="html"><![CDATA[<p>L1 data缓存为什么一般只有32K或者64K？为什么不能更大一点？更大不是更好吗？</p><p>至少有这么两个原因。L1缓存因为会频繁被访问，所以优化目标是hit time，缓存size越大，hit time越长。另外现代CPU普遍采用virtually index physically tagged（VIPT）的L1缓存，所以L1数据缓存的大小实际上就是page size * associativity。譬如linux-x86上page size一般是4K，那L1d缓存每一个way就只能放4K大小的数据，想缓存总大小大一点就得增加associativity，譬如如果associativity是8，L1d就能是32K。但是associativity太大也会导致hit time上去。再譬如像Mac OS上page size是16K，L1d缓存就能做得更大一点。</p><p>注：实际VIPT做缓存查找时，虚拟地址的部分就是页表项，所以实际上虚拟地址部分就对应了page size。为什么跟associativity有关，因为associativity决定了一个页表项的虚拟地址可以映射到几个cacheline，为了最大化利用associativity也就是page size*associativity</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 体系结构 </tag>
            
            <tag> CPU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ndk std_thread 获取pid</title>
      <link href="/ndk-pid/"/>
      <url>/ndk-pid/</url>
      
        <content type="html"><![CDATA[<p>最近在解决tvm绑核问题时，发现android下绑核只有<code>sched_setaffinity</code>函数，这导致无法使用标准库中的<code>td::thread::native_handle_type thread</code> 进行绑核操作。虽然在ndk 21以上的版本提供了<code>pthread_gettid_np</code>函数获取线程相应的pid，但在较低版本中，还是没办法直接使用。</p><p>看下ndk 中 std 标准库上thread 的实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_LIBCPP_TYPE_VIS</span> thread</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">__libcpp_thread_t</span> __t_;</span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">typedef</span> __thread_id id;</span><br><span class="line">    <span class="keyword">typedef</span> <span class="type">__libcpp_thread_t</span> native_handle_type;</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">pthread_t</span> <span class="type">__libcpp_thread_t</span>;</span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">pthread_t</span>;</span><br></pre></td></tr></table></figure><p>上面可以看出，在ndk的实现中<code>native_handle_type</code> 等价于<code>pthread_t</code>, 再根据<code>pthread_gettid_np</code>的实现，可以发现 ，<code>pthread_t</code> 其实就是<code>pthread_internal_t</code>的地址。在<code>pthread_internal_t</code>中保存了线程的<code>tid</code> </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">pid_t</span> <span class="title">pthread_gettid_np</span><span class="params">(<span class="type">pthread_t</span> t)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> __pthread_internal_gettid(t, <span class="string">&quot;pthread_gettid_np&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> __pthread_internal_gettid(<span class="type">pthread_t</span> thread_id, <span class="type">const</span> <span class="type">char</span>* caller) &#123;</span><br><span class="line"><span class="type">pthread_internal_t</span>* thread = __pthread_internal_find(thread_id, caller);</span><br><span class="line"><span class="keyword">return</span> thread ? thread-&gt;tid : <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span>* next;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">pthread_internal_t</span>* prev;</span><br><span class="line"></span><br><span class="line"><span class="type">pthread_attr_t</span> attr;</span><br><span class="line"></span><br><span class="line"><span class="type">pid_t</span> tid;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> allocated_on_heap;</span><br><span class="line"></span><br><span class="line"><span class="type">pthread_cond_t</span> join_cond;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> join_count;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span>* return_value;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> internal_flags;</span><br><span class="line"></span><br><span class="line"><span class="type">__pthread_cleanup_t</span>* cleanup_stack;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span>** tls; <span class="comment">/* thread-local storage area */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">* The dynamic linker implements dlerror(3), which makes it hard for us to implement this</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">* per-thread buffer by simply using malloc(3) and free(3).</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __BIONIC_DLERROR_BUFFER_SIZE 512</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> dlerror_buffer[__BIONIC_DLERROR_BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">&#125; <span class="type">pthread_internal_t</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
            <tag> NDK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解LLM——LLM&amp;&amp; SD 基本概念</title>
      <link href="/LLM_SD_Basic/"/>
      <url>/LLM_SD_Basic/</url>
      
        <content type="html"><![CDATA[<h2 id="Causual-LM"><a href="#Causual-LM" class="headerlink" title="Causual LM"></a>Causual LM</h2><p>这里以llama模型为例，通常在执行用户输入之前会有一个[[文章&#x2F;LM basic知识#Prefill]]的过程。然后根据用户promts 得到输出。</p><img src="/LLM_SD_Basic/2462804-20230609220042409-2086756901.png" class=""><h3 id="Perfix-LM"><a href="#Perfix-LM" class="headerlink" title="Perfix LM"></a>Perfix LM</h3><p>这里以GLM为例介绍，展示了基本的流程。</p><img src="/LLM_SD_Basic/2462804-20230609220056534-615175021.png" class=""><h2 id="prefix-LM和causal-LM的区别"><a href="#prefix-LM和causal-LM的区别" class="headerlink" title="prefix LM和causal LM的区别"></a>prefix LM和causal LM的区别</h2><p>attention mask不同，prefix LM的prefix部分的token互相能看到，causal LM严格遵守只有后面的token才能看到前面的token的规则。</p><h2 id="Prefill"><a href="#Prefill" class="headerlink" title="Prefill"></a>Prefill</h2><p>对于causual LM，在正式推理前，需要一部分前置输入，这个过程就是Prefill。主要目的是产生 kv cache</p><blockquote><p>the prefill stage which takes a prompt sequence to generate the key-value cache (KV cache) for each transformer layer of the LLM</p></blockquote><p><strong>prefill phase</strong></p><p>$$<br>x^i_K &#x3D; x^i · w^i_K; x^i_V &#x3D; x^i · w^i_V<br>$$</p><p>$$<br>x^i_Q &#x3D; x^i · w^i_Q<br>$$<br>$$<br>x^i_{Out} &#x3D; fSoftmax(\frac{x^i_Q (x^i_K)^T}{\sqrt{h}}) · x^i_V · w^i_O + x^i \</p><p>$$</p><p>$$<br>x^(i+1) &#x3D; frelu(x^i_{out} ·w_1)·w_2+x^i_{out}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> SD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解LLM —— LoRA</title>
      <link href="/LoRA/"/>
      <url>/LoRA/</url>
      
        <content type="html"><![CDATA[<ul><li>论文链接：<a href="https://arxiv.org/abs/2106.09685">link</a></li><li>code: <a href="https://github.com/microsoft/LoRA">github</a></li></ul><h2 id="什么是LoRA"><a href="#什么是LoRA" class="headerlink" title="什么是LoRA"></a>什么是LoRA</h2><p>LoRA，英文全称<strong>L</strong>ow-<strong>R</strong>ank <strong>A</strong>daptation of Large Language Models，直译为大语言模型的低阶适应，是一种PEFT（参数高效性微调方法），这是微软的研究人员为了解决大语言模型微调而开发的一项技术。当然除了LoRA，参数高效性微调方法中实现最简单的方法还是Prompt tuning，固定模型前馈层参数，仅仅更新部分embedding参数即可实现低成本微调大模型，建议可从Prompt tuning开始学起。</p><p>LoRA的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型微调类似的效果</p><img src="/LoRA/2462804-20230609214112382-1836386385.png" class=""><h2 id="why-works"><a href="#why-works" class="headerlink" title="why works"></a>why works</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>给定一个预训练模型$P_{\Phi}(y|x)$ , fine tuning 的过程可以表示为<br>$$<br>\max_{\Phi}\sum_{x,y\in Z} \sum_{t&#x3D;1}^{|y|} {log(P_{\Phi}(y_t|x,y&lt;t))}<br>$$<br>对于fine tuning前后参数变化，其实就是<br>$$<br>\Phi &#x3D; \Phi_0+\Delta \Phi<br>$$<br>这种方案有一个缺点，对不同的下游任务，$\Delta \Phi$ 需要训练，而且$\Delta \Phi$ 的参数维度跟$\Phi$一样大，如果是GPT-3的话参数量要175B了。<br>如果$\Delta \Phi$ 够小，只调整$\Delta \Phi$ 这部分参数是不是就可以减少资源使用了。所以问题可以表示为<br>$$<br>\max_{\Phi}\sum_{x,y\in Z} \sum_{t&#x3D;1}^{|y|} {log(P_{\Phi_0+\Delta \Phi(\Theta)}(y_t|x,y&lt;t))}<br>$$</p><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>对于NN模型来说，权重都是满秩的。但是对于特定任务来说，</p><blockquote><p>预训练的语言模型具有较低的“固有维度”，尽管随机投影到较小的子空间，但仍然可以有效地学习<br>the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace</p></blockquote><p>基于此，假设与训练的LLM也具有这个性质，finetuning 的过程中也有一个低秩的性质。</p><p>对于权重  $W_0 \in \mathbb{R}^{d\times k}$ ,权重更新可以表示为 $W_0+\Delta W$ ,考虑低秩分解，即为$W_0+\Delta W &#x3D; W_0+BA$ , 其中$B \in \mathbb{R}^{d\times r}$, $A\in \mathbb{R}^{r\times k}$ , $r &lt;&lt; \min(d,k)$<br>则：<br>$$<br>h&#x3D;W_0x+\Delta Wx&#x3D;W_0x+BAx<br>$$</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="huggingface"><a href="#huggingface" class="headerlink" title="huggingface"></a>huggingface</h3><ul><li>code <a href="https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora.py">link</a></li></ul><p><a href="https://spaces.ac.cn/archives/9590">梯度视角下的lora</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TVM－MLC LLM 调优方案</title>
      <link href="/mlc-llm/"/>
      <url>/mlc-llm/</url>
      
        <content type="html"><![CDATA[<p>LLM 等GPT大模型大火以后,TVM社区推出了自己的部署方案，支持Llama，Vicuna，Dolly等模型在iOS、Android、GPU、浏览器等平台上部署运行。</p><p><a href="https://github.com/mlc-ai/mlc-llm">https://github.com/mlc-ai/mlc-llm</a></p><p>本文在之前作者介绍的基础上,简要介绍一下mlc的调优部署方案。</p><h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h2><p>在正式介绍TVM mlc.ai部署LLM方案之前，首先简要介绍一下当前主流LLM的一个工作流程。</p><img src="/mlc-llm/2462804-20230621222850510-751335110.png" class=""><blockquote><p>需要说明一点的是，上图中的prefill跟Decode指的的同一个模型，只是输入的shape存在差异。</p></blockquote><p>这里的示意图省略了很多，只是大致描述一下pipeline。<br>在处理用户输入时，此时长度大小是不能确定的，这时候是完全的是一个完全的动态shape的。但在decode过程中由于是token by token的，这时候网络中的中除了kv cache相关几个部分，其他大多数的操作都是固定shape的，就可以用已有的算法调优了。</p><h2 id="MLC-AI-部署调优方案"><a href="#MLC-AI-部署调优方案" class="headerlink" title="MLC.AI 部署调优方案"></a>MLC.AI 部署调优方案</h2><p>以下以RedPajama3B模型的tuning跟build过程介绍一下mlc的方案。</p><h3 id="pipeline-组成"><a href="#pipeline-组成" class="headerlink" title="pipeline 组成"></a>pipeline 组成</h3><p>在已经支持的几个模型里面均有<code>get_model</code> 这个函数，在这个函数里面会创建下面4个IRModel。</p><ul><li>encoding_func</li><li>decoding_func</li><li>create_kv_cache_func</li><li>create_softmax_func</li><li>create_metadata_func</li></ul><p><strong>encoding_func</strong><br>这对应了上图中的prefill过程，在每次用户输入后调用。由于用户输入的不确定性，所以这个过程基本上都是动态shape的，很难确定到底输入是多大，也不适合搜索调优。</p><p><strong>decoding_func</strong><br>这是上图中decode过程的一部分，因为这个过程是token by token的，在计算过程中大部分的计算是固定shape的。</p><p><strong>create kv cache func</strong><br>这里是直接调用的<code>relax.vm</code>中的函数，创建的是kv cache的存储相关。</p><p><strong>create softmax func</strong><br>这个也是解码过程的一部分，确切的说是采样过程中计算的一部分</p><p>** create_metadata_func **<br>模型的meta信息，比如<code>model_name</code>、<code>stop_tokens</code>等</p><h3 id="部署优化"><a href="#部署优化" class="headerlink" title="部署优化"></a>部署优化</h3><p>构建完以后，就进入到优化的阶段了。下面根据build.py过程描述一下过程。</p><ol><li><p>API构图构建了相关的模型，读取权重</p></li><li><p>量化</p></li><li><p>优化PASS</p><ol><li>FuseTransposeMatmul</li><li>FuseDecodeMatmulEwise</li><li>DeadCodeElimination</li><li>LiftTransformParams</li><li>split_transform_deploy_mod</li></ol></li><li><p>Codegen 生成代码</p><ol><li>DispatchTIROperatorAdreno&#x2F;DispatchTIROperator&#x2F;DefaultGPUSchedule 手动优化的sch</li><li>MetaScheduleApplyDatabase搜索的log生成固定shape的sch</li></ol></li></ol><h3 id="Tuning"><a href="#Tuning" class="headerlink" title="Tuning"></a>Tuning</h3><p>在MLC-LLM的代码仓里面已经提供了tuning的脚本，有一点需要先做一下，先调用build.py的文件，把静态shape的相关的函数分离出来。就得到了tuning文件中需要的<code>mod_tir_static.py</code></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TVM 源码阅读PASS — VectorizeLoop</title>
      <link href="/VectorizeLoop/"/>
      <url>/VectorizeLoop/</url>
      
        <content type="html"><![CDATA[<p>VectorizeLoop这个PASS就是对标记为<code>ForKind::kVectorized</code>的<code>For</code>循环做向量化处理，并对For循环中的语句涉及到的变量，替换为<code>Ramp</code>，以便于在Codegen的过程中生成相关的向量化运算的指令。</p><p>VectorizeLoop这个PASS的入口函数如下，只有在打开<code>enable_vectorize=true</code>的情况下载才会被启用，否则<code>VectorizeSkipper</code>会把<code>ForKind::kVectorized</code>的<code>For</code>循环替换为普通循环。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Pass <span class="title">VectorizeLoop</span><span class="params">(<span class="type">bool</span> enable_vectorize)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> pass_func = [=](PrimFunc f, IRModule m, PassContext ctx) &#123;</span><br><span class="line">    <span class="keyword">auto</span>* n = f.<span class="built_in">CopyOnWrite</span>();</span><br><span class="line">    <span class="keyword">if</span> (enable_vectorize) &#123;</span><br><span class="line">      n-&gt;body = <span class="built_in">LoopVectorizer</span>()(std::<span class="built_in">move</span>(n-&gt;body));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      n-&gt;body = <span class="built_in">VectorizeSkipper</span>()(std::<span class="built_in">move</span>(n-&gt;body));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> f;</span><br><span class="line">  &#125;;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">CreatePrimFuncPass</span>(pass_func, <span class="number">0</span>, <span class="string">&quot;tir.VectorizeLoop&quot;</span>, &#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面就以UT中的几个例子，介绍一下源码实现。</p><h2 id="vectorize-loop"><a href="#vectorize-loop" class="headerlink" title="vectorize_loop"></a>vectorize_loop</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dtype = <span class="string">&quot;int64&quot;</span></span><br><span class="line">n = te.var(<span class="string">&quot;n&quot;</span>)</span><br><span class="line">ib = tvm.tir.ir_builder.create()</span><br><span class="line">A = ib.pointer(<span class="string">&quot;float32&quot;</span>, name=<span class="string">&quot;A&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ib.for_range(<span class="number">0</span>, n) <span class="keyword">as</span> i:</span><br><span class="line"> <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, <span class="number">4</span>, kind=<span class="string">&quot;vectorize&quot;</span>) <span class="keyword">as</span> j:</span><br><span class="line">     A[i*<span class="number">4</span>+j] += tvm.tir.const(<span class="number">1</span>, A.dtype)</span><br><span class="line">stmt = ib.get()</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">isinstance</span>(stmt.body, tvm.tir.For)</span><br><span class="line">mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([A, n], stmt))</span><br><span class="line">stmt = tvm.tir.transform.VectorizeLoop()(mod)[<span class="string">&quot;main&quot;</span>].body</span><br></pre></td></tr></table></figure><p>上面的这个代码完成的是，向量加法，长度为4n的向量A，对每个元素+1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before</span></span><br><span class="line"><span class="keyword">for</span> (i, <span class="number">0</span>, n) &#123;</span><br><span class="line">  vectorized (j, <span class="number">0</span>, <span class="number">4</span>) &#123;</span><br><span class="line">    A[((i*<span class="number">4</span>) + j)] = (A[((i*<span class="number">4</span>) + j)] + 1f)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># after</span></span><br><span class="line"><span class="keyword">for</span> (i, <span class="number">0</span>, n) &#123;</span><br><span class="line">  A[ramp((i*<span class="number">4</span>), <span class="number">1</span>, <span class="number">4</span>)] = (A[ramp((i*<span class="number">4</span>), <span class="number">1</span>, <span class="number">4</span>)] + x4(1f))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到在经过<code>VectorizeLoop</code>的PASS以后，内层的循环消掉了，替换成为了一个Ramp的向量指令，这个在CPU中会被替换为SIMD指令（neon，AVX等）</p><h4 id="PASS流程"><a href="#PASS流程" class="headerlink" title="PASS流程"></a>PASS流程</h4><p>在向量化的处理的PASS中是在LoopVectorizer中处理的，处理For循环部分。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LoopVectorizer</span> : <span class="keyword">public</span> StmtMutator &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function">Stmt <span class="title">VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span> <span class="keyword">final</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (op-&gt;kind == ForKind::kVectorized) &#123;</span><br><span class="line">      <span class="built_in">ICHECK</span>(<span class="built_in">is_zero</span>(op-&gt;min));</span><br><span class="line">      <span class="keyword">auto</span>* extent_as_int = op-&gt;extent.<span class="built_in">as</span>&lt;IntImmNode&gt;();</span><br><span class="line">      <span class="keyword">if</span> (!extent_as_int || extent_as_int-&gt;value &lt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;Failed to vectorize loop with extent &quot;</span> &lt;&lt; op-&gt;extent;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">Vectorizer</span>(op-&gt;loop_var, <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(extent_as_int-&gt;value))(op-&gt;body);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> StmtMutator::<span class="built_in">VisitStmt_</span>(op);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>当遇到需要向量化的节点时，首先记录循环变量和范围，这个在后续替换相应的Load和Store操作为Ramp时用到。然后就到了Vectorizer部分，遍历For循环体，修改相应的stmt。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Vectorizer</span>(Var var, <span class="type">int</span> var_lanes) : <span class="built_in">var_</span>(var), <span class="built_in">var_lanes_</span>(var_lanes) &#123;</span><br><span class="line">    ramp_ = <span class="built_in">Ramp</span>(<span class="number">0</span>, <span class="number">1</span>, var_lanes);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Vectorizer中对不同的<code>PrimExpr</code>、<code>Stmt</code>做了重载。这里不逐一介绍，就以上面的向量加计算，介绍一下用到的函数以及流程。</p><p>首先看一下这里的上面sch的For的循环内的计算逻辑：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A[((i*<span class="number">4</span>) + j)] = (A[((i*<span class="number">4</span>) + j)] + <span class="number">1f</span>)</span><br></pre></td></tr></table></figure><p>因为TVM中，Stmt的表达可以视为一个DSL的语言，访问的时候也是按照深度优先的策略遍历的AST，这里把上面的计算过程简单表示为一个AST的语法树，然后再分析一下流程中调用的各个函数是如何处理的。</p><img src="/VectorizeLoop/2462804-20230624144328795-2055285024.png" class=""><p>从上面的AST的示意图可以看出来，对于上面的sch，依次访问了<code>BufferStoreNode</code>、<code>Add</code> <code>Mul</code>、<code>BufferLoadNode</code> 等。这里就以这几个Node的处理介绍一下向量化的过程。</p><p>所谓向量化的过程就是把这个标记为<code>kVectorized</code>的标量循环操作映射到向量化的操作，对于上面的例子来说就是把所有关于<code>j</code>的访问映射为RampNode，以便于后续处理可以正确生成相应的指令。</p><h5 id="BufferStoreNode"><a href="#BufferStoreNode" class="headerlink" title="BufferStoreNode"></a>BufferStoreNode</h5><p><code>BufferStoreNode</code>中有三部分：</p><ul><li>buffer——写入的buffer</li><li>value——待写入的值或者表达式</li><li>indices——写入buffer的坐标<br>这里的目的就是修改<code>value</code>和<code>indices</code>中的内容。<br>对于<code>indices</code>，是在这里完成的。最终通过<code>MapHelper</code>依次访问了<code>indices</code>的表达式。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> fmutate = [<span class="keyword">this</span>](<span class="type">const</span> PrimExpr&amp; index) &#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(index); &#125;;</span><br><span class="line">Array&lt;PrimExpr&gt; indices = op-&gt;indices.<span class="built_in">Map</span>(fmutate);</span><br></pre></td></tr></table></figure><p>对于<code>value</code> 则是直接遍历。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr value = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;value);</span><br></pre></td></tr></table></figure><h5 id="AddNode"><a href="#AddNode" class="headerlink" title="AddNode"></a>AddNode</h5><p>对于<code>AddNode</code>和<code>SubNode</code> 都会走到<code>AddSubVec</code>这个模板函数。<br>这个函数里面首先会遍历左右表达式，</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr a = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;a);</span><br><span class="line">PrimExpr b = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;b);</span><br><span class="line"><span class="keyword">if</span> (a.<span class="built_in">same_as</span>(op-&gt;a) &amp;&amp; b.<span class="built_in">same_as</span>(op-&gt;b)) &#123;</span><br><span class="line"> <span class="keyword">return</span> <span class="built_in">GetRef</span>&lt;PrimExpr&gt;(op);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="type">int</span> lanes = std::<span class="built_in">max</span>(a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>(), b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>());</span><br><span class="line"><span class="keyword">if</span> (lanes != <span class="number">1</span>) &#123;</span><br><span class="line"> <span class="type">const</span> RampNode* b_ramp = b.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="type">const</span> RampNode* a_ramp = a.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="keyword">if</span> (a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; b_ramp) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(<span class="built_in">fcompute</span>(a, b_ramp-&gt;base),</span><br><span class="line"> <span class="built_in">fcompute</span>(<span class="built_in">make_zero</span>(b_ramp-&gt;stride.<span class="built_in">dtype</span>()), b_ramp-&gt;stride), b_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; a_ramp) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(<span class="built_in">fcompute</span>(a_ramp-&gt;base, b), a_ramp-&gt;stride, a_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">fcompute</span>(<span class="built_in">BroadcastTo</span>(a, lanes), <span class="built_in">BroadcastTo</span>(b, lanes));</span><br></pre></td></tr></table></figure><p>如果遍历之后没有变化，就直接返回了。而对于这里的我们需要计算的是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((i*<span class="number">4</span>) + j)</span><br></pre></td></tr></table></figure><p><code>j</code> 是需要向量化的坐标。<code>i*4</code> 是没有变化的。遍历以后<code>a</code>没变化，<code>b</code>变成了<code>T.Ramp(0, 1, 4)</code> 这时候<code>lanes=4</code>，会走到第一个<code>if</code>分支，返回的是新构造的<code>RampNode</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">T.Ramp(i * 4, 1, 4)</span><br></pre></td></tr></table></figure><p>其他的分支也类似。比如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A[i * <span class="number">4</span> + j] + T.<span class="built_in">float32</span>(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// --- after ---</span></span><br><span class="line">A[i * <span class="number">4</span>:i * <span class="number">4</span> + <span class="number">4</span>]   T.<span class="built_in">float32</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>这里会把a、b broadcast为一个向量再做计算。</p><h5 id="VarNode"><a href="#VarNode" class="headerlink" title="VarNode"></a>VarNode</h5><p>对于这里的VarNode判断就比较简单了，如果匹配到的是需要向量化的变量，就返回构造函数中构造的<code>RampNode</code>，否则就返回。其他的操作，暂时略过。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Var var = <span class="built_in">GetRef</span>&lt;Var&gt;(op);</span><br><span class="line"><span class="keyword">if</span> (var.<span class="built_in">same_as</span>(var_)) &#123;</span><br><span class="line"> <span class="keyword">return</span> ramp_;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"> <span class="keyword">return</span> std::<span class="built_in">move</span>(var);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="MulNode"><a href="#MulNode" class="headerlink" title="MulNode"></a>MulNode</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">PrimExpr a = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;a);</span><br><span class="line">PrimExpr b = <span class="keyword">this</span>-&gt;<span class="built_in">VisitExpr</span>(op-&gt;b);</span><br><span class="line"><span class="keyword">if</span> (a.<span class="built_in">same_as</span>(op-&gt;a) &amp;&amp; b.<span class="built_in">same_as</span>(op-&gt;b)) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">GetRef</span>&lt;PrimExpr&gt;(op);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="type">int</span> lanes = std::<span class="built_in">max</span>(a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>(), b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>());</span><br><span class="line"><span class="keyword">if</span> (lanes != <span class="number">1</span>) &#123;</span><br><span class="line"> <span class="type">const</span> RampNode* b_ramp = b.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="type">const</span> RampNode* a_ramp = a.<span class="built_in">as</span>&lt;RampNode&gt;();</span><br><span class="line"> <span class="keyword">if</span> (a_ramp &amp;&amp; b.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; analyzer_.<span class="built_in">CanProve</span>(b &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(a_ramp-&gt;base * b, a_ramp-&gt;stride * b, a_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (b_ramp &amp;&amp; a.<span class="built_in">dtype</span>().<span class="built_in">lanes</span>() == <span class="number">1</span> &amp;&amp; analyzer_.<span class="built_in">CanProve</span>(a &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="built_in">Ramp</span>(b_ramp-&gt;base * a, b_ramp-&gt;stride * a, b_ramp-&gt;lanes);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">Mul</span>(<span class="built_in">BroadcastTo</span>(a, lanes), <span class="built_in">BroadcastTo</span>(b, lanes));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">BinaryVec</span>&lt;Mul&gt;(op);</span><br></pre></td></tr></table></figure><p>这里的处理逻辑与Add基本一致。只是在计算RampNode的时候有点区别。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVE特性以及寄存器</title>
      <link href="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/"/>
      <url>/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p>SVE对比NEON有几个新增的地方。</p><ol><li>变长的向量</li><li>支持Gather-load &amp;&amp; Scatter-store</li></ol><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/gather.png" class=""><ol start="3"><li><p>可以由P寄存器控制向量通道的计算</p><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/Pvector.png" class=""></li><li><p>由软件控制的向量切分。</p><ol><li>基于First Fault 寄存器完成的，加载不合法内存页的时候，会有记录 <img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/20230905222847.png" class=""></li></ol></li><li><p>扩展浮点和位运算的水平缩减</p><img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/140339805.png" class=""></li></ol><h2 id="SVE-寄存器"><a href="#SVE-寄存器" class="headerlink" title="SVE 寄存器"></a>SVE 寄存器</h2><ul><li>Scalable vector registers<br><code>Z0-Z15</code>, 支持double、float、float16，int64、int32、int16、int8<br>向量寄存器长度128-2048bit可变，具体取决于SoC厂商确定，当前手机上上商用的由联发科的天玑9200，长度是128bit，这部分与NEON共用。</li><li>Scalable predicate registers<br>谓词寄存器，<ul><li>P0-P7 控制的数据加载、存取、计算</li><li>P8-P15做循环控制</li><li>FFR ： 用来软件推测的FFR寄存器<img src="/SVE%E7%89%B9%E6%80%A7%E4%BB%A5%E5%8F%8A%E5%AF%84%E5%AD%98%E5%99%A8/SVE.png" class=""></li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 体系结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tir_to_llvm_ir</title>
      <link href="/tir-to-llvm-ir/"/>
      <url>/tir-to-llvm-ir/</url>
      
        <content type="html"><![CDATA[<p>TVM在编译过程中，经历了</p><pre class="mermaid">graph LR  A[3rd IR] --> B[Relay IR]  B --> C[TIR]  C --> D[LLVM IR]  C -->E[Source]</pre><p>这一系列的过程。其中在生成cpu、rocm、nvptx、hexagon等平台的相关代码的时候，会先由TVM的<code>TIR</code>转换为<code>LLVM IR</code>,在后续由LLVM生成相关的机器码。</p><p>这一步是由<code>tvm::codegen::Build</code>调用转换的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">runtime::Module <span class="title">Build</span><span class="params">(IRModule mod, Target target)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (transform::PassContext::<span class="built_in">Current</span>()</span><br><span class="line">          -&gt;<span class="built_in">GetConfig</span>&lt;Bool&gt;(<span class="string">&quot;tir.disable_assert&quot;</span>, <span class="built_in">Bool</span>(<span class="literal">false</span>))</span><br><span class="line">          .<span class="built_in">value</span>()) &#123;</span><br><span class="line">    mod = tir::transform::<span class="built_in">SkipAssert</span>()(mod);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> target_attr_map = tvm::TargetKind::<span class="built_in">GetAttrMap</span>&lt;FTVMTIRToRuntime&gt;(<span class="string">&quot;TIRToRuntime&quot;</span>);</span><br><span class="line">  <span class="keyword">if</span> (target_attr_map.<span class="built_in">count</span>(target-&gt;kind)) &#123;</span><br><span class="line">    <span class="keyword">return</span> target_attr_map[target-&gt;kind](mod, target);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the build function.</span></span><br><span class="line">  std::string build_f_name = <span class="string">&quot;target.build.&quot;</span> + target-&gt;kind-&gt;name;</span><br><span class="line">  <span class="type">const</span> PackedFunc* bf = runtime::Registry::<span class="built_in">Get</span>(build_f_name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(bf != <span class="literal">nullptr</span>) &lt;&lt; build_f_name &lt;&lt; <span class="string">&quot; is not enabled&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> (*bf)(mod, target);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在LLVM相关的target时候，这里的<code>build_f_name</code>就是<code>target.build.llvm</code></p><p>这时候会走到</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;target.build.llvm&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_body_typed</span>([](IRModule mod, Target target) -&gt; runtime::Module &#123;</span><br><span class="line">      <span class="keyword">auto</span> n = <span class="built_in">make_object</span>&lt;LLVMModuleNode&gt;();</span><br><span class="line">      n-&gt;<span class="built_in">Init</span>(mod, target);</span><br><span class="line">      <span class="keyword">return</span> runtime::<span class="built_in">Module</span>(n);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>在<code>Init</code>函数中创建codegen的具体类：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">LLVMModuleNode::Init</span><span class="params">(<span class="type">const</span> IRModule&amp; mod, <span class="type">const</span> Target&amp; target)</span> </span>&#123;</span><br><span class="line">  llvm_instance_ = std::<span class="built_in">make_unique</span>&lt;LLVMInstance&gt;();</span><br><span class="line">  <span class="function">With&lt;LLVMTarget&gt; <span class="title">llvm_target</span><span class="params">(*llvm_instance_, target)</span></span>;</span><br><span class="line">  llvm::TargetMachine* tm = llvm_target-&gt;<span class="built_in">GetOrCreateTargetMachine</span>();</span><br><span class="line">  <span class="comment">// 这里会根据target得到不同的codegen的实现类</span></span><br><span class="line">  std::unique_ptr&lt;CodeGenLLVM&gt; cg = CodeGenLLVM::<span class="built_in">Create</span>(llvm_target.<span class="built_in">get</span>());</span><br><span class="line"></span><br><span class="line">  std::string entry_func;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  skip crt/cpp systemlib options</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> kv : mod-&gt;functions) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!kv.second-&gt;<span class="built_in">IsInstance</span>&lt;PrimFuncNode&gt;()) &#123;</span><br><span class="line">      <span class="comment">// (@jroesch): we relax constraints here, Relay functions will just be ignored.</span></span><br><span class="line">      <span class="built_in">DLOG</span>(INFO) &lt;&lt; <span class="string">&quot;Can only lower IR Module with PrimFuncs, but got &quot;</span> &lt;&lt; kv.second-&gt;<span class="built_in">GetTypeKey</span>();</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">auto</span> f = <span class="built_in">Downcast</span>&lt;PrimFunc&gt;(kv.second);</span><br><span class="line">    <span class="keyword">auto</span> global_symbol = f-&gt;<span class="built_in">GetAttr</span>&lt;String&gt;(tvm::attr::kGlobalSymbol);</span><br><span class="line">    <span class="type">bool</span> is_entry_func = f-&gt;<span class="built_in">HasNonzeroAttr</span>(tir::attr::kIsEntryFunc);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (global_symbol) &#123;</span><br><span class="line">      function_names_.<span class="built_in">push_back</span>(global_symbol.<span class="built_in">value</span>());</span><br><span class="line">      <span class="keyword">if</span> (is_entry_func) &#123;</span><br><span class="line">        entry_func = global_symbol.<span class="built_in">value</span>();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 初始化CodeGenLLVM, 会产生builder_, module_等llvm 中codegen需要的基础数据结构</span></span><br><span class="line">  cg-&gt;<span class="built_in">Init</span>(<span class="string">&quot;TVMMod&quot;</span>, llvm_target.<span class="built_in">get</span>(), system_lib_prefix, </span><br><span class="line">             system_lib_prefix.<span class="built_in">defined</span>(),</span><br><span class="line">           target_c_runtime);</span><br><span class="line">  cg-&gt;<span class="built_in">SetFastMathFlags</span>(llvm_target-&gt;<span class="built_in">GetFastMathFlags</span>());</span><br><span class="line">    <span class="comment">// 核心功能,tir 转化为llvm ir就在此</span></span><br><span class="line">  cg-&gt;<span class="built_in">AddFunctionsOrdered</span>(mod-&gt;functions.<span class="built_in">begin</span>(), mod-&gt;functions.<span class="built_in">end</span>());</span><br><span class="line">  <span class="keyword">if</span> (entry_func.<span class="built_in">length</span>() != <span class="number">0</span>) &#123;</span><br><span class="line">    cg-&gt;<span class="built_in">AddMainFunction</span>(entry_func);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  module_owning_ptr_ = cg-&gt;<span class="built_in">Finish</span>();</span><br><span class="line">  module_ = module_owning_ptr_.<span class="built_in">get</span>();</span><br><span class="line">  llvm_target-&gt;<span class="built_in">SetTargetMetadata</span>(module_);</span><br><span class="line">  module_-&gt;<span class="built_in">addModuleFlag</span>(llvm::Module::Override, <span class="string">&quot;Debug Info Version&quot;</span>,</span><br><span class="line">                         llvm::DEBUG_METADATA_VERSION);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Linux_shell中提取文件名和路径</title>
      <link href="/Linux-shell%E4%B8%AD%E6%8F%90%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D%E5%92%8C%E8%B7%AF%E5%BE%84/"/>
      <url>/Linux-shell%E4%B8%AD%E6%8F%90%E5%8F%96%E6%96%87%E4%BB%B6%E5%90%8D%E5%92%8C%E8%B7%AF%E5%BE%84/</url>
      
        <content type="html"><![CDATA[<p>首先假设我的文件全称：&#x2F;home&#x2F;luna&#x2F;Desktop&#x2F;Software&#x2F;softHLA&#x2F;HLAreporter.v103&#x2F;HLAreporter.sh.</p><h1 id="获取文件名"><a href="#获取文件名" class="headerlink" title="获取文件名"></a>获取文件名</h1><h1 id="使用-，-str"><a href="#使用-，-str" class="headerlink" title="使用${}，${str##*/}"></a>使用<code>$&#123;&#125;，$&#123;str##*/&#125;</code></h1><p>这个命令的作用就是去掉变量str从左边算起的最后一个&#x2F;字符及其左边的内容，返回的值是从左边算起的最后一个&#x2F;（不含该字符）的右边的所有内容，例子很简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=<span class="variable">$&#123;str##*/&#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$file</span></span><br><span class="line">HLAreporter.sh  <span class="comment">## 运行结果</span></span><br></pre></td></tr></table></figure><h1 id="使用awk语句"><a href="#使用awk语句" class="headerlink" title="使用awk语句"></a>使用awk语句</h1><p>因为在ubuntu下面，路径都是以&#x2F;为隔开的，那么我们就以&#x2F;为分隔符，然后把最后部分打印，赋值，例子如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=`echo $str | awk -F &quot;/&quot; &#x27;&#123;print $NF&#125;&#x27;`</span><br><span class="line">echo $file</span><br><span class="line">HLAreporter.sh</span><br></pre></td></tr></table></figure><h1 id="使用官方函数basename"><a href="#使用官方函数basename" class="headerlink" title="使用官方函数basename"></a>使用官方函数basename</h1><p>bash shell本身提供了basename命令，可以直接获取路径名最后的文件名，实现代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=$(basename $str)</span><br><span class="line">echo $file</span><br><span class="line">HLAreporter.sh</span><br></pre></td></tr></table></figure><h1 id="后缀和文件名分开"><a href="#后缀和文件名分开" class="headerlink" title="后缀和文件名分开"></a>后缀和文件名分开</h1><p>使用${}<br>在这里分别使用&#x2F;和.作为分隔符来进行处理，代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">file=$&#123;str##*/&#125;</span><br><span class="line">filename=$&#123;file%.*&#125;</span><br><span class="line">suffix=$&#123;file##*.&#125;</span><br><span class="line">echo $file, $filename, $suffix</span><br><span class="line">HLAreporter.sh, HLAreporter, sh</span><br><span class="line"></span><br><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103.tar.gz</span><br><span class="line">file=$&#123;str##*/&#125;</span><br><span class="line">filename=$&#123;file%%.*&#125;</span><br><span class="line">suffix=$&#123;file#*.&#125;</span><br><span class="line">echo $file, $filename, $suffix</span><br><span class="line">HLAreporter.v103.tar.gz, HLAreporter, v103.tar.gz</span><br></pre></td></tr></table></figure><p>用的是Shell的参数扩展(Parameter Extension)功能，解释如下：</p><p><code>$&#123;str##*/&#125;</code>: 从左边开始删除str中最大匹配(longest matching pattern) <em>&#x2F; 的字符串<br><code>$&#123;str%/*&#125;</code>：从右边开始删除str中最小匹配(shortest matching pattern) &#x2F;</em> 的字符串<br><code>$&#123;file##*.&#125;</code>：从左边开始删除file中最大匹配(longest matching pattern) <em>. 的字符串<br><code>$&#123;file%.*&#125;</code>：从右边开始删除file中最小匹配(shortest matching pattern) .</em> 的字符串<br><code>$&#123;file%%.*&#125;</code>：从右边开始删除file中最大匹配(longest matching pattern) .* 的字符串<br><code>$&#123;file#*.&#125;</code>：从左边开始删除file中小匹配(shortest matching pattern) *. 的字符串<br><code>#</code>：表示从左边算起第一个<br><code>%</code>：表示从右边算起第一个<br><code>##</code>：表示从左边算起最后一个<br><code>%%</code>：表示从右边算起最后一个<br>换句话来说，<code>＃</code>总是表示左边算起，<code>％</code>总是表示右边算起。<br>参数扩展有多种形式，在shell编程中可以用作参数的拼接，字符串的替换，参数列表截取，变量初值等操作，这里不再详述，请参考右边的功能列表和官方文档.</p><h1 id="参数扩展功能列表"><a href="#参数扩展功能列表" class="headerlink" title="参数扩展功能列表"></a>参数扩展功能列表</h1><p>参数形式扩展后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x&#123;y,z&#125;xy xz</span><br><span class="line">$&#123;x&#125;&#123;y, z&#125;$&#123;x&#125;y $&#123;x&#125;z</span><br><span class="line">$&#123;x&#125;&#123;y, $z&#125;$&#123;x&#125;y $&#123;x&#125;$&#123;z&#125;</span><br><span class="line">$&#123;param#pattern&#125;从param前面删除pattern的最小匹配</span><br><span class="line">$&#123;param##pattern&#125;从param前面删除pattern的最大匹配</span><br><span class="line">$&#123;param%pattern&#125;从param后面删除pattern的最小匹配</span><br><span class="line">$&#123;param%%pattern&#125;从param后面删除pattern的最大匹配</span><br><span class="line">$&#123;param/pattern/string&#125;从param中用string替换pattern的第一次匹配，string可为空</span><br><span class="line">$&#123;param//pattern/string&#125;从param中用string替换pattern的所有匹配，string可为空</span><br><span class="line">$&#123;param:3:2&#125;截取$param中索引3开始的2个字符</span><br><span class="line">$&#123;param:3&#125;截取$param中索引3至末尾的字符</span><br><span class="line">$&#123;@:3:2&#125;截取参数列表$@中第3个开始的2个参数</span><br><span class="line">$&#123;param:-word&#125;若$param为空或未设置，则参数式返回word，$param不变</span><br><span class="line">$&#123;param:+word&#125;若$param为非空，则参数式返回word，$param不变</span><br><span class="line">$&#123;param:=word&#125;若$param为空或为设置，则参数式返回word，同时$param设置为word</span><br><span class="line">$&#123;param:?message&#125;若$param为空或为设置，则输出错误信息message，若包含空白符，则需引号</span><br></pre></td></tr></table></figure><p>获取路径名<br>使用${}，${str%&#x2F;*}<br>去掉变量var从右边算起的第一个’&#x2F;’字符及其右边的内容，返回从右边算起的第一个’&#x2F;’（不含该字符）的左边的内容。使用例子及结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str=/home/luna/Desktop/Software/softHLA/HLAreporter.v103/HLAreporter.sh</span><br><span class="line">path=$&#123;str%/*&#125;</span><br><span class="line">echo $path</span><br><span class="line">/home/luna/Desktop/Software/softHLA/HLAreporter.v103</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> shell </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>packfunc</title>
      <link href="/packfunc/"/>
      <url>/packfunc/</url>
      
        <content type="html"><![CDATA[<p>为实现多种语言支持，需要满足以下几点：</p><ul><li>部署：编译结果可以从<code>python/javascript/c++</code>调用。</li><li>Debug: 在python中定义一个函数，在编译函数中调用。</li><li>链接：编写驱动程序以调用设备特定代码（如CUDA），可以在编译的host侧调用</li><li>原型：python侧定义IR PASS，并从C++后端调用该代码</li><li>接口暴露：c++后端代码暴露到python侧</li><li>实验：将编译的函数运送到嵌入式设备，可以直接在嵌入式设备上运行</li></ul><p>tvm希望在任何一个语言中定义的函数，可以在其他的语言中都可以调用。同样希望runtime尽可能的轻量化，以方便在嵌入式设备上部署。</p><h1 id="PackedFunc"><a href="#PackedFunc" class="headerlink" title="PackedFunc"></a>PackedFunc</h1><p><code>PackedFunc</code>是解决上述问题的一个优雅的方案。一个<code>PackedFunc</code>对象对应着一个函数调用，即使定义与调用分散在不同语言之间也可以满足。下面展示一个C++的例子。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;tvm/runtime/packed_func.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">MyAdd</span><span class="params">(TVMArgs args, TVMRetValue* rv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// automatically convert arguments to desired type.</span></span><br><span class="line">  <span class="type">int</span> a = args[<span class="number">0</span>];</span><br><span class="line">  <span class="type">int</span> b = args[<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// automatically assign value return to rv</span></span><br><span class="line">  *rv = a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CallPacked</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  PackedFunc myadd = <span class="built_in">PackedFunc</span>(MyAdd);</span><br><span class="line">  <span class="comment">// get back 3</span></span><br><span class="line">  <span class="type">int</span> c = <span class="built_in">myadd</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的例子中，定义了一个<code>MyAdd</code>的<code>PackedFunc</code>，接受两个参数，<code>args</code>表示输入参数， <code>rv</code>表示返回值。这个参数是类型无关的(type-erased)，这意味着函数签名中对输入输出参数的类型没有限制。这样，当调用这个函数的时候， 从栈上获取输入参数（TVMArgs），通过TVMRetValue返回函数返回值。</p><p>通过C++的模板技巧，可以像正常函数一样调用<code>PackedFunc</code>。由于类型无关的特性，可以在像python这样的动态类型的语言中调用<code>PackedFunc</code>，而无需插入额外其他的胶水代码。下面展示了<code>PackedFunc</code> 的注册及其在python端的调用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// register a global packed function in c++</span></span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;myadd&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>(MyAdd);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line">myadd = tvm.get_global_func(<span class="string">&quot;myadd&quot;</span>)</span><br><span class="line"><span class="comment"># prints 3</span></span><br><span class="line"><span class="built_in">print</span>(myadd(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>多数的<code>PackedFunc</code>技巧依赖于<code>TVMArgs</code>和<code>TVMRetValue</code>，我们限制其中的参数类型，下面是主要用的类型：</p><ul><li>int, float and string</li><li>PackedFunc itself</li><li>Module for compiled modules</li><li>DLTensor* for tensor object exchange</li><li>TVM Object to represent any object in IR</li></ul><p>这个限制，使得实现及其简单而且无需序列化操作。虽然增加了限制，但对于DL开发来说，大多数场景下仅仅需要传递<code>DLTensor</code>和数字就够了。</p><p>既然<code>PackedFunc</code>可以将另外的PackedFunc作为函数参数，那就可以在python与c++之间传递函数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">callback</span>(<span class="params">msg</span>):</span><br><span class="line">  <span class="built_in">print</span>(msg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to PackedFunc</span></span><br><span class="line">f = tvm.convert(callback)</span><br><span class="line">callhello = tvm.get_global_func(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line"><span class="comment"># prints hello world</span></span><br><span class="line">callhello(f)</span><br></pre></td></tr></table></figure><p>TVM 提供了极简的C API，使得将PackedFunc可以方便地嵌入到其他的语言中。除python外，还支持java、JavaScript。</p><p>PackFunction不仅用于tvm编译器中，同样也用于开发的技术栈中。在tvm中所有的PASS函数都通过PackedFunc暴露给前端的。编译结果同样是通过PackedFunc打包的。</p><p>为了保证runtime尽可能的小，runtime中隔离了IR对象的支持。这使得runtime大小只有200~600k，具体的大小取决于平台驱动部分。</p><p>PackedFunc带来的调用开销很小，仅仅是通过栈传递了一些参数对象，只要不通过它包装较小的函数，就是OK的。总之，PackedFunc是tvm中通用的胶水代码，支持了tvm的编译部署。</p><p>额外的部分：</p><h2 id="c-注册，python调用"><a href="#c-注册，python调用" class="headerlink" title="c++ 注册，python调用"></a>c++ 注册，python调用</h2><p>上文中介绍注册时，使用到了一个C++宏<code>TVM_REGISTER_GLOBAL</code>，这里介绍中间是如何链接起来的。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;callhello&quot;</span>)</span><br><span class="line">.<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//展开就是</span></span><br><span class="line"><span class="built_in">TVM_STR_CONCAT</span>(TVM_FUNC_REG_VAR_DEF, __COUNTER__) = ::tvm::runtime::Registry::<span class="built_in">Register</span>(<span class="string">&quot;callhello&quot;</span>).<span class="built_in">set_body</span>([](TVMArgs args, TVMRetValue* rv) &#123;</span><br><span class="line">  PackedFunc f = args[<span class="number">0</span>];</span><br><span class="line">  <span class="built_in">f</span>(<span class="string">&quot;hello world&quot;</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>这里的<code>::tvm::runtime::Registry::Register</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Registry&amp; <span class="title">Registry::Register</span><span class="params">(<span class="type">const</span> std::string&amp; name, <span class="type">bool</span> can_override)</span> </span>&#123;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  Manager* m = Manager::<span class="built_in">Global</span>();<span class="comment">//这是个静态对象，Manager持有一个map来记录注册对象</span></span><br><span class="line">  <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">lock</span><span class="params">(m-&gt;mutex)</span></span>;</span><br><span class="line">  <span class="keyword">if</span> (m-&gt;fmap.<span class="built_in">count</span>(name)) &#123;</span><br><span class="line">    <span class="built_in">ICHECK</span>(can_override) &lt;&lt; <span class="string">&quot;Global PackedFunc &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is already registered&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Registry* r = <span class="keyword">new</span> <span class="built_in">Registry</span>();</span><br><span class="line">  r-&gt;name_ = name;</span><br><span class="line">  m-&gt;fmap[name] = r;</span><br><span class="line">  <span class="keyword">return</span> *r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面看下Registry的实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Registry for global function */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Registry</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">//设置函数体</span></span><br><span class="line">  <span class="function">TVM_DLL Registry&amp; <span class="title">set_body</span><span class="params">(PackedFunc f)</span></span>;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body</span><span class="params">(PackedFunc::FType f)</span> </span>&#123;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">PackedFunc</span>(f));</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//给一个任意函数，萃取函数签名</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> FLambda&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_typed</span><span class="params">(FLambda f)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> FType = <span class="keyword">typename</span> detail::function_signature&lt;FLambda&gt;::FType;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;FType&gt;(std::<span class="built_in">move</span>(f), name_).<span class="built_in">packed</span>());</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//给一个类成员函数、返回值、参数，使用lambda包装</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_method</span><span class="params">(R (T::*f)(Args...))</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](T target, Args... params) -&gt; R &#123;</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="built_in">return</span> (target.*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(T, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args&gt;</span><br><span class="line">  <span class="function">Registry&amp; <span class="title">set_body_method</span><span class="params">(R (T::*f)(Args...) <span class="type">const</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](<span class="type">const</span> T target, Args... params) -&gt; R &#123;</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="built_in">return</span> (target.*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(<span class="type">const</span> T, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> TObjectRef, <span class="keyword">typename</span> TNode, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args,</span><br><span class="line">            <span class="keyword">typename</span> = <span class="keyword">typename</span> std::enable_if&lt;std::is_base_of&lt;ObjectRef, TObjectRef&gt;::value&gt;::type&gt;</span><br><span class="line">  Registry&amp; <span class="built_in">set_body_method</span>(<span class="built_in">R</span> (TNode::*f)(Args...)) &#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](TObjectRef ref, Args... params) &#123;</span><br><span class="line">      TNode* target = ref.<span class="keyword">operator</span>-&gt;();</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="keyword">return</span> (target-&gt;*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(TObjectRef, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> TObjectRef, <span class="keyword">typename</span> TNode, <span class="keyword">typename</span> R, <span class="keyword">typename</span>... Args,</span><br><span class="line">            <span class="keyword">typename</span> = <span class="keyword">typename</span> std::enable_if&lt;std::is_base_of&lt;ObjectRef, TObjectRef&gt;::value&gt;::type&gt;</span><br><span class="line">  Registry&amp; <span class="built_in">set_body_method</span>(<span class="built_in">R</span> (TNode::*f)(Args...) <span class="type">const</span>) &#123;</span><br><span class="line">    <span class="keyword">auto</span> fwrap = [f](TObjectRef ref, Args... params) &#123;</span><br><span class="line">      <span class="type">const</span> TNode* target = ref.<span class="keyword">operator</span>-&gt;();</span><br><span class="line">      <span class="comment">// call method pointer</span></span><br><span class="line">      <span class="keyword">return</span> (target-&gt;*f)(params...);</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set_body</span>(<span class="built_in">TypedPackedFunc</span>&lt;<span class="built_in">R</span>(TObjectRef, Args...)&gt;(fwrap, name_));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> Registry&amp; <span class="title">Register</span><span class="params">(<span class="type">const</span> std::string&amp; name, <span class="type">bool</span> <span class="keyword">override</span> = <span class="literal">false</span>)</span></span>;  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> <span class="type">bool</span> <span class="title">Remove</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>;</span><br><span class="line">  </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> <span class="type">const</span> PackedFunc* <span class="title">Get</span><span class="params">(<span class="type">const</span> std::string&amp; name)</span></span>; </span><br><span class="line">  <span class="function">TVM_DLL <span class="type">static</span> std::vector&lt;std::string&gt; <span class="title">ListNames</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">Manager</span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">protected</span>:</span><br><span class="line">  std::string name_;</span><br><span class="line">  PackedFunc func_;</span><br><span class="line">  <span class="keyword">friend</span> <span class="keyword">struct</span> <span class="title class_">Manager</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>上面注册以后是在一个全局对象中，下一部就看python侧如何调用的。</p><p>python端最终会调用到 <code>_get_global_func</code>函数，具体实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_global_func</span>(<span class="params">name, allow_missing=<span class="literal">False</span></span>):</span><br><span class="line">    handle = PackedFuncHandle()</span><br><span class="line">    check_call(_LIB.TVMFuncGetGlobal(c_str(name), ctypes.byref(handle)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> handle.value:</span><br><span class="line">        <span class="keyword">return</span> _make_packed_func(handle, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> allow_missing:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">&quot;Cannot find global function %s&quot;</span> % name)</span><br></pre></td></tr></table></figure><p>进而会调用到<code>TVMFuncGetGlobal</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMFuncGetGlobal</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, TVMFunctionHandle* out)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">API_BEGIN</span>();</span><br><span class="line">  <span class="type">const</span> tvm::runtime::PackedFunc* fp = tvm::runtime::Registry::<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="keyword">if</span> (fp != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">    *out = <span class="keyword">new</span> tvm::runtime::<span class="built_in">PackedFunc</span>(*fp);  <span class="comment">// NOLINT(*)</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    *out = <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">API_END</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里既可以发现<code>tvm::runtime::Registry::Get(name)</code>来查找相关注册函数的。</p><h2 id="python注册，c-调用"><a href="#python注册，c-调用" class="headerlink" title="python注册，c++ 调用"></a>python注册，c++ 调用</h2><p>如下面的函数，通过装饰器注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tvm._ffi.register_func(<span class="params"><span class="string">&quot;relay.backend.lower_call&quot;</span></span>)</span></span><br></pre></td></tr></table></figure><p>在c++中调用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> <span class="keyword">auto</span> flower_call = tvm::runtime::Registry::<span class="built_in">Get</span>(<span class="string">&quot;relay.backend.lower_call&quot;</span>);</span><br></pre></td></tr></table></figure><p>下面介绍以下python的注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register_func</span>(<span class="params">func_name, f=<span class="literal">None</span>, override=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">callable</span>(func_name):</span><br><span class="line">        f = func_name</span><br><span class="line">        func_name = f.__name__</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(func_name, <span class="built_in">str</span>):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;expect string function name&quot;</span>)</span><br><span class="line"></span><br><span class="line">    ioverride = ctypes.c_int(override)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">myf</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;internal register function&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(myf, PackedFuncBase):</span><br><span class="line">            myf = convert_to_tvm_func(myf) <span class="comment">#转化为packfunc</span></span><br><span class="line">        <span class="comment">#注册</span></span><br><span class="line">        check_call(_LIB.TVMFuncRegisterGlobal(c_str(func_name), myf.handle, ioverride))</span><br><span class="line">        <span class="keyword">return</span> myf</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> f:</span><br><span class="line">        <span class="keyword">return</span> register(f)</span><br><span class="line">    <span class="keyword">return</span> register</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_to_tvm_func</span>(<span class="params">pyfunc</span>):</span><br><span class="line">    local_pyfunc = pyfunc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cfun</span>(<span class="params">args, type_codes, num_args, ret, _</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; ctypes function &quot;&quot;&quot;</span></span><br><span class="line">        num_args = num_args.value <span class="keyword">if</span> <span class="built_in">isinstance</span>(num_args, ctypes.c_int) <span class="keyword">else</span> num_args</span><br><span class="line">        pyargs = (C_TO_PY_ARG_SWITCH[type_codes[i]](args[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_args))</span><br><span class="line">        <span class="comment"># pylint: disable=broad-except</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            rv = local_pyfunc(*pyargs)</span><br><span class="line">        <span class="keyword">except</span> Exception:</span><br><span class="line">            msg = traceback.format_exc()</span><br><span class="line">            msg = py2cerror(msg)</span><br><span class="line">            _LIB.TVMAPISetLastError(c_str(msg))</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rv <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(rv, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">&quot;PackedFunction can only support one return value&quot;</span>)</span><br><span class="line">            temp_args = []</span><br><span class="line">            values, tcodes, _ = _make_tvm_args((rv,), temp_args)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(ret, TVMRetValueHandle):</span><br><span class="line">                ret = TVMRetValueHandle(ret)</span><br><span class="line">            <span class="keyword">if</span> _LIB.TVMCFuncSetReturn(ret, values, tcodes, ctypes.c_int(<span class="number">1</span>)) != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">raise</span> get_last_ffi_error()</span><br><span class="line">            _ = temp_args</span><br><span class="line">            _ = rv</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    handle = PackedFuncHandle()</span><br><span class="line">    f = TVMPackedCFunc(cfun)</span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> We will need to use python-api to increase ref count of the f</span></span><br><span class="line">    <span class="comment"># TVM_FREE_PYOBJ will be called after it is no longer needed.</span></span><br><span class="line">    pyobj = ctypes.py_object(f)</span><br><span class="line">    ctypes.pythonapi.Py_IncRef(pyobj)</span><br><span class="line">    <span class="keyword">if</span> _LIB.TVMFuncCreateFromCFunc(f, pyobj, TVM_FREE_PYOBJ, ctypes.byref(handle)) != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> get_last_ffi_error()</span><br><span class="line">    <span class="keyword">return</span> _make_packed_func(handle, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMFuncRegisterGlobal</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, TVMFunctionHandle f, <span class="type">int</span> <span class="keyword">override</span>)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">API_BEGIN</span>();</span><br><span class="line">  tvm::runtime::Registry::<span class="built_in">Register</span>(name, <span class="keyword">override</span> != <span class="number">0</span>)</span><br><span class="line">      .<span class="built_in">set_body</span>(*<span class="built_in">static_cast</span>&lt;tvm::runtime::PackedFunc*&gt;(f));</span><br><span class="line">  <span class="built_in">API_END</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM教程】 自定义relay算子</title>
      <link href="/tvm-custom-op/"/>
      <url>/tvm-custom-op/</url>
      
        <content type="html"><![CDATA[<p>本文为tvm 教程的翻译版。这部分介绍了如何在tvm中添加新的relay算子，具体的是以一个累乘（cumprod）算子为例进行介绍。</p><p>新增relay算子基本是下面几个步骤：</p><ol><li>定义新增算子的属性节点（Attribute Node），声明在编译时已知的固定参数</li><li>为新增算子编写类型关系，以集成到relay的类型系统中</li><li>使用C++ <code>RELAY_REGISTER_OP</code> 宏，为新增算子注册生命参数数量、类型、提示信息</li><li>算子的compute</li><li>注册算子的compute、schedule</li><li>定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook</li><li>将上面的 Python API hook 封装成简洁的调用方式</li><li>为新的relay 算子编写测试</li></ol><h2 id="新增算子的属性节点"><a href="#新增算子的属性节点" class="headerlink" title="新增算子的属性节点"></a>新增算子的属性节点</h2><p>算子属性是编译期已知的参数。以卷积算子为例，strid、dilation就属于卷积算子的属性。这部分算子属性定义在<code>include/tvm/relay/attrs/</code>下。<br>最终来说，我们期望定义有如下属性说明的算子，其python侧的接口如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cumprod</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Numpy style cumprod op. Return the cumulative inclusive product of the elements along</span></span><br><span class="line"><span class="string">    a given axis.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    data : relay.Expr</span></span><br><span class="line"><span class="string">        The input data to the operator.</span></span><br><span class="line"><span class="string">    axis : int, optional</span></span><br><span class="line"><span class="string">        Axis along which the cumulative product is computed. The default (None) is to compute</span></span><br><span class="line"><span class="string">        the cumprod over the flattened array.</span></span><br><span class="line"><span class="string">    dtype : string, optional</span></span><br><span class="line"><span class="string">        Type of the returned array and of the accumulator in which the elements are multiplied.</span></span><br><span class="line"><span class="string">        If dtype is not specified, it defaults to the dtype of data.</span></span><br><span class="line"><span class="string">    exclusive : bool, optional</span></span><br><span class="line"><span class="string">        If true will return exclusive product in which the first element is not</span></span><br><span class="line"><span class="string">        included. In other terms, if true, the j-th output element would be</span></span><br><span class="line"><span class="string">        the product of the first (j-1) elements. Otherwise, it would be the product of</span></span><br><span class="line"><span class="string">        the first j elements. The product of zero elements will be 1.</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    result : relay.Expr</span></span><br><span class="line"><span class="string">        The result has the same size as data, and the same shape as data if axis is not None.</span></span><br><span class="line"><span class="string">        If axis is None, the result is a 1-d array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><code>.cumsum()</code>有类似的接口。</p><p>因此，在定义我们新增算子（cumprod）属性时，需要选择操作的轴、数据类型和排他性作为属性字段。<code>include/tvm/relay/attrs/transform.h</code></p><p>ScanopAttrs 这里定义了对累加、累乘等操作的属性定义。对累乘来说就不需要额外定义了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*! \brief Attributes used in cumsum and cumprod operator */</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">ScanopAttrs</span> : <span class="keyword">public</span> tvm::AttrsNode&lt;ScanopAttrs&gt; &#123;</span><br><span class="line">  Integer axis;</span><br><span class="line">  DataType dtype;</span><br><span class="line">  Bool exclusive = <span class="built_in">Bool</span>(<span class="literal">false</span>);</span><br><span class="line">  <span class="built_in">TVM_DECLARE_ATTRS</span>(ScanopAttrs, <span class="string">&quot;relay.attrs.ScanopAttrs&quot;</span>) &#123;</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(axis).<span class="built_in">describe</span>(<span class="string">&quot;The axis to operate over&quot;</span>).<span class="built_in">set_default</span>(<span class="built_in">NullValue</span>&lt;Integer&gt;());</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(dtype).<span class="built_in">describe</span>(<span class="string">&quot;Output data type&quot;</span>).<span class="built_in">set_default</span>(<span class="built_in">NullValue</span>&lt;DataType&gt;());</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(exclusive)</span><br><span class="line">        .<span class="built_in">describe</span>(<span class="string">&quot;The first element is not included&quot;</span>)</span><br><span class="line">        .<span class="built_in">set_default</span>(<span class="built_in">Bool</span>(<span class="literal">false</span>));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>但是如果是其他的算子，需要自己定义相应的属性节点。如<code>BiasAdd</code>就需要单独定义</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">BiasAddAttrs</span> : <span class="keyword">public</span> tvm::AttrsNode&lt;BiasAddAttrs&gt; &#123;</span><br><span class="line">  <span class="type">int</span> axis;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">TVM_DECLARE_ATTRS</span>(BiasAddAttrs, <span class="string">&quot;relay.attrs.BiasAddAttrs&quot;</span>) &#123;</span><br><span class="line">    <span class="built_in">TVM_ATTR_FIELD</span>(axis).<span class="built_in">describe</span>(<span class="string">&quot;The axis to add the bias&quot;</span>).<span class="built_in">set_default</span>(<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="类型推导-Type-Relation"><a href="#类型推导-Type-Relation" class="headerlink" title="类型推导 Type Relation"></a>类型推导 Type Relation</h2><p>为了算子注册的灵活性以及relay算子有更好的泛化能力，relay算子通过输入输出之间的类型关系来实例化。<br>这些关系通过一系列的函数进行表示（这些函数是以算子输入输出类型为参数，返回满足类型关系的输入输出列表）， 、、？<br>这包括编译期已知的输入输出的shape 信息<br>本质上，算子relation除了推到输出类型外，还能够强制指定类型规则（检查输入类型）。</p><p>然后就是官网教程的给的例子<code>src/relay/op/tensor/transform.cc</code>。这里依旧是<code>ScanopAttrs</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_NODE_TYPE</span>(ScanopAttrs);</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ScanopRel</span><span class="params">(<span class="type">const</span> Array&lt;Type&gt;&amp; types, <span class="type">int</span> num_inputs, <span class="type">const</span> Attrs&amp; attrs, <span class="type">const</span> TypeReporter&amp; reporter)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// types: [data, output]</span></span><br><span class="line">    <span class="built_in">ICHECK_EQ</span>(types.<span class="built_in">size</span>(), <span class="number">2</span>) &lt;&lt; <span class="string">&quot;Expects two types, one for the input and another for the output&quot;</span>;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* data = types[<span class="number">0</span>].<span class="built_in">as</span>&lt;TensorTypeNode&gt;(); <span class="comment">//输入的tensor信息</span></span><br><span class="line">    <span class="keyword">if</span> (data == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="built_in">ICHECK</span>(types[<span class="number">0</span>].<span class="built_in">as</span>&lt;IncompleteTypeNode&gt;())</span><br><span class="line">        &lt;&lt; <span class="string">&quot;Scanop: expect input type to be TensorType but get &quot;</span> &lt;&lt; types[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* param = attrs.<span class="built_in">as</span>&lt;ScanopAttrs&gt;(); <span class="comment">//算子属性</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> dtype = param-&gt;dtype;</span><br><span class="line">    <span class="keyword">if</span> (dtype.<span class="built_in">is_void</span>()) &#123;</span><br><span class="line">        dtype = data-&gt;dtype;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//设置输出tensor属性</span></span><br><span class="line">    <span class="keyword">if</span> (param-&gt;axis.<span class="built_in">defined</span>()) &#123;</span><br><span class="line">        reporter-&gt;<span class="built_in">Assign</span>(types[<span class="number">1</span>], <span class="built_in">TensorType</span>(data-&gt;shape, dtype));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">auto</span> prod = data-&gt;shape[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">1</span>; i &lt; data-&gt;shape.<span class="built_in">size</span>(); ++i) &#123;</span><br><span class="line">            prod = prod * data-&gt;shape[i];</span><br><span class="line">        &#125;</span><br><span class="line">        reporter-&gt;<span class="built_in">Assign</span>(types[<span class="number">1</span>], <span class="built_in">TensorType</span>(&#123;prod&#125;, dtype));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的例子可以看出 XXXOpRel 的主要功能是根据输入类型确定输出类型。特别的， <code>TensorType</code>的构造函数可以看出，需要指定输出的shape信息，这部分主要目的就是infershape和infertype。</p><h2 id="关联算子的参数数目、属性"><a href="#关联算子的参数数目、属性" class="headerlink" title="关联算子的参数数目、属性"></a>关联算子的参数数目、属性</h2><p>这一步的操作，为自定义算子注册算子名称，通过调用接口增加算子注释。这里需要用到C++的宏<code>RELAY_REGISTER_OP</code><br>涉及的参数含义如下：</p><ul><li>Arity（参数数量）</li><li>位置参数的名称和描述</li><li>支持级别（1 表示内部实现;较高的数字表示较少的内部支持或外部支持的算子）</li><li>算子的类型关系</li><li>优化算子时有用的其他注释。<br><code>src/relay/op/tensor/transform.cc</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;cumsum&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(</span><br><span class="line">        <span class="string">R&quot;doc(Return the cumulative sum of the elements along a given axis.)doc&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">1</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">3</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Cumsum&quot;</span>, ScanopRel)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;TOpPattern&gt;(<span class="string">&quot;TOpPattern&quot;</span>, kOpaque);</span><br><span class="line"></span><br><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;cumprod&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(</span><br><span class="line">        <span class="string">R&quot;doc(Return the cumulative product of the elements along a given axis.)doc&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">1</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">3</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Cumprod&quot;</span>, ScanopRel)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;TOpPattern&gt;(<span class="string">&quot;TOpPattern&quot;</span>, kOpaque);<span class="comment">// 不融合</span></span><br></pre></td></tr></table></figure><p>注：<code>set_attr&lt;TOpPattern&gt;(&quot;TOpPattern&quot;, );</code>此处表示融合算子是，跳过此算子。</p><h2 id="编写的算子compute"><a href="#编写的算子compute" class="headerlink" title="编写的算子compute"></a>编写的算子compute</h2><p>到现在，我们已经实现了算子的接口，但是还缺少算子的compute逻辑。这部分内容超出了这个教程的范围。<br>对于<code>cumprod</code>和<code>cumsum</code>，CPU实现可以参考<code>python/tvm/topi/scan.py</code>，GPU实现可以参考<code>python/tvm/topi/cuda/scan.py</code>。<br>这里这两个的实现，直接在TIR基础上实现得到的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scanop</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data: tvm.te.Tensor,</span></span><br><span class="line"><span class="params">    binop: <span class="type">Callable</span>[[<span class="string">&quot;tvm.Expr&quot;</span>, <span class="string">&quot;tvm.Expr&quot;</span>], <span class="string">&quot;tvm.Expr&quot;</span>],</span></span><br><span class="line"><span class="params">    identity_value: <span class="string">&quot;tvm.Expr&quot;</span>,</span></span><br><span class="line"><span class="params">    op_name: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    axis: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    exclusive: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; tvm.te.Tensor:</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> dtype == <span class="string">&quot;&quot;</span>:</span><br><span class="line">        dtype = data.dtype</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> exclusive <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        exclusive = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maybe_cast</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">if</span> dtype != data.dtype:</span><br><span class="line">            <span class="keyword">return</span> cast(x, dtype)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    axis_mul_before = <span class="number">1</span></span><br><span class="line">    axis_mul_after = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> axis <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        axis = <span class="number">0</span></span><br><span class="line">        cumsum_axis_len = prod(data.shape)</span><br><span class="line">        shape = (cumsum_axis_len,)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(axis, <span class="built_in">int</span>):</span><br><span class="line">            axis = get_const_int(axis)</span><br><span class="line"></span><br><span class="line">        shape = data.shape</span><br><span class="line">        cumsum_axis_len = shape[axis]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> axis &lt; <span class="number">0</span>:</span><br><span class="line">            axis = <span class="built_in">len</span>(shape) + axis</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, value <span class="keyword">in</span> <span class="built_in">enumerate</span>(shape, <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">if</span> i &lt; axis:</span><br><span class="line">                axis_mul_before *= value</span><br><span class="line">            <span class="keyword">elif</span> i &gt; axis:</span><br><span class="line">                axis_mul_after *= value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gen_ir</span>(<span class="params">data_buf, out_buf</span>):</span><br><span class="line">        ib = ir_builder.create()</span><br><span class="line">        data_buf = ib.buffer_ptr(data_buf)</span><br><span class="line">        out_buf = ib.buffer_ptr(out_buf)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, axis_mul_before * axis_mul_after, <span class="string">&quot;fused&quot;</span>, kind=<span class="string">&quot;parallel&quot;</span>) <span class="keyword">as</span> fused:</span><br><span class="line">            i = fused // axis_mul_after</span><br><span class="line">            j = fused % axis_mul_after</span><br><span class="line">            base_idx = i * cumsum_axis_len * axis_mul_after + j</span><br><span class="line">            <span class="keyword">if</span> exclusive:</span><br><span class="line">                out_buf[base_idx] = cast(identity_value, dtype)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out_buf[base_idx] = maybe_cast(data_buf[base_idx])</span><br><span class="line">            <span class="keyword">with</span> ib.for_range(<span class="number">0</span>, cumsum_axis_len - <span class="number">1</span>, <span class="string">&quot;_k&quot;</span>) <span class="keyword">as</span> _k:</span><br><span class="line">                k = _k + <span class="number">1</span></span><br><span class="line">                cur_idx = base_idx + k * axis_mul_after</span><br><span class="line">                prev_idx = base_idx + (k - <span class="number">1</span>) * axis_mul_after</span><br><span class="line">                <span class="keyword">if</span> exclusive:</span><br><span class="line">                    out_buf[cur_idx] = binop(out_buf[prev_idx], maybe_cast(data_buf[prev_idx]))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    out_buf[cur_idx] = binop(out_buf[prev_idx], maybe_cast(data_buf[cur_idx]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ib.get()</span><br><span class="line"></span><br><span class="line">    out_buf = decl_buffer(shape, dtype, <span class="string">&quot;out_buf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> extern(</span><br><span class="line">        [shape],</span><br><span class="line">        [data],</span><br><span class="line">        <span class="keyword">lambda</span> ins, outs: gen_ir(ins[<span class="number">0</span>], outs[<span class="number">0</span>]),</span><br><span class="line">        dtype=dtype,</span><br><span class="line">        out_buffers=[out_buf],</span><br><span class="line">        name=op_name,</span><br><span class="line">        tag=op_name,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data: tvm.te.Tensor,</span></span><br><span class="line"><span class="params">    axis: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    exclusive: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; tvm.te.Tensor:</span><br><span class="line">    <span class="keyword">return</span> scanop(</span><br><span class="line">        data=data,</span><br><span class="line">        binop=generic.add,</span><br><span class="line">        identity_value=<span class="number">0</span>,</span><br><span class="line">        op_name=<span class="string">&quot;cumsum_generic&quot;</span>,</span><br><span class="line">        axis=axis,</span><br><span class="line">        dtype=dtype,</span><br><span class="line">        exclusive=exclusive,</span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="注册算子的compute、schedule"><a href="#注册算子的compute、schedule" class="headerlink" title="注册算子的compute、schedule"></a>注册算子的compute、schedule</h2><p>在实现了算子compute逻辑以后，需要与我们实现的算子接口绑定在一起。在TVM中，这就需要不仅实现算子的compute接口，还要实现对应的schedule。而strategy就是对compute选择合适的schedule。<br>以卷积算子为例，算子编译时，可能会发现这是一个depthwise卷积，进而去选择更高效的schedule实现。</p><p>一般情况下，仅仅考虑CPU、GPU版本即可。<br><code>python/tvm/relay/op/strategy/generic.py</code> <code>python/tvm/relay/op/strategy/cuda.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wrap_compute_scanop</span>(<span class="params">topi_compute</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Wrap scanop style topi compute&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_compute_scanop</span>(<span class="params">attrs, inputs, _</span>):</span><br><span class="line">        <span class="keyword">return</span> [topi_compute(inputs[<span class="number">0</span>], attrs.axis, attrs.dtype, attrs.exclusive)]</span><br><span class="line">    <span class="keyword">return</span> _compute_scanop</span><br><span class="line"></span><br><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;cumsum_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum generic strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cumsum), <span class="comment">#上面写的compute</span></span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_extern),</span><br><span class="line">        name=<span class="string">&quot;cumsum.generic&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br><span class="line"></span><br><span class="line"><span class="meta">@cumsum_strategy.register(<span class="params">[<span class="string">&quot;cuda&quot;</span>, <span class="string">&quot;gpu&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy_cuda</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum cuda strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cuda.cumsum),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_scan),</span><br><span class="line">        name=<span class="string">&quot;cumsum.cuda&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><p>对于每个strategy，与对应的compute、schedule通过<code>add_implementation</code>关联起来。<br>这里的shape_func时对输入时动态shape厂家推导有用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cumsum</span></span><br><span class="line"><span class="meta">@_reg.register_compute(<span class="params"><span class="string">&quot;cumsum&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cumsum</span>(<span class="params">attrs, inputs, output_type</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute definition of cumsum&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [topi.cumsum(inputs[<span class="number">0</span>], attrs.axis, attrs.dtype, attrs.exclusive)]</span><br><span class="line"></span><br><span class="line">_reg.register_strategy(<span class="string">&quot;cumsum&quot;</span>, strategy.cumsum_strategy)</span><br><span class="line">_reg.register_shape_func(<span class="string">&quot;cumsum&quot;</span>, <span class="literal">False</span>, elemwise_shape_func)</span><br></pre></td></tr></table></figure><h2 id="定义C-函数，为新增算子生成调用节点，并为该函数注册-Python-API-hook"><a href="#定义C-函数，为新增算子生成调用节点，并为该函数注册-Python-API-hook" class="headerlink" title="定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook"></a>定义C++函数，为新增算子生成调用节点，并为该函数注册 Python API hook</h2><p>现在我们有一个可以调用的relay算子了，下一步就是如何通过relay call node调用。这就需要实现一个函数，传递相应的参数给对于的relay算子，并且返回对应算子的Call Node（这个算子最终在Relay表达式的AST里面）。</p><p>当前不支持直接调用 Attrs和参数。所以需要在函数中构造对应的AttrsNode，传递给对应的Call Node。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Expr <span class="title">MakeCumsum</span><span class="params">(Expr data, Integer axis, DataType dtype, Bool exclusive)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> attrs = <span class="built_in">make_object</span>&lt;ScanopAttrs&gt;();</span><br><span class="line">    attrs-&gt;dtype = dtype;</span><br><span class="line">    attrs-&gt;axis = axis;</span><br><span class="line">    attrs-&gt;exclusive = exclusive;</span><br><span class="line">    <span class="type">static</span> <span class="type">const</span> Op&amp; op = Op::<span class="built_in">Get</span>(<span class="string">&quot;cumsum&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">Call</span>(op, &#123;data&#125;, <span class="built_in">Attrs</span>(attrs), &#123;&#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;relay.op._make.cumsum&quot;</span>).<span class="built_in">set_body_typed</span>(MakeCumsum);</span><br></pre></td></tr></table></figure><p><code>Op::Get(&quot;cumsum&quot;)</code>的实现如下。具体怎么注册到<code>OpRegistry</code>的，TODO</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">const</span> Op&amp; <span class="title">Op::Get</span><span class="params">(<span class="type">const</span> String&amp; name)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> OpRegEntry* reg = OpRegistry::<span class="built_in">Global</span>()-&gt;<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(reg != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;AttributeError: Operator &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is not registered&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> reg-&gt;<span class="built_in">op</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里看一下Call的实现，实际上是得到一个call Node，里面保存了算子及其属性信息。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Call::<span class="built_in">Call</span>(Expr op, Array&lt;Expr&gt; args, Attrs attrs, Array&lt;Type&gt; type_args, Span span) &#123;</span><br><span class="line">  ObjectPtr&lt;CallNode&gt; n = <span class="built_in">make_object</span>&lt;CallNode&gt;();</span><br><span class="line">  n-&gt;op = std::<span class="built_in">move</span>(op);</span><br><span class="line">  n-&gt;args = std::<span class="built_in">move</span>(args);</span><br><span class="line">  n-&gt;attrs = std::<span class="built_in">move</span>(attrs);</span><br><span class="line">  n-&gt;type_args = std::<span class="built_in">move</span>(type_args);</span><br><span class="line">  n-&gt;span = std::<span class="built_in">move</span>(span);</span><br><span class="line">  data_ = std::<span class="built_in">move</span>(n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Op::Get</code> <code>src/relay/op/tensor/transform.cc</code></p><p>相关接口暴露到python侧，是通过<code>.TVM_REGISTER_GLOBAL</code> <code>MakeCumsum</code> <code>MakeCumprod</code> <code>relay.op._make.cumsum(...)</code> <code>relay.op._make.cumsum(...)</code>实现的。</p><p>细节TODO</p><h2 id="将上面的-Python-API-hook-封装成简洁的调用方式"><a href="#将上面的-Python-API-hook-封装成简洁的调用方式" class="headerlink" title="将上面的 Python API hook 封装成简洁的调用方式"></a>将上面的 Python API hook 封装成简洁的调用方式</h2><p>为更方便的使用，通常的做法是构造单独的函数，因此最好封装成更简洁的python接口。教程的例子，定义在<br><code>TVM_REGISTER_GLOBAL</code> <code>python/tvm/relay/op/transform.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _make.cumsum(data, axis, dtype, exclusive)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumprod</span>(<span class="params">data, axis=<span class="literal">None</span>, dtype=<span class="literal">None</span>, exclusive=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> _make.cumprod(data, axis, dtype, exclusive)</span><br></pre></td></tr></table></figure><p>特别的，如果不定参数的，需要包成Tuple形式进行传递。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">concat</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Concatenate the input tensors along the zero axis.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    args: list of Tensor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    tensor: The concatenated tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    tup = <span class="type">Tuple</span>(<span class="built_in">list</span>(args))</span><br><span class="line">    <span class="keyword">return</span> _make.concat(tup)</span><br></pre></td></tr></table></figure><h2 id="为新的relay-算子编写测试"><a href="#为新的relay-算子编写测试" class="headerlink" title="为新的relay 算子编写测试"></a>为新的relay 算子编写测试</h2><p>参考 <code>tests/python/relay/test_op_level3.py</code></p><p>ref: <a href="https://tvm.apache.org/docs/dev/relay_add_op.html">https://tvm.apache.org/docs/dev/relay_add_op.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM模型编译】2. relay算子构造</title>
      <link href="/tvm-relay-op-construct/"/>
      <url>/tvm-relay-op-construct/</url>
      
        <content type="html"><![CDATA[<p>从TVM的官方<a href="https://www.cnblogs.com/wanger-sjtu/p/15046641.html">Tutorial</a>里面，介绍了如何新增自定义算子。(这是我翻译的)</p><p>之前的文章讲到了<a href="../tvm-onnx-to-relay">onnx 算子转换到Relay IR</a>的过程<br>下面以Conv2d算子介绍，编译过程中 Relay IR是如何被调用的。</p><h2 id="relay-算子调用"><a href="#relay-算子调用" class="headerlink" title="relay 算子调用"></a>relay 算子调用</h2><p>上面的<code>get_relay_op</code>实际上是查找所有 relay ir算子，其代码在<code>python/tvm/relay/frontend/common.py</code>中的<code>get_relay_op</code>。继续以conv卷积算子为例介绍。上文所述的转换算子中，有下面的语句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> candidate <span class="keyword">in</span> (_op, _op.nn, _op.image, _op.vision, _op.contrib):</span><br><span class="line">    op = <span class="built_in">getattr</span>(candidate, op_name, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> op <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>对于<code>conv2d</code>算子，在<code>_op.nn</code>中，找到conv2d实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data,</span></span><br><span class="line"><span class="params">    weight,</span></span><br><span class="line"><span class="params">    strides=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    padding=(<span class="params"><span class="number">0</span>, <span class="number">0</span></span>),</span></span><br><span class="line"><span class="params">    dilation=(<span class="params"><span class="number">1</span>, <span class="number">1</span></span>),</span></span><br><span class="line"><span class="params">    groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    channels=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    kernel_size=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    data_layout=<span class="string">&quot;NCHW&quot;</span>,</span></span><br><span class="line"><span class="params">    kernel_layout=<span class="string">&quot;OIHW&quot;</span>,</span></span><br><span class="line"><span class="params">    out_layout=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params">    out_dtype=<span class="string">&quot;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(kernel_size, <span class="built_in">int</span>):</span><br><span class="line">        kernel_size = (kernel_size, kernel_size)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(strides, <span class="built_in">int</span>):</span><br><span class="line">        strides = (strides, strides)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(dilation, <span class="built_in">int</span>):</span><br><span class="line">        dilation = (dilation, dilation)</span><br><span class="line">    padding = get_pad_tuple2d(padding)</span><br><span class="line">    <span class="keyword">return</span> _make.conv2d( data, weight, strides, padding, dilation, groups, channels, kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>这里的<code>_make.conv2d</code>是通过下面的PackFunc注册得到的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tvm._ffi._init_api(<span class="string">&quot;relay.op.nn._make&quot;</span>, __name__)</span><br></pre></td></tr></table></figure><p>在<code>src/relay/op/nn/convolution.cc</code>找到conv2d的注册函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TVM_REGISTER_GLOBAL</span>(<span class="string">&quot;relay.op.nn._make.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_body_typed</span>([](Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span><br><span class="line">                       Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span><br><span class="line">                       Array&lt;IndexExpr&gt; kernel_size, String data_layout, String kernel_layout,</span><br><span class="line">                       String out_layout, DataType out_dtype) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">MakeConv</span>&lt;Conv2DAttrs&gt;(data, weight, strides, padding, dilation, groups, channels,</span><br><span class="line">                                   kernel_size, data_layout, kernel_layout, out_layout, out_dtype,</span><br><span class="line">                                   <span class="string">&quot;nn.conv2d&quot;</span>);</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>MakeConv 是对所有卷积的模板，根据参数实例化相应的函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">inline</span> Expr <span class="title">MakeConv</span><span class="params">(Expr data, Expr weight, Array&lt;IndexExpr&gt; strides, Array&lt;IndexExpr&gt; padding,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; dilation, <span class="type">int</span> groups, IndexExpr channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Array&lt;IndexExpr&gt; kernel_size, std::string data_layout,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string kernel_layout, std::string out_layout, DataType out_dtype,</span></span></span><br><span class="line"><span class="params"><span class="function">                     std::string op_name)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> attrs = <span class="built_in">make_object</span>&lt;T&gt;();</span><br><span class="line">  attrs-&gt;strides = std::<span class="built_in">move</span>(strides);</span><br><span class="line">  attrs-&gt;padding = std::<span class="built_in">move</span>(padding);</span><br><span class="line">  attrs-&gt;dilation = std::<span class="built_in">move</span>(dilation);</span><br><span class="line">  attrs-&gt;groups = groups;</span><br><span class="line">  attrs-&gt;channels = std::<span class="built_in">move</span>(channels);</span><br><span class="line">  attrs-&gt;kernel_size = std::<span class="built_in">move</span>(kernel_size);</span><br><span class="line">  attrs-&gt;data_layout = std::<span class="built_in">move</span>(data_layout);</span><br><span class="line">  attrs-&gt;kernel_layout = std::<span class="built_in">move</span>(kernel_layout);</span><br><span class="line">  attrs-&gt;out_layout = std::<span class="built_in">move</span>(out_layout);</span><br><span class="line">  attrs-&gt;out_dtype = std::<span class="built_in">move</span>(out_dtype);</span><br><span class="line">  <span class="type">const</span> Op&amp; op = Op::<span class="built_in">Get</span>(op_name);</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">Call</span>(op, &#123;data, weight&#125;, <span class="built_in">Attrs</span>(attrs), &#123;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里通过<code>Op::Get(op_name);</code> 获取对应relay算子，在<code>Op::Get</code>函数中发现是通过查表得到。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// find operator by name</span></span><br><span class="line"><span class="function"><span class="type">const</span> Op&amp; <span class="title">Op::Get</span><span class="params">(<span class="type">const</span> String&amp; name)</span> </span>&#123;</span><br><span class="line">  <span class="type">const</span> OpRegEntry* reg = OpRegistry::<span class="built_in">Global</span>()-&gt;<span class="built_in">Get</span>(name);</span><br><span class="line">  <span class="built_in">ICHECK</span>(reg != <span class="literal">nullptr</span>) &lt;&lt; <span class="string">&quot;AttributeError: Operator &quot;</span> &lt;&lt; name &lt;&lt; <span class="string">&quot; is not registered&quot;</span>;</span><br><span class="line">  <span class="keyword">return</span> reg-&gt;<span class="built_in">op</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注册是通过C++的<code>RELAY_REGISTER_OP(&quot;nn.conv2d&quot;)</code>宏注册到<code>OpRegistry::Global()</code>中。宏展开为</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">static</span> __attribute__((unused))::tvm::OpRegEntry&amp; __make_Op230 =</span><br><span class="line">    ::tvm::OpRegEntry::<span class="built_in">RegisterOrGet</span>(<span class="string">&quot;nn.conv2d&quot;</span>).<span class="built_in">set_name</span>()</span><br></pre></td></tr></table></figure><p>注册过程：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">RELAY_REGISTER_OP</span>(<span class="string">&quot;nn.conv2d&quot;</span>)</span><br><span class="line">    .<span class="built_in">describe</span>(<span class="string">R&quot;code(2D convolution layer (e.g. spatial convolution over images).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This layer creates a convolution kernel that is convolved</span></span><br><span class="line"><span class="string">with the layer input to produce a tensor of outputs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">- **data**: This depends on the `layout` parameter. Input is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, in_channels, height, width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string">- **weight**: (channels, in_channels, kernel_size[0], kernel_size[1])</span></span><br><span class="line"><span class="string">- **out**:  This depends on the `layout` parameter. Output is 4D array of shape</span></span><br><span class="line"><span class="string">            (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">)code&quot;</span> TVM_ADD_FILELINE)</span><br><span class="line">    .<span class="built_in">set_attrs_type</span>&lt;Conv2DAttrs&gt;()</span><br><span class="line">    .<span class="built_in">set_num_inputs</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;data&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The input tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">add_argument</span>(<span class="string">&quot;weight&quot;</span>, <span class="string">&quot;Tensor&quot;</span>, <span class="string">&quot;The weight tensor.&quot;</span>)</span><br><span class="line">    .<span class="built_in">set_support_level</span>(<span class="number">2</span>)</span><br><span class="line">    .<span class="built_in">add_type_rel</span>(<span class="string">&quot;Conv2D&quot;</span>, Conv2DRel&lt;Conv2DAttrs&gt;)</span><br><span class="line">    .<span class="built_in">set_attr</span>&lt;FInferCorrectLayout&gt;(<span class="string">&quot;FInferCorrectLayout&quot;</span>, ConvInferCorrectLayout&lt;Conv2DAttrs&gt;);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>返回的是<code>OpRegEntry</code>，后续的<code>set_name</code>等，则是通过<code>OpRegEntry</code>的get接口（返回的是OpNode），构造对应的Relay op</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【tvm解析】3. Operator Strategy 机制</title>
      <link href="/tvm-op-strategy/"/>
      <url>/tvm-op-strategy/</url>
      
        <content type="html"><![CDATA[<p>Relay Operator Strategy是建立Relay IR与TOPI算子库的桥梁，通过Relay Operator Strategy，每个Relay IR至少与一个compute和一个schedule注册关联起来。至少一个原因在于，一个算子在不同后端设备上有不同的实现，而且一个算子可能有多种计算算法，适应不同场景。</p><p>在增加relay IR 的教程里面注册算子的compute、schedule中，就是通过<code>OpStrategy</code>关联算子的compute与schedule</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;cumsum_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cumsum_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;cumsum generic strategy&quot;&quot;&quot;</span></span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_scanop(topi.cumsum), <span class="comment">#上面写的compute</span></span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_extern),</span><br><span class="line">        name=<span class="string">&quot;cumsum.generic&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><h2 id="Operator-Strategy-Design"><a href="#Operator-Strategy-Design" class="headerlink" title="Operator Strategy Design"></a>Operator Strategy Design</h2><p><code>OpStrategy</code>的核心为<code>OpImplementation</code>，包含了一组compute及对应的schedule，不同实现的名字，选择优先级（参见下文的选择策略）。</p><p>OpStrategy中包含一系列的<code>OpSpecialization</code>，每个<code>OpSpecialization</code>包含一组<code>SpecializedCondition</code>（参考<code>include/tvm/te/schedule.h</code>）. 如果<code>SpecializedCondition</code>为空（null），表示是一个通用的实现，反之则是对于特定情形优化的。<code>SpecializedCondition</code>包含了这一算子的多个TE实现，以及实现被调用的条件。</p><p>最后一点，对给定的workload，一个strategy 函数或者<code>FTVMStrategy</code>,决定了使用哪个compute和schedule，因此这部分需要与relay算子对应起来。<br><code>FTVMStrategy </code>实现位置在<code>include/tvm/target/generic_func.h</code>,是一个通用函数，对于给定硬件平台可以重写。函数签名是</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">OpStrategy</span>(<span class="type">const</span> Attrs&amp; attrs, <span class="type">const</span> Array&lt;Tensor&gt;&amp; inputs, <span class="type">const</span> Type&amp; out_type, <span class="type">const</span> Target&amp; target)</span><br></pre></td></tr></table></figure><p>对给定算子属性信息、输入、输出类型以及平台设备，这个函数返回相应的<code>OpStrategy</code>.</p><h2 id="手写一个-Strategy-函数"><a href="#手写一个-Strategy-函数" class="headerlink" title="手写一个 Strategy 函数"></a>手写一个 Strategy 函数</h2><p>tvm 推荐在python侧来写Strategy 函数，在python侧提供了OpStrategy类，其中包含一个add_implementation方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tvm._ffi.register_object(<span class="params"><span class="string">&quot;relay.OpStrategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OpStrategy</span>(<span class="title class_ inherited__">Object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Operator strategy&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__init_handle_by_constructor__(_make.OpStrategy)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_implementation</span>(<span class="params">self, compute, schedule, name=<span class="string">&quot;default&quot;</span>, plevel=<span class="number">10</span></span>):</span><br><span class="line">        _OpStrategyAddImplementation(self, compute, schedule, name, plevel)</span><br></pre></td></tr></table></figure><p>后面以topk的算子为例，介绍了如何手写 Strategy 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用的</span></span><br><span class="line"><span class="comment"># add to python/tvm/relay/op/strategy/generic.py</span></span><br><span class="line"><span class="meta">@override_native_generic_func(<span class="params"><span class="string">&quot;topk_strategy&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topk_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_topk(topi.topk),</span><br><span class="line">        wrap_topi_schedule(topi.generic.schedule_topk),</span><br><span class="line">        name=<span class="string">&quot;topk.generic&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对GPU CUDA的</span></span><br><span class="line"><span class="comment"># add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.</span></span><br><span class="line"><span class="meta">@topk_strategy.register(<span class="params">[<span class="string">&quot;cuda&quot;</span>, <span class="string">&quot;gpu&quot;</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topk_strategy_cuda</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">    strategy = _op.OpStrategy()</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_my_new_op(topi.cuda.topk),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_topk),</span><br><span class="line">        name=<span class="string">&quot;topk.cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><p>为了满足Strategy 函数对于函数签名的要求（see <code>FTVMCompute</code> and <code>FTVMSchedule</code> in <code>include/tvm/relay/op_attr_types.h</code>），这里对topk的compute和schedule做了一层封装。由于算子属性不同，通常需要算子开发者自己写这部分的封装函数。</p><p>上面的例子比较简单，对于一个设备平台只有一个实现，但对一些其他的复杂算子来说，需要针对不同的算法来写相应的schedule，以卷积算子为例，可以直接写滑窗来计算，也可以使用winograd算法计算。这种情况下有多个implementation：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">strategy.add_implementation(</span><br><span class="line">    wrap_compute_conv2d(topi.cuda.conv2d_nchw),</span><br><span class="line">    wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw),</span><br><span class="line">    name=<span class="string">&quot;conv2d_nchw.cuda&quot;</span>,</span><br><span class="line">    plevel=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> winograd_condition:</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_conv2d(topi.cuda.conv2d_nchw_winograd),</span><br><span class="line">        wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw_winograd),</span><br><span class="line">        name=<span class="string">&quot;conv2d_nchw_winograd.cuda&quot;</span>,</span><br><span class="line">        plevel=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><p>可以看到这两个是优先级不同，在满足winograd算法的情况下，会优先选择winograd算法。这样也可以新增条件，新增implentation。<br>同样也可以对不同shape设置不同的优先级策略。下面的例子就是在<code>m &gt; 16</code>时，有额外的计算策略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dense_strategy</span>(<span class="params">attrs, inputs, out_type, target</span>):</span><br><span class="line">  m = inputs[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">  strategy = _op.OpStrategy()</span><br><span class="line">  strategy.add_implementation(</span><br><span class="line">    wrap_compute_dense(dense_compute1),</span><br><span class="line">    wrap_topi_schedule(dense_schedule1),</span><br><span class="line">    name=<span class="string">&quot;dense_common&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tvm.te.SpecializedCondition(m &gt; <span class="number">16</span>):</span><br><span class="line">    strategy.add_implementation(</span><br><span class="line">        wrap_compute_dense(dense_compute2),</span><br><span class="line">        wrap_topi_schedule(dense_schedule2),</span><br><span class="line">        name=<span class="string">&quot;dense_for_large_m&quot;</span>,</span><br><span class="line">        plevel=<span class="number">15</span>)</span><br><span class="line">  <span class="keyword">return</span> strategy</span><br></pre></td></tr></table></figure><h2 id="将算子-Strategy-绑定到算子"><a href="#将算子-Strategy-绑定到算子" class="headerlink" title="将算子 Strategy 绑定到算子"></a>将算子 Strategy 绑定到算子</h2><p>定义了算子strategy函数以后，需要跟算子绑定在一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register_strategy(<span class="string">&quot;topk&quot;</span>, strategy.topk_strategy)</span><br></pre></td></tr></table></figure><p>然而，对于一个算子来说，写它的strategy函数是比较困难的，对简单算子来说，这里提供了两种方案。<br>第一个:算子是单射的、广播、reduce操作时候，可以通过 <code>register_injective_schedule</code>, <code>register_broadcast_schedule</code>、 <code>register_reduce_schedule</code>，这就避免自己手写schedule了。不过这种方式对于任意后端设备都是通用的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register_broadcast_schedule(<span class="string">&quot;add&quot;</span>)</span><br></pre></td></tr></table></figure><p>第二种：对于没有明确pattern的算子，可以用<code>register_schedule</code>实现对任意后端的注册。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用兜底的</span></span><br><span class="line"><span class="comment"># add to python/tvm/relay/op/strategy/generic.py</span></span><br><span class="line"><span class="meta">@generic_func</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">schedule_pool</span>(<span class="params">attrs, outs, target</span>):</span><br><span class="line">    <span class="keyword">with</span> target:</span><br><span class="line">        <span class="keyword">return</span> topi.generic.schedule_pool(outs, attrs.layout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果特定target的，需要在对应的文件下增加</span></span><br><span class="line"><span class="comment"># add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.</span></span><br><span class="line"><span class="meta">@schedule_pool.register(<span class="params"><span class="string">&quot;cpu&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">schedule_pool_cpu</span>(<span class="params">attrs, outs, target</span>):</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">register_schedule(<span class="string">&quot;nn.max_pool2d&quot;</span>, strategy.schedule_pool)</span><br></pre></td></tr></table></figure><h2 id="Operator-Strategy-选择"><a href="#Operator-Strategy-选择" class="headerlink" title="Operator Strategy 选择"></a>Operator Strategy 选择</h2><p>一个算子有多个Strategy的时候，选择策略是什么呢？</p><p>对于静态shape：首先会根据搜索时候的tune log选择最佳实现，如果tune log中没有或者已有auto TVM模板中有特定的实现，则会根据优先级选择对应的实现。如果多个实现具有相同优先级，选哪个就不确定了。</p><p>动态shape场景，则会选择高优先级的情况。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tvm-多线程代码生成和运行</title>
      <link href="/tvm-cpu-multi-thread/"/>
      <url>/tvm-cpu-multi-thread/</url>
      
        <content type="html"><![CDATA[<h3 id="调用链"><a href="#调用链" class="headerlink" title="调用链"></a>调用链</h3><p>tvm搜索算子在需要多线程运行的算子，是在codegen阶段时插入<code>TVMBackendParallelLaunch</code>的调用。<br><code>TVMBackendParallelLaunch</code> 是tvm的线程池并行化入口，具体如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief The callback function to execute a parallel lambda</span></span><br><span class="line"><span class="comment"> * \param task_id the task id of the function. //这里实际就是线程池线程编码，对应第几个线程</span></span><br><span class="line"><span class="comment"> * \param penv The parallel environment backs the execution. // num_task, sync</span></span><br><span class="line"><span class="comment"> * \param cdata The supporting closure data.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">int</span> <span class="params">(*FTVMParallelLambda)</span><span class="params">(<span class="type">int</span> task_id, TVMParallelGroupEnv* penv, <span class="type">void</span>* cdata)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*!</span></span><br><span class="line"><span class="comment"> * \brief Backend function for running parallel jobs.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \param flambda The parallel function to be launched.</span></span><br><span class="line"><span class="comment"> * \param cdata The closure data. // 可以认为时循环的变量 codegen时生成</span></span><br><span class="line"><span class="comment"> * \param num_task Number of tasks to launch, can be 0, means launch</span></span><br><span class="line"><span class="comment"> *           with all available threads. // codegen 时写入的是0，运行时根据配置写入</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * \return 0 when no error is thrown, -1 when failure happens</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">TVMBackendParallelLaunch</span><span class="params">(FTVMParallelLambda flambda, <span class="type">void</span>* cdata, <span class="type">int</span> num_task)</span></span>;</span><br></pre></td></tr></table></figure><p><code>flambda</code>的调用在单线程和多线程下略有区别。</p><p>单线程运行时</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (num_workers == <span class="number">1</span>) &#123;</span><br><span class="line">    std::atomic&lt;<span class="type">int32_t</span>&gt; sync_counter&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    TVMParallelGroupEnv env;</span><br><span class="line">    env.num_task = <span class="number">1</span>;</span><br><span class="line">    env.sync_handle = &amp;sync_counter;</span><br><span class="line">    (*flambda)(<span class="number">0</span>, &amp;env, cdata);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>多线程运行时</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// launcher-&gt;Init(flambda, cdata, num_task, need_sync != 0);</span></span><br><span class="line"><span class="keyword">this</span>-&gt;cdata = cdata;</span><br><span class="line"><span class="keyword">this</span>-&gt;flambda = flambda;</span><br><span class="line"><span class="keyword">this</span>-&gt;env.num_task = num_task;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (queue-&gt;<span class="built_in">Pop</span>(&amp;task, spin_count)) &#123;</span><br><span class="line">    <span class="built_in">ICHECK</span>(task.launcher != <span class="literal">nullptr</span>);</span><br><span class="line">    TVMParallelGroupEnv* penv = &amp;(task.launcher-&gt;env);</span><br><span class="line">    <span class="type">void</span>* cdata = task.launcher-&gt;cdata;</span><br><span class="line">    <span class="keyword">if</span> ((*task.launcher-&gt;flambda)(task.task_id, penv, cdata) == <span class="number">0</span>) &#123;</span><br><span class="line">      task.launcher-&gt;<span class="built_in">SignalJobFinish</span>();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      task.launcher-&gt;<span class="built_in">SignalJobError</span>(task.task_id);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>可以看到 待并行函数中 <code>TVMParallelGroupEnv* penv</code> 包含了实际的运行时线程，运行时可以根据这个确定每个线程的工作区间和步长。<br><code>cdata</code>则是线程运行时需要变量信息，闭包变量。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>对要并行的函数，实际上是按照<code>lambda</code>表达式的方式生成的。<code>FTVMParallelLambda</code> 的输入参数前两个是运行时确定的，第三个是捕获的外部变量。</p><h2 id="codegen-过程"><a href="#codegen-过程" class="headerlink" title="codegen 过程"></a>codegen 过程</h2><p>下面验证一下上述的猜测。</p><p>codegen过程中，实际上是在遍历<code>tir Stmt</code>的AST，因为生成的循环都是基于For的，调用过程也比较简单了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span>  <span class="comment">// -&gt; </span></span></span><br><span class="line"><span class="function"><span class="title">CreateParallelLaunch</span><span class="params">(For(op-&gt;loop_var, op-&gt;min, op-&gt;extent, op-&gt;kind, op-&gt;body,</span></span></span><br><span class="line"><span class="params"><span class="function">                        op-&gt;thread_binding, op-&gt;annotations),</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="number">0</span>, std::string(<span class="string">&quot;loop_parallel_&quot;</span>) + op-&gt;loop_var-&gt;name_hint.c_str())</span></span>;   <span class="comment">// -&gt;</span></span><br><span class="line">CodeGenCPU::<span class="built_in">VisitStmt_</span>(<span class="type">const</span> ForNode* op);</span><br></pre></td></tr></table></figure><p>当遍历到For节点时， 根据属性判断是否并行加速。这里只分析加速场景。此时<code>parallel_env_.penv == nullptr</code> 创建多线程调用函数，进入<code>CreateParallelLaunch</code>函数。<br>然后 再生成 For的遍历逻辑。<code>this-&gt;VisitStmt(body);</code> 这里的<code>body</code>其实还是<code>For</code> ，这时候就进入 </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// already in parallel env.</span></span><br></pre></td></tr></table></figure><p>前文的猜测也在这里得到验证。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::VisitStmt_</span><span class="params">(<span class="type">const</span> ForNode* op)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">ICHECK</span>(<span class="built_in">is_zero</span>(op-&gt;min));</span><br><span class="line">  <span class="keyword">if</span> (op-&gt;kind == ForKind::kSerial || op-&gt;kind == ForKind::kUnrolled) &#123;</span><br><span class="line">    CodeGenLLVM::<span class="built_in">VisitStmt_</span>(op);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (op-&gt;kind == ForKind::kParallel) &#123;</span><br><span class="line">    <span class="keyword">if</span> (parallel_env_.penv == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">      <span class="built_in">CreateParallelLaunch</span>(<span class="built_in">For</span>(op-&gt;loop_var, op-&gt;min, op-&gt;extent, op-&gt;kind, op-&gt;body,</span><br><span class="line">                               op-&gt;thread_binding, op-&gt;annotations),</span><br><span class="line">                           <span class="number">0</span>, std::<span class="built_in">string</span>(<span class="string">&quot;loop_parallel_&quot;</span>) + op-&gt;loop_var-&gt;name_hint.<span class="built_in">c_str</span>());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// already in parallel env.</span></span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.task_id.<span class="built_in">defined</span>());</span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.num_task.<span class="built_in">defined</span>());</span><br><span class="line">      <span class="built_in">ICHECK</span>(parallel_env_.penv != <span class="literal">nullptr</span>);</span><br><span class="line">      DataType t = op-&gt;extent.<span class="built_in">dtype</span>();</span><br><span class="line">      PrimExpr num_task = <span class="built_in">cast</span>(t, parallel_env_.num_task);</span><br><span class="line">      PrimExpr task_id = <span class="built_in">cast</span>(t, parallel_env_.task_id);</span><br><span class="line">      <span class="built_in">ICHECK</span>(!parallel_env_.in_parallel_loop)</span><br><span class="line">          &lt;&lt; <span class="string">&quot;Nested parallel loop is not supported by threadpool, try fuse them instead&quot;</span>;</span><br><span class="line">      parallel_env_.in_parallel_loop = <span class="literal">true</span>;</span><br><span class="line">      <span class="keyword">if</span> (parallel_env_.stride_pattern) &#123;</span><br><span class="line">        <span class="built_in">CreateSerialFor</span>(<span class="built_in">MakeValue</span>(task_id), <span class="built_in">MakeValue</span>(op-&gt;extent), <span class="built_in">MakeValue</span>(num_task),</span><br><span class="line">                        op-&gt;loop_var, op-&gt;body);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        PrimExpr step = (op-&gt;extent + num_task - <span class="built_in">make_const</span>(t, <span class="number">1</span>)) / num_task;</span><br><span class="line">        PrimExpr begin = <span class="built_in">min</span>(task_id * step, op-&gt;extent);</span><br><span class="line">        PrimExpr end = <span class="built_in">min</span>((task_id + <span class="built_in">make_const</span>(t, <span class="number">1</span>)) * step, op-&gt;extent);</span><br><span class="line">        <span class="built_in">CreateSerialFor</span>(<span class="built_in">MakeValue</span>(begin), <span class="built_in">MakeValue</span>(end),</span><br><span class="line">                        llvm::ConstantInt::<span class="built_in">getSigned</span>(<span class="built_in">GetLLVMType</span>(end), <span class="number">1</span>), op-&gt;loop_var, op-&gt;body);</span><br><span class="line">      &#125;</span><br><span class="line">      parallel_env_.in_parallel_loop = <span class="literal">false</span>;</span><br><span class="line">      ++parallel_env_.parallel_loop_count;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">LOG</span>(FATAL) &lt;&lt; <span class="string">&quot;cannot handle for type &quot;</span> &lt;&lt; op-&gt;kind;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    const Stmt&amp; body  For 循环的statement</span></span><br><span class="line"><span class="comment">    int num_task, 这里设置的是0，根据运行时参数确定使用线程</span></span><br><span class="line"><span class="comment">    std::string name</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CodeGenCPU::CreateParallelLaunch</span><span class="params">(<span class="type">const</span> Stmt&amp; body, <span class="type">int</span> num_task, std::string name)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// closure data</span></span><br><span class="line">  llvm::Function* f =</span><br><span class="line">      llvm::Function::<span class="built_in">Create</span>(ftype_tvm_parallel_lambda_, llvm::Function::PrivateLinkage,</span><br><span class="line">                             <span class="string">&quot;__tvm_parallel_lambda&quot;</span>, module_.<span class="built_in">get</span>());</span><br><span class="line">  <span class="built_in">SetTargetAttributes</span>(f);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// allocate and setup the closure, call the closure. //For 循环内部变量。这里需要声明一下</span></span><br><span class="line">  Array&lt;Var&gt; vfields = tir::<span class="built_in">UndefinedVars</span>(body, &#123;&#125;);</span><br><span class="line">  <span class="type">uint64_t</span> nbytes;</span><br><span class="line">  TypedPointer cdata = <span class="built_in">PackClosureData</span>(vfields, &amp;nbytes, <span class="string">&quot;closure_&quot;</span> + name); <span class="comment">// 可以认为时循环的变量</span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> TVM_LLVM_VERSION &gt;= 90</span></span><br><span class="line">  <span class="keyword">auto</span> launch_callee = llvm::<span class="built_in">FunctionCallee</span>(ftype_tvm_parallel_launch_, <span class="built_in">RuntimeTVMParallelLaunch</span>());</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="keyword">auto</span> launch_callee = <span class="built_in">RuntimeTVMParallelLaunch</span>();</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  llvm::BasicBlock* par_launch_end = <span class="built_in">CheckCallSuccess</span>(builder_-&gt;<span class="built_in">CreateCall</span>(</span><br><span class="line">      launch_callee,</span><br><span class="line">      &#123;f, builder_-&gt;<span class="built_in">CreatePointerCast</span>(cdata.addr, t_void_p_), <span class="built_in">ConstInt32</span>(num_task)&#125;));</span><br><span class="line">  <span class="comment">// Setup the closure function.</span></span><br><span class="line">  <span class="keyword">auto</span>* lambda_entry =</span><br><span class="line">      llvm::BasicBlock::<span class="built_in">Create</span>(*llvm_target_-&gt;<span class="built_in">GetContext</span>(), <span class="string">&quot;parallel_closure_entry&quot;</span>, f);</span><br><span class="line">  builder_-&gt;<span class="built_in">SetInsertPoint</span>(lambda_entry);</span><br><span class="line">  <span class="keyword">auto</span> it = f-&gt;<span class="built_in">arg_begin</span>();</span><br><span class="line">  llvm::Value* task_id = &amp;(*it++);</span><br><span class="line">  task_id-&gt;<span class="built_in">setName</span>(<span class="string">&quot;task_id&quot;</span>);</span><br><span class="line">  llvm::Value* penv = &amp;(*it++);</span><br><span class="line">  cdata.addr = builder_-&gt;<span class="built_in">CreatePointerCast</span>(&amp;(*it++), cdata.addr-&gt;<span class="built_in">getType</span>());</span><br><span class="line">  <span class="comment">// setup new variable map, swap it with current var context.</span></span><br><span class="line">  std::unordered_map&lt;<span class="type">const</span> VarNode*, llvm::Value*&gt; new_vmap;</span><br><span class="line">  <span class="built_in">UnpackClosureData</span>(cdata, vfields, &amp;new_vmap);</span><br><span class="line">  <span class="comment">// setup parallel env</span></span><br><span class="line">  ParallelEnv par_env;</span><br><span class="line">  par_env.task_id = <span class="built_in">Var</span>(<span class="string">&quot;task_id&quot;</span>, DataType::<span class="built_in">Int</span>(<span class="number">32</span>));</span><br><span class="line">  par_env.num_task = <span class="built_in">Var</span>(<span class="string">&quot;num_task&quot;</span>, DataType::<span class="built_in">Int</span>(<span class="number">32</span>));</span><br><span class="line">  new_vmap[par_env.task_id.<span class="built_in">get</span>()] = task_id;</span><br><span class="line">  new_vmap[par_env.num_task.<span class="built_in">get</span>()] = builder_-&gt;<span class="built_in">CreateLoad</span>(</span><br><span class="line">      t_int32_,</span><br><span class="line">      builder_-&gt;<span class="built_in">CreateInBoundsGEP</span>(t_tvm_parallel_group_env_, penv, &#123;<span class="built_in">ConstInt32</span>(<span class="number">0</span>), <span class="built_in">ConstInt32</span>(<span class="number">1</span>)&#125;),</span><br><span class="line">      <span class="string">&quot;num_task&quot;</span>);</span><br><span class="line">  par_env.penv = penv;</span><br><span class="line">  <span class="keyword">auto</span> new_analyzer = std::<span class="built_in">make_unique</span>&lt;arith::Analyzer&gt;();</span><br><span class="line">  std::<span class="built_in">swap</span>(function_, f);</span><br><span class="line">  std::<span class="built_in">swap</span>(parallel_env_, par_env);</span><br><span class="line">  std::<span class="built_in">swap</span>(analyzer_, new_analyzer);</span><br><span class="line">  std::<span class="built_in">swap</span>(var_map_, new_vmap);</span><br><span class="line">  <span class="keyword">this</span>-&gt;<span class="built_in">VisitStmt</span>(body);</span><br><span class="line">  builder_-&gt;<span class="built_in">CreateRet</span>(<span class="built_in">ConstInt32</span>(<span class="number">0</span>));</span><br><span class="line">  <span class="comment">// swap the var map back, now we are back on track.</span></span><br><span class="line">  std::<span class="built_in">swap</span>(var_map_, new_vmap);</span><br><span class="line">  std::<span class="built_in">swap</span>(analyzer_, new_analyzer);</span><br><span class="line">  std::<span class="built_in">swap</span>(parallel_env_, par_env);</span><br><span class="line">  std::<span class="built_in">swap</span>(function_, f);</span><br><span class="line">  <span class="built_in">ICHECK_NE</span>(par_env.parallel_loop_count, <span class="number">0</span>) &lt;&lt; <span class="string">&quot;Cannot find parallel loop within parallel launch&quot;</span>;</span><br><span class="line">  builder_-&gt;<span class="built_in">SetInsertPoint</span>(par_launch_end);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++&#39;s most vexing parse</title>
      <link href="/most-vexing-parse/"/>
      <url>/most-vexing-parse/</url>
      
        <content type="html"><![CDATA[<p>C++’s most vexing parse 是 Scott Meyers 在其名著《Effective STL》中创造的一个术语。Scott 用这个术语来形容 C++ 标准对于 declaration 语句的消歧义（ambiguity resolution）约定与常人的认知相悖。</p><p><strong>最令人烦恼的解析</strong> （<strong>most vexing parse</strong>）是C++中的一种反直觉的二义性解析形式。 在一些场景下，编译器无法区分某语句是初始化时某对象的参数，还是声明一个函数时指定参数类型。在这些情况下，编译器将该行解释为函数声明。</p><p>形如 <code>Type()</code> 或 <code>Type(name)</code> 的表达在某些情况下具有歧义（syntax ambiguity）。</p><h3 id="C风格强制类型转换"><a href="#C风格强制类型转换" class="headerlink" title="C风格强制类型转换"></a>C风格强制类型转换</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">f</span><span class="params">(<span class="type">double</span> my_dbl)</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="type">int</span>(my_dbl))</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的第 2 行是有歧义的。一种可能的解释是声明一个变量<code>i</code>，初始值通过转换<code>my_dbl</code> 到一个<code>int</code>而来。但是，<code>C</code> 允许在函数参数声明周围使用多余的括号；因此，声明的i实际上等同于以下代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A function named i takes an integer and returns an integer.</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="type">int</span> my_dbl)</span></span>;</span><br></pre></td></tr></table></figure><h3 id="未命名的临时对象"><a href="#未命名的临时对象" class="headerlink" title="未命名的临时对象"></a>未命名的临时对象</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Timer</span> &#123;&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">TimeKeeper</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">TimeKeeper</span><span class="params">(Timer t)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">get_time</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer())</span></span>;</span><br><span class="line">  <span class="keyword">return</span> time_keeper.<span class="built_in">get_time</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer())</span></span>;</span><br></pre></td></tr></table></figure><p>是有歧义的，它可以被解释为：</p><ol><li>一个变量：定义为类<code>TimeKeeper</code>的变量<code>time_keeper</code>，用类<code>Timer</code>的匿名实例初始化。</li><li>一个函数声明：声明了一个函数<code>time_keeper</code>，返回一个<code>TimeKeeper</code>，有一个（未命名的）参数。参数的类型是一个（指向）不接受输入并返回<code>Timer</code>对象的函数（的指针）。</li></ol><p>[C ++标准]采取第二种解释，这与上面的第9行不一致。例如，<code>Clang++</code>警告第9行存在最令人烦恼的解析，并报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">clang++ time_keeper.cc</span></span><br><span class="line">**timekeeper.cc:9:25: warning: parentheses were disambiguated as a function declaration**</span><br><span class="line">      **[-Wvexing-parse]**</span><br><span class="line">  TimeKeeper time_keeper(Timer());</span><br><span class="line">                        **^~~~~~~~~**</span><br><span class="line">**timekeeper.cc:9:26: note:** add a pair of parentheses to declare a variable</span><br><span class="line">  TimeKeeper time_keeper(Timer());</span><br><span class="line">                         ^</span><br><span class="line">                         (      )</span><br><span class="line">**timekeeper.cc:10:21: error: member reference base type &#x27;TimeKeeper (Timer (*)())&#x27; is not a**</span><br><span class="line">      **structure or union**</span><br><span class="line">  return time_keeper.get_time();</span><br><span class="line">         **~~~~~~~~~~~^~~~~~~~~**</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>这些有歧义的声明往往不会被解析为程序员所期望的语句。C++ 中的函数类型通常隐藏在<code>typedef</code>之后，并且通常具有显式引用或指针限定符。要强制扭转解析的结果，<strong>常见做法是换一种不同的对象创建或转换语法</strong>。</p><p>在类型转换的示例中，有两种替代语法：“C 风格强制类型转换”</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// declares a variable of type int</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">((<span class="type">int</span>)my_dbl)</span></span>;</span><br></pre></td></tr></table></figure><p>或一个static_cast转换：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">i</span><span class="params">(<span class="keyword">static_cast</span>&lt;<span class="type">int</span>&gt;(my_dbl))</span></span>;</span><br></pre></td></tr></table></figure><p>在变量声明的示例中，首选方法（自 C++11 起）是统一（大括号）初始化。 这也允许完全省略类型名称：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Any of the following work:</span></span><br><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(Timer&#123;&#125;)</span></span>;</span><br><span class="line">TimeKeeper time_keeper&#123;<span class="built_in">Timer</span>()&#125;;</span><br><span class="line">TimeKeeper time_keeper&#123;Timer&#123;&#125;&#125;;</span><br><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">(     &#123;&#125;)</span></span>;</span><br><span class="line">TimeKeeper time_keeper&#123;     &#123;&#125;&#125;;</span><br></pre></td></tr></table></figure><p>在 C++11 之前，强制获得预期解释的常用手段是使用额外的括号或拷贝初始化：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TimeKeeper <span class="title">time_keeper</span><span class="params">( <span class="comment">/*Avoid MVP*/</span> (Timer()))</span></span>; <span class="comment">// 增加一个括号</span></span><br><span class="line">TimeKeeper time_keeper = <span class="built_in">TimeKeeper</span>(<span class="built_in">Timer</span>());  <span class="comment">// c++ 17 拷贝运算可以被优化</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM模型编译】1. onnx2relay</title>
      <link href="/tvm-onnx-to-relay/"/>
      <url>/tvm-onnx-to-relay/</url>
      
        <content type="html"><![CDATA[<p><a href="../tvm-onnx">上一篇</a>介绍了onnx模型在tvm中优化的总体流程。</p><p>在这一篇中，介绍onnx模型到relay模型的转换流程，主要涉及了以下几个方面：</p><ul><li>onnx算子到relay算子转换</li><li>relay算子实现</li></ul><p>这一篇介绍onnx算子到relay算子转换过程</p><h2 id="onnx算子到relay算子转换"><a href="#onnx算子到relay算子转换" class="headerlink" title="onnx算子到relay算子转换"></a>onnx算子到relay算子转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># onnx -&gt; relay</span></span><br><span class="line">mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)</span><br></pre></td></tr></table></figure><p>这部分实现是在<code>python/tvm/relay/frontend/onnx.py</code>中。实现转换过程的核心在于<code>GraphProto</code>这个类。这个类中实现了读取onnx模型各个节点、输入输出，映射onnx算子到relay IR的过程。对外接口为<code>from_onnx</code>这个函数。其伪代码可以大致表示为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">from_onnx</span>(<span class="params">self, graph, opset, get_output_expr=<span class="literal">False</span></span>):</span><br><span class="line">    inputs, params = read_model_inputs(graph) <span class="comment"># 模型参数</span></span><br><span class="line">    nodes = read_model_node(graph) <span class="comment"># 模型节点、算子信息</span></span><br><span class="line">    convert_map = _get_convert_map(opset) <span class="comment"># 模型转换map</span></span><br><span class="line">    check_op_support(nodes, convert_map)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">        op = self._convert_operator(op_name, inputs, attr, opset)</span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>从这里可以知道ONNX前端的每个算子转化与<code>_get_convert_map</code>有关。<br><code>_convert_operator</code>完成了算子转换过程。具体的<code>convert_map</code>包含了所有支持算子的转换函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_convert_operator</span>(<span class="params">self, op_name, inputs, attrs, opset</span>):</span><br><span class="line">    convert_map = _get_convert_map(opset)</span><br><span class="line">    <span class="keyword">if</span> op_name <span class="keyword">in</span> _identity_list: <span class="comment"># 对onnx这里是空的</span></span><br><span class="line">        sym = get_relay_op(op_name)(*inputs, **attrs)</span><br><span class="line">    <span class="keyword">elif</span> op_name <span class="keyword">in</span> convert_map:</span><br><span class="line">        sym = convert_map[op_name](inputs, attrs, self._params)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;Operator &#123;&#125; not implemented.&quot;</span>.<span class="built_in">format</span>(op_name))</span><br><span class="line">    <span class="keyword">return</span> sym</span><br></pre></td></tr></table></figure><p>以卷积算子为例，介绍具体的转换过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Conv&quot;</span>: Conv.get_converter(opset)</span><br></pre></td></tr></table></figure><p>Conv算子的实际转换操作来自于</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv</span>(<span class="title class_ inherited__">OnnxOpConverter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Operator converter for Conv.&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_impl_v1</span>(<span class="params">cls, inputs, attr, params</span>):</span><br><span class="line">        <span class="comment"># Use shape of input to determine convolution type.</span></span><br><span class="line">        data = inputs[<span class="number">0</span>]</span><br><span class="line">        input_shape = infer_shape(data)</span><br><span class="line">        ndim = <span class="built_in">len</span>(input_shape)</span><br><span class="line">        <span class="comment"># auto_pad ...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># construct op from attrs</span></span><br><span class="line">        out = AttrCvt(</span><br><span class="line">            op_name=dimension_picker(<span class="string">&quot;conv&quot;</span>),</span><br><span class="line">            transforms=&#123;</span><br><span class="line">                <span class="string">&quot;kernel_shape&quot;</span>: <span class="string">&quot;kernel_size&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dilations&quot;</span>: (<span class="string">&quot;dilation&quot;</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="string">&quot;pads&quot;</span>: (<span class="string">&quot;padding&quot;</span>, <span class="number">0</span>),</span><br><span class="line">                <span class="string">&quot;group&quot;</span>: (<span class="string">&quot;groups&quot;</span>, <span class="number">1</span>),</span><br><span class="line">            &#125;,</span><br><span class="line">            custom_check=dimension_constraint(),</span><br><span class="line">        )([data, inputs[<span class="number">1</span>]], attr, params)</span><br><span class="line"></span><br><span class="line">        use_bias = <span class="built_in">len</span>(inputs) == <span class="number">3</span></span><br><span class="line">        <span class="keyword">if</span> use_bias:</span><br><span class="line">            out = _op.nn.bias_add(out, inputs[<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>这里通过<code>AttrCvt</code>类中构建相应的<code>relay</code>算子,<code>python/tvm/relay/frontend/common.py</code></p><p><code>AttrCvt</code>类包括两部分，<code>__init__</code> 和 <code>__call__</code>，前者根据收集初始化参数，后者完成Relay IR算子构建。</p><p><code>__call__</code>中的实现主要完成了算子属性读取、转换。根据转换后输入构建Relay IR</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> get_relay_op(op_name)(*inputs, **new_attrs)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【TVM模型编译】0.onnx模型优化流程.md</title>
      <link href="/tvm-onnx/"/>
      <url>/tvm-onnx/</url>
      
        <content type="html"><![CDATA[<p>本文以及后续文章，着重于介绍tvm的完整编译流程。<br>后续文章将会按照以上流程，介绍tvm源码。其中涉及一些编程技巧、以及tvm概念，不在此部分进行进一步讲解，另有文章进行介绍。</p><p>首先介绍一下，从onnx模型转为tvm模型的基本步骤。大致可以分为以下几步：</p><ol><li>onnx模型转到relay IR</li><li>基于Relay IR优化</li><li>导出优化模型</li><li>加载运行模型</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">onnx_model = onnx.load(model_path)</span><br><span class="line">target = <span class="string">&quot;llvm&quot;</span></span><br><span class="line">input_name = <span class="string">&quot;1&quot;</span></span><br><span class="line">shape_dict = &#123;input_name: x.shape&#125;</span><br><span class="line"><span class="comment"># onnx -&gt; relay</span></span><br><span class="line">mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)</span><br><span class="line"><span class="comment"># model build</span></span><br><span class="line"><span class="keyword">with</span> tvm.transform.PassContext(opt_level=<span class="number">3</span>):</span><br><span class="line">    lib = relay.build(mod, target=target, params=params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the library at local temporary directory.</span></span><br><span class="line">fcompile = ndk.create_shared <span class="keyword">if</span> <span class="keyword">not</span> local_demo <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">lib.export_library(<span class="string">&quot;net.so&quot;</span>, fcompile)</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cpp load compiled so</span></span><br><span class="line">tvm::runtime::Module mod_factory = tvm::runtime::Module::<span class="built_in">LoadFromFile</span>(<span class="string">&quot;lib/net.so&quot;</span>);</span><br><span class="line">  <span class="comment">// create the graph executor module</span></span><br><span class="line">tvm::runtime::Module gmod = mod_factory.<span class="built_in">GetFunction</span>(<span class="string">&quot;default&quot;</span>)(dev);</span><br><span class="line">tvm::runtime::PackedFunc set_input = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;set_input&quot;</span>);</span><br><span class="line">tvm::runtime::PackedFunc get_output = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;get_output&quot;</span>);</span><br><span class="line">tvm::runtime::PackedFunc run = gmod.<span class="built_in">GetFunction</span>(<span class="string">&quot;run&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the C++ API</span></span><br><span class="line">tvm::runtime::NDArray x = tvm::runtime::NDArray::<span class="built_in">Empty</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, DLDataType&#123;kDLFloat, <span class="number">32</span>, <span class="number">1</span>&#125;, dev);</span><br><span class="line">tvm::runtime::NDArray y = tvm::runtime::NDArray::<span class="built_in">Empty</span>(&#123;<span class="number">2</span>, <span class="number">2</span>&#125;, DLDataType&#123;kDLFloat, <span class="number">32</span>, <span class="number">1</span>&#125;, dev);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">    <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(x-&gt;data)[i * <span class="number">2</span> + j] = i * <span class="number">2</span> + j;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// set the right input</span></span><br><span class="line"><span class="built_in">set_input</span>(<span class="string">&quot;1&quot;</span>, x);</span><br><span class="line"><span class="comment">// run the code</span></span><br><span class="line"><span class="built_in">run</span>();</span><br><span class="line"><span class="comment">// get the output</span></span><br><span class="line"><span class="built_in">get_output</span>(<span class="number">0</span>, y);</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>利用 Github Actions 自动部署 Hexo 博客</title>
      <link href="/auto-deploy/"/>
      <url>/auto-deploy/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Github Actions 可以很方便实现 CI&#x2F;CD 工作流，类似 Travis 的用法，来帮我们完成一些工作，比如实现自动化测试、打包、部署等操作。当我们运行 Jobs 时，它会创建一个容器 (runner)，容器支持：Ubuntu、Windows 和 MacOS 等系统，在容器中我们可以安装软件，利用安装的软件帮我们处理一些数据，然后把处理好的数据推送到某个地方。</p><p>本文将介绍利用 Github Actions 实现自动部署 hexo 到 Github Pages，在之前我们需要写完文章执行 <code>hexo generate --deploy</code> 来部署，当你文章比较多的时候，可能还需要等待很久，而且还可能会遇到本地安装的 Node.js 版本与 Hexo 不兼容的问题，目前我就是因为电脑的 Node.js 版本升到 v14 版本导致与 Hexo 不兼容部署不了，才来捣腾 Github Actions 功能的。利用 Github Actions 你将会没有这些烦恼。</p><h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><h2 id="创建所需仓库"><a href="#创建所需仓库" class="headerlink" title="创建所需仓库"></a>创建所需仓库</h2><ol><li>创建 your.github.io 仓库用来存放博客和静态博客页面，这两个在不同分支。</li></ol><h2 id="生成部署密钥"><a href="#生成部署密钥" class="headerlink" title="生成部署密钥"></a>生成部署密钥</h2><p>一路按回车直到生成成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-keygen -t rsa -b 4096 -C <span class="string">&quot;<span class="subst">$(git config user.email)</span>&quot;</span> -f gh-pages -N <span class="string">&quot;&quot;</span></span></span><br></pre></td></tr></table></figure><p>当前目录下会有 gh-pages 和 gh-pages.pub 两个文件。</p><h2 id="配置部署密钥"><a href="#配置部署密钥" class="headerlink" title="配置部署密钥"></a>配置部署密钥</h2><p>复制  <code>gh-pages</code> 文件内容，在仓库 <code>Settings -&gt; secrets and variables -&gt; new repository secret</code> 页面上添加。</p><ol><li>在 <code>Name</code> 输入框填写 <code>ACTIONS_DEPLOY_KEY</code>。</li><li>在 <code>secret</code>输入框填写  <code>gh-pages</code> 文件内容。</li></ol><img src="/auto-deploy/1693718105040.png" class="" width="1693718105040"><p>复制  <code>gh-pages.pub</code> 文件内容，在 仓库 Settings -&gt; Deploy keys -&gt; Add deploy key 页面上添加。</p><ol><li>在 Title 输入框填写 HEXO_DEPLOY_PUB。</li><li>在 Key 输入框填写  gh-pages.pub 文件内容。</li><li>勾选 Allow write access 选项。</li></ol><img src="/auto-deploy/1693718293575.png" class="" width="1693718293575"><h1 id="编写-Github-Actions"><a href="#编写-Github-Actions" class="headerlink" title="编写 Github Actions"></a>编写 Github Actions</h1><h2 id="Workflow-模版"><a href="#Workflow-模版" class="headerlink" title="Workflow 模版"></a>Workflow 模版</h2><p>在 blog 仓库根目录下创建 .github&#x2F;workflows&#x2F;deploy.yml 文件，目录结构如下。</p><figure class="highlight plaintext"><figcaption><span>(repository)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">└── .github</span><br><span class="line">    └── workflows</span><br><span class="line">        └── deploy.yml</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在 deploy.yml 文件中粘贴以下内容。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">Pages</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 触发器、分支</span></span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">gh-pages</span>  <span class="comment"># default branch</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="comment"># 子任务</span></span><br><span class="line">  <span class="attr">pages:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span> <span class="comment"># 定运行所需要的虚拟机环境</span></span><br><span class="line">    <span class="attr">permissions:</span></span><br><span class="line">      <span class="attr">contents:</span> <span class="string">write</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">submodules:</span> <span class="literal">true</span></span><br><span class="line">          <span class="attr">fetch-depth:</span> <span class="number">1</span></span><br><span class="line">      <span class="comment"># 每个name表示一个步骤:step</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Use</span> <span class="string">Node.js</span> <span class="number">18.</span><span class="string">x</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/setup-node@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">node-version:</span> <span class="string">&#x27;18.17.1&#x27;</span> <span class="comment"># 自己正在使用的node版本即可</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Global</span> <span class="string">Config</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">          <span class="attr">ACTIONS_DEPLOY_KEY:</span> <span class="string">$&#123;&#123;secrets.ACTIONS_DEPLOY_KEY&#125;&#125;</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          sudo timedatectl set-timezone &quot;Asia/Shanghai&quot;</span></span><br><span class="line"><span class="string">          mkdir -p ~/.ssh/</span></span><br><span class="line"><span class="string">          echo &quot;$ACTIONS_DEPLOY_KEY&quot; &gt; ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">          chmod 600 ~/.ssh/id_rsa</span></span><br><span class="line"><span class="string">          git config --global user.email &quot;xx&quot;</span></span><br><span class="line"><span class="string">          git config --global user.name &quot;XXX&quot;</span></span><br><span class="line"><span class="string"></span>      <span class="comment"># - run: node -v # 查看node版本号</span></span><br><span class="line">      <span class="comment"># 缓存依赖项: https://docs.github.com/cn/actions/using-workflows/caching-dependencies-to-speed-up-workflows</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Cache</span> <span class="string">NPM</span> <span class="string">dependencies</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/cache@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="comment"># npm cache files are stored in `~/.npm` on Linux/macOS</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">~/.npm</span></span><br><span class="line">          <span class="comment"># path: node_modules</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">$&#123;&#123;</span> <span class="string">runner.OS</span> <span class="string">&#125;&#125;-npm-cache</span></span><br><span class="line">          <span class="attr">restore-keys:</span> <span class="string">|</span></span><br><span class="line"><span class="string">            $&#123;&#123; runner.OS &#125;&#125;-npm-cache</span></span><br><span class="line"><span class="string"></span>      <span class="comment"># 查看路径 : /home/runner/work/blog/blog</span></span><br><span class="line">      <span class="comment"># - name: Look Path</span></span><br><span class="line">      <span class="comment">#   run: pwd</span></span><br><span class="line">      <span class="comment"># 查看文件</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Look</span> <span class="string">Dir</span> <span class="string">List</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">tree</span> <span class="string">-L</span> <span class="number">3</span> <span class="string">-a</span></span><br><span class="line">      <span class="comment"># 第一次或者依赖发生变化的时候执行 Install Dependencies，其它构建的时候不需要这一步</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Install</span> <span class="string">Dependencies</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">install</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Look</span> <span class="string">Dir</span> <span class="string">List</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">tree</span> <span class="string">-L</span> <span class="number">3</span> <span class="string">-a</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Clean</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">clean</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">build</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Deploy</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">npm</span> <span class="string">run</span> <span class="string">deploy</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Get</span> <span class="string">the</span> <span class="string">output</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          echo &quot;$&#123;&#123; steps.deploy.outputs.notify &#125;&#125;&quot;</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure><h1 id="hexo配置文件"><a href="#hexo配置文件" class="headerlink" title="hexo配置文件"></a>hexo配置文件</h1><p>blog 根目录下，名为 _config.yml，配置一下deploy的分支信息。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span> </span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span> </span><br><span class="line">  <span class="attr">repository:</span> <span class="string">xxx</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">deploy</span></span><br></pre></td></tr></table></figure><h1 id="执行任务"><a href="#执行任务" class="headerlink" title="执行任务"></a>执行任务</h1><p>写一篇文章，push 到 仓库的 master 分支，在仓库 Actions 页面查看当前 task。</p><p>当任务完成后查看您的博客 <a href="https://your.github.io/">https://your.github.io</a>，如果不出意外的话已经可以看到新添加的文章了。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>偷懒是人类发展的动力，人都有偷懒的想法，目的就是为了让自己能够活得更好，经过几千年的不断发展，现在人偷懒的方式无疑更加的先进。</p>]]></content>
      
      
      
        <tags>
            
            <tag> CI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++初始化列表</title>
      <link href="/cpp-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%88%97%E8%A1%A8/"/>
      <url>/cpp-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%88%97%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>类对象的构造顺序是：<br>1.分配内存，调用构造函数时，隐式／显示的初始化各数据成员；<br>2.进入构造函数后在构造函数中执行一般赋值与计算。</p><h3 id="使用初始化列表有两个原因："><a href="#使用初始化列表有两个原因：" class="headerlink" title="使用初始化列表有两个原因："></a>使用初始化列表有两个原因：</h3><p><strong>原因1.必须这样做：</strong></p><p>《C++ Primer》中提到在以下三种情况下需要使用初始化成员列表： </p><ol><li>需要<strong>初始化的数据成员是对象</strong>的情况(这里包含了<strong>继承</strong>情况下，通过显示调用父类的构造函数对父类数据成员进行初始化)； </li><li>需要初始化<strong>const修饰的类成员</strong>； </li><li>需要初始化<strong>引用成员数据</strong>；</li></ol><blockquote><p>1 的说明：数据成员是对象，并且这个对象只有含参数的构造函数，没有无参数的构造函数；</p></blockquote><p><strong>原因2.效率要求这样做：</strong><br>类对象的构造顺序显示，进入构造函数体后，进行的是计算，是对成员变量的赋值操作，显然，赋值和初始化是不同的，这样就体现出了效率差异，如果不用成员初始化列表，那么类对自己的类成员分别进行的是一次隐式的默认构造函数的调用，和一次赋值操作符的调用，如果是类对象，这样做效率就得不到保障。 </p><h4 id="类成员是对象时的运行分析"><a href="#类成员是对象时的运行分析" class="headerlink" title="类成员是对象时的运行分析"></a>类成员是对象时的运行分析</h4><p><strong>不使用初始化列表</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> i;</span><br><span class="line"><span class="built_in">Base</span>()</span><br><span class="line">&#123;</span><br><span class="line">i = <span class="number">0</span>;</span><br><span class="line">cout &lt;&lt; i &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Base无参构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Base</span>(<span class="type">int</span> tmp)</span><br><span class="line">&#123;</span><br><span class="line">i = tmp;</span><br><span class="line">cout &lt;&lt; i &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Base含参构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Base</span>(<span class="type">const</span> Base &amp;tmp)</span><br><span class="line">&#123;</span><br><span class="line">i = tmp.i;</span><br><span class="line">cout &lt;&lt; i &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Base拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line">Base b;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Test</span>(<span class="type">int</span> i, Base tmp)</span><br><span class="line">&#123;</span><br><span class="line">a = i;</span><br><span class="line">b = tmp;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test 构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">~<span class="built_in">Test</span>()</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test析构函数&quot;</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">const</span> Test &amp;b)</span><br><span class="line">&#123;</span><br><span class="line">a = b.a;</span><br><span class="line">cout &lt;&lt; b.a &lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt; <span class="string">&quot;Test拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">Base b;</span><br><span class="line"><span class="function">Test <span class="title">a</span><span class="params">(<span class="number">10</span>, b)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出为</p><blockquote><p>0       Base无参构造函数<br>0       Base拷贝构造函数<br>0       Base无参构造函数<br>10      Test 构造函数<br>10      Test析构函数</p></blockquote><p>如果不使用初始化列表，Test对象初始化时，需要先调用一次无参构造参数，然后赋值运算，如果没有实现，将会报错。</p><p><strong>使用初始化列表</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line">Base b;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Test</span>(<span class="type">int</span> i, Base tmp):<span class="built_in">a</span>(i),<span class="built_in">b</span>(tmp)</span><br><span class="line">&#123;</span><br><span class="line">cout &lt;&lt; a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test 构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">~<span class="built_in">Test</span>()</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;Test析构函数&quot;</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">const</span> Test &amp;b)</span><br><span class="line">&#123;</span><br><span class="line">a = b.a;</span><br><span class="line">cout &lt;&lt; b.a &lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt; <span class="string">&quot;Test拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出为</p><blockquote><p>0       Base无参构造函数<br>0       Base拷贝构造函数<br>0       Base拷贝构造函数<br>10      Test 构造函数<br>10      Test析构函数</p></blockquote><p>使用了初始化列表以后，就可以直接调用拷贝构造函数。如果是传入引用，拷贝构造函数的调用就没有了。</p><h4 id="注"><a href="#注" class="headerlink" title="注"></a>注</h4><ol><li>类里面的任何成员变量在定义时是不能初始化的。 </li><li>一般的数据成员可以在构造函数中初始化。 </li><li>const数据成员必须在构造函数的初始化列表中初始化。 </li><li>static要在类的定义外面初始化。 </li><li>数组成员是不能在初始化列表里初始化的。 </li><li>不能给数组指定明显的初始化。</li></ol><blockquote><p>3和5决定了，类成员中不能定义常量数组。</p></blockquote><p>初始化列表中成员列出的顺序和它们在类中声明的顺序相同.</p><blockquote><p>对一个对象的所有成员来说，它们的析构函数被调用的顺序总是和它们在构造函数里被创建的顺序相反。那么，如果允许上面的情况（即，成员按它们在初始化列表上出现的顺序被初始化）发生，编译器就要为每一个对象跟踪其成员初始化的顺序，以保证它们的析构函数以正确的顺序被调用。这会带来昂贵的开销。所以，为了避免这一开销，同一种类型的所有对象在创建（构造）和摧毁（析构）过程中对成员的处理顺序都是相同的,而不管成员在初始化列表中的顺序如何</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初始化方法-基本到kaiming</title>
      <link href="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/"/>
      <url>/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么需要初始化"><a href="#为什么需要初始化" class="headerlink" title="为什么需要初始化"></a>为什么需要初始化</h3><p>初始化的原因，</p><ul><li>防止每一层的输出太大或者太小，导致梯度反向传播过程中，梯度爆炸或者梯度消失。</li><li>不能采用统一值得原因，因为统一值得初始化会使得每一层网络在不同通道学到得特征相同。</li></ul><p>上述原因都会导致，网络模型不能收敛。</p><h4 id="简单例子得说明"><a href="#简单例子得说明" class="headerlink" title="简单例子得说明"></a>简单例子得说明</h4><p>假如我们有一个输入<code>x</code> ，定义为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">512</span>)</span><br></pre></td></tr></table></figure><p><code>x</code>是&#96;均值为 $0$，方差是 $1$ 的高斯分布。然后定义一个100层的神经网络（注：不包含激活函数）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x = a @ x</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br></pre></td></tr></table></figure><p>那么得到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(nan),tensor(nan))</span><br></pre></td></tr></table></figure><p>输出已经是无穷大了。通过下面的代码，可以知道大概29层以后，输出就已经无法计算了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x = a @ x</span><br><span class="line">    <span class="keyword">if</span> torch.isnan(x.std()):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(i) <span class="comment"># 28</span></span><br></pre></td></tr></table></figure><p>既然输出太大，我们把神经网络的初始化变小一点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)*<span class="number">0.01</span></span><br><span class="line">    x = a @ x</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># 0, 0</span></span><br></pre></td></tr></table></figure><p>那么得到</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.</span>),tensor(<span class="number">0.</span>))</span><br></pre></td></tr></table></figure><p>这时候的输出就太小，没办法计算了。</p><h3 id="怎么找到合适的初始化方法"><a href="#怎么找到合适的初始化方法" class="headerlink" title="怎么找到合适的初始化方法"></a>怎么找到合适的初始化方法</h3><p>对于神经网络来说，前向传播过程就是矩阵运算，假设一层的输出为$y$<br>$$<br>y_i&#x3D; \sum_{k&#x3D;1}^{n-1}a_{i,k}x_k<br>$$</p><p>$i$ 是矩阵 $\mathbf{m}$ 的行，$k$ 是矩阵 $\mathbf{m}$ 的列。python的计算代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y[i] = <span class="built_in">sum</span>([c*d <span class="keyword">for</span> c,d <span class="keyword">in</span> <span class="built_in">zip</span>(a[i], x)])</span><br></pre></td></tr></table></figure><p>可以证明，在给定的层，从标准正态分布初始化的输入$x$ 和权重矩阵 $a$ 的矩阵乘积平均具有非常接近输入<strong>连接数的平方根的标准偏差</strong>，在例子中是$\sqrt{512}$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    y = a @ x</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"><span class="built_in">print</span>(mean()/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.00889449315816164  22.629779825053976</span></span><br><span class="line"><span class="built_in">print</span>(math.sqrt(<span class="number">512</span>))</span><br><span class="line"><span class="comment"># 22.627416997969522</span></span><br><span class="line"></span><br><span class="line">mean,var = <span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    b = torch.randn(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    y = a @ x</span><br><span class="line">    z = b @ y</span><br><span class="line">    mean += z.mean().item()</span><br><span class="line">    var += z.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.6010947234869003 511.8684602024235</span></span><br></pre></td></tr></table></figure><p>如果我们根据如何定义矩阵乘法来看前向传播的过程：</p><p>为了计算 $y$，我们将输入 $x$ 的一个元素的乘以矩阵 $\mathbf{a}$ 的一列的512个乘积然后相加。在使用标准正态分布初始化$x$ 和 $a$ 的示例中，这$512$ 个数字中的每一个的平均值为 $0$，标准差为$1$。</p><blockquote><p><strong>经过一层网络运算以后，均值没变，方差扩大了$\sqrt{512}$倍。</strong></p></blockquote><p>因此在初始化的是，缩小$\sqrt{512}$倍，那么输出结果就能保证不<strong>爆炸</strong>了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    y = a @ x</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.00039810733370250094 1.0007971983717594</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x = a @ x</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-0.0048) tensor(1.2810)</span></span><br></pre></td></tr></table></figure><h3 id="Xavier-Initialization"><a href="#Xavier-Initialization" class="headerlink" title="Xavier Initialization"></a>Xavier Initialization</h3><p>上面介绍的情况是在不含有激活的函数情形，如果增加了激活函数，是否仍能保持不变呢？对于不同类型的激活函数，是不是有不同的表现呢？最开始用的激活函数多数为对称的，并且导数从中间到两边有递减为0。比如，常用的<code>tanh</code>和<code>sigmoid</code>函数。</p><p>下面的结果是在上面的例子中分别增加了，<code>tanh</code>和<code>sigmoid</code>函数的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sigmoid</span></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x = torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.5057) tensor(0.1180)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tanh</span></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-0.0051) tensor(0.0879)</span></span><br></pre></td></tr></table></figure><p>可以看到经过激活函数以后，函数方差明显变小了。在训练过程中这就会导致，导致梯度过小，使得训练难以进行。</p><p>上面用的是正态分布，如果采用均匀分布呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(-<span class="number">1</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-3.8077e-26) tensor(1.2476e-24)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(-1.) tensor(0.)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(1.0000) tensor(3.8114e-06)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.Tensor(<span class="number">512</span>,<span class="number">512</span>).uniform_(-<span class="number">1</span>,<span class="number">1</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x =  torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.4934) tensor(0.0659)</span></span><br></pre></td></tr></table></figure><p>方差都出人意料的小。这就几乎不能学习到什么有用的特征了。</p><p>为此，Glorot and Bengio 提出了<code>Xavier initialization</code>的初始化方式</p><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>这种初始化方式是从随机均匀分布初始化神经网络的，均匀分布的范围是<br>$$<br>\pm \frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}<br>$$<br>这里的 $n_i$ 是输入神经元数目，$n_{i+1}$ 是输出神经元数目。</p><p>Glorot and Bengio 认为Xavier 初始化方法，可以在包含激活函数的神经网络中保持方差的变化很小。</p><img src="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/Xavier.png" class="" title="img"><p>除此之外，同样证明了，传统方法在底层网络方差大，高层网络方差趋近于0的现象。</p><img src="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/Xavier2.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m,n</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(m,n).uniform_(-<span class="number">1</span>,<span class="number">1</span>)/math.sqrt(<span class="number">6.</span>/(m+n))</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x =  torch.tanh( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.0854) tensor(0.9933)</span></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x =  torch.sigmoid( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#tensor(0.4686) tensor(0.4976)</span></span><br></pre></td></tr></table></figure><h3 id="Kaiming-Initialization"><a href="#Kaiming-Initialization" class="headerlink" title="Kaiming Initialization"></a>Kaiming Initialization</h3><p>进来CV领域中，激活方法多是采用<code>Relu</code> 函数。对于这个函数。之前的初始化方法，又有哪些不一样？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    x = torch.relu(a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># tensor(4.6656e-16) tensor(6.7154e-16)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x =  torch.relu( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># tensor(nan) tensor(nan)</span></span><br></pre></td></tr></table></figure><p>之前的初始化方法，对于<code>Relu</code>函数都不奏效了。那对于每一层来说，有什么变化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>)</span><br><span class="line">    y = torch.relu(a @ x)</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#9.01142036409378 15.991211348807246</span></span><br><span class="line"><span class="built_in">print</span>(math.sqrt(<span class="number">512</span>/<span class="number">2</span>))</span><br><span class="line"><span class="comment">#16.0</span></span><br></pre></td></tr></table></figure><p>可以看到，这时候的输出跟输入网络层数大小是有关系的。在下面的实验验证以下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mean,var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    x = torch.randn(<span class="number">512</span>)</span><br><span class="line">    a = torch.randn(<span class="number">512</span>,<span class="number">512</span>)/math.sqrt(<span class="number">512</span>/<span class="number">2.</span>)</span><br><span class="line">    y = torch.relu(a @ x)</span><br><span class="line">    mean += y.mean().item()</span><br><span class="line">    var += y.<span class="built_in">pow</span>(<span class="number">2</span>).mean().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mean/<span class="number">10000</span>, math.sqrt(var/<span class="number">10000</span>))</span><br><span class="line"><span class="comment">#0.5640919140070677 1.0003173674661943</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kaiming</span>(<span class="params">m,n</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.randn(m,n)*math.sqrt(<span class="number">2.</span>/m)</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">512</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a = kaiming(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x = torch.relu( a @ x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment"># tensor(0.8135) tensor(1.2431)</span></span><br></pre></td></tr></table></figure><p>对照本部分开始的结果<code>kaiming</code>方法在对于<code>Relu</code>函数更有优势。</p><p>下图给出了两种方法在一个30层CNN上的结果。</p><img src="/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95-%E5%9F%BA%E6%9C%AC%E5%88%B0kaiming/kaiming.png" class="" title="kaiming method"><hr><p><strong>来源</strong>  <a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">Weight Initialization in Neural Networks: A Journey From the Basics to Kaiming</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表反转</title>
      <link href="/%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/"/>
      <url>/%E9%93%BE%E8%A1%A8%E5%8F%8D%E8%BD%AC/</url>
      
        <content type="html"><![CDATA[<h2 id="leetcode-206-单链表反转"><a href="#leetcode-206-单链表反转" class="headerlink" title="leetcode 206 单链表反转"></a>leetcode 206 单链表反转</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL</span><br><span class="line">输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL</span><br></pre></td></tr></table></figure><h3 id="迭代方法"><a href="#迭代方法" class="headerlink" title="迭代方法"></a>迭代方法</h3><p>首先设置<code>pre,cur,lat</code>三个指针</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pre   cur  lat</span><br><span class="line">null   1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; null</span><br></pre></td></tr></table></figure><p>接着<code>cur.next = pre</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pre   cur  lat</span><br><span class="line">null &lt;-1    2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; null</span><br></pre></td></tr></table></figure><p>接着<code>pre = cur，cur = lat，lat = lat.next</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">      pre  cur  lat</span><br><span class="line">null &lt;-1    2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; null</span><br></pre></td></tr></table></figure><p>重复上述操作直到<code>lat=None</code>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">                     pre  cur  lat</span><br><span class="line">null &lt;-1 &lt;- 2 &lt;- 3 &lt;- 4    5 -&gt; null</span><br></pre></td></tr></table></figure><p><strong>代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">        pre = <span class="literal">None</span></span><br><span class="line">        cur = head </span><br><span class="line">        <span class="keyword">while</span> cur != <span class="literal">None</span>:</span><br><span class="line">            cur.<span class="built_in">next</span>, pre, cur =  pre, cur,cur.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> pre</span><br></pre></td></tr></table></figure><h3 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head</span>):</span><br><span class="line">    <span class="keyword">if</span> head == <span class="literal">None</span> <span class="keyword">or</span> head.<span class="built_in">next</span> == <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> head</span><br><span class="line">    node = self.reverseList(head.<span class="built_in">next</span>)</span><br><span class="line">    head.<span class="built_in">next</span>.<span class="built_in">next</span> = head</span><br><span class="line">    head.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure><h2 id="leetcode-25-单链表-k组反转"><a href="#leetcode-25-单链表-k组反转" class="headerlink" title="leetcode 25 单链表-k组反转"></a>leetcode 25 单链表-k组反转</h2><blockquote><p>给定这个链表：<code>1-&gt;2-&gt;3-&gt;4-&gt;5</code><br>当 <code>k = 2</code> 时，应当返回: <code>2-&gt;1-&gt;4-&gt;3-&gt;5</code><br>当 <code>k = 3</code> 时，应当返回: <code>3-&gt;2-&gt;1-&gt;4-&gt;5</code></p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>柔性数组</title>
      <link href="/%E6%9F%94%E6%80%A7%E6%95%B0%E7%BB%84/"/>
      <url>/%E6%9F%94%E6%80%A7%E6%95%B0%E7%BB%84/</url>
      
        <content type="html"><![CDATA[<p>这个问题是阿里的一个面试题。当时没有很清楚，答得很差，特地实验看一下运行结果。</p><blockquote><p>在结构体中定义了一个<code>char*</code>指针，与定义一个零元素的<code>char</code>数组有什么区别？</p></blockquote><h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>常用来构成缓冲区。比起指针，用空数组有这样的优势：</p><ul><li>不需要初始化，数组名直接就是所在的偏移；</li><li>不占任何空间，指针需要占用int长度空间，空数组不占任何空间。<blockquote><p>“这个数组不占用任何内存”，意味着这样的结构节省空间；<br>“该数组的内存地址就和它后面的元素地址相同”，意味着无需初始化，数组名就是后面元素的地址，直接就能当指针使用。</p></blockquote></li></ul><p>这样的写法最适合制作动态buffer，因为可以这样分配空间<code>malloc(sizeof(structXXX) + buff_len)</code>; 直接就把buffer的结构体和缓冲区一块分配了。用起来也非常方便，因为现在空数组其实变成了buff_len长度的数组了。这样的好处是：</p><ul><li>一次分配解决问题，省了不少麻烦。为了防止内存泄露，如果是分两次分配(结构体和缓冲区)，那么要是第二次malloc失败了，必须回滚释放第一个分配的结构体。这样带来了编码麻烦。其次，分配了第二个缓冲区以后，如果结构里面用的是指针，还要为这个指针赋值。同样，在free这个buffer的时候，用指针也要两次free。如果用空数组，所有问题一次解决。</li><li>小内存的管理是非常困难的，如果用指针，这个buffer的struct部分就是小内存了，在系统内存在多了势必严重影响内存管理的性能。要是用空数组把struct和实际数据缓冲区一次分配大块问题，就没有这个问题。如此看来，用空数组既简化编码，又解决了小内存碎片问题提高了性能。</li></ul><p>结构体最后使用0或1长度数组的原因：</p><blockquote><p>为了方便的管理内存缓冲区(其实就是分配一段连续的内存，减少内存的碎片化)，如果直接使用指针而不使用数组，那么，在分配内存缓冲区时，就必须分配结构体一次，然后再分配结构体内的指针一次，(而此时分配的内存已经与结构体的内存不连续了，所有要分别管理即申请和释放)而如果使用数组，那么只需要一次就可以全部分配出来，反过来，释放时也是一样，使用数组，一次释放。使用指针，得先释放结构体内的指针，再释放结构体，还不能颠倒顺序</p></blockquote><p>结构体中最后一个成员为[1]长度数组的用法：与长度为[0]数组的用法相同，改写为[1]是出于可移植性的考虑。有些编译器不支持[0]数组，可将其改成[]或[1].</p><h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span>&#123;</span><br><span class="line">    <span class="type">int</span> a;</span><br><span class="line">    <span class="type">char</span>* b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span>&#123;</span><br><span class="line">    <span class="type">int</span> a;</span><br><span class="line">    <span class="type">char</span> b[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了说明这个问题，我们定义一下几个结构体作为比较：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span>* b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">C</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> b[<span class="number">0</span>];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">D</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> b[<span class="number">1</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">E</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> b[<span class="number">10</span>];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(A) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(B) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(C) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(D) &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="built_in">sizeof</span>(E) &lt;&lt; endl;</span><br></pre></td></tr></table></figure><p>输出占用空间大小为</p><blockquote><p>4 8 4 8 16</p></blockquote><p>可以看到<code>struct A</code>大小为<code>int</code>大小 4字节，<code>struct B</code>由于包含了一个指针，在32位系统中，大小为 4字节，总共8字节。<code>strcut C</code>大小为4字节，明显<code>char[0]</code>没有分配内存。<code>struct D</code>大小由于内存对齐原因得到为8字节。<code>struct E</code>大小同样由于内存对齐原因得到为16字节。</p><p>由此可以看到长度为0的数组没有分配内存。<br>为了更详细的说明内存分配情况，我们查看一下每个的地址.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A a; B b; C c; D d; E e;</span><br><span class="line">cout &lt;&lt; &amp;a.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;b.b - (<span class="type">int</span>)&amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;b.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;c.b - (<span class="type">int</span>)&amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;c.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;d.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;d.b - (<span class="type">int</span>)&amp;d.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;d.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;e.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;e.b - (<span class="type">int</span>)&amp;e.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;e.b &lt;&lt; endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出为:</p><blockquote><p>00AFFB4C<br>00AFFB3C        4       00AFFB40<br>00AFFB30        4       00AFFB34<br>00AFFB20        4       00AFFB24<br>00AFFB08        4       00AFFB0C</p></blockquote><p>可以看到每个<code>b</code>都指向了同一位置,<code>int a</code>后面一位的地址.<br>为了更清楚的描述,在中间插入一个<code>char c</code>可以看到有:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">A</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">B</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line"><span class="type">char</span>* b;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">C</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line"><span class="type">char</span> b[<span class="number">0</span>];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">A a;</span><br><span class="line">B b;</span><br><span class="line">C c;</span><br><span class="line">cout &lt;&lt; &amp;a.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;b.b - (<span class="type">int</span>)&amp;b.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;b.b &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; &amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; (<span class="type">int</span>)&amp;c.b - (<span class="type">int</span>)&amp;c.a &lt;&lt; <span class="string">&#x27;\t&#x27;</span> &lt;&lt; &amp;c.b &lt;&lt; endl;</span><br></pre></td></tr></table></figure><p>输出为:</p><blockquote><p>012FF9E8<br>012FF9D4        8       012FF9DC<br>012FF9C4        5       012FF9C9</p></blockquote><p>很明显可以得到结论,<code>char b[0]</code>不分配内存,但是可以获得结构体的末尾地址.</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++构造函数</title>
      <link href="/cpp-%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0/"/>
      <url>/cpp-%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="调用拷贝构造函数的几种情况"><a href="#调用拷贝构造函数的几种情况" class="headerlink" title="调用拷贝构造函数的几种情况"></a>调用拷贝构造函数的几种情况</h2><blockquote><p>当类中成员有<strong>指针变量</strong>、类中有<strong>动态内存分配</strong>时常常需要用户自己定义拷贝构造函数。</p></blockquote><p>在什么情况下系统会调用拷贝构造函数：</p><blockquote><p>（1）用类的一个对象去初始化另一个对象时<br>（2）当函数的形参是类的对象时（也就是<strong>值传递</strong>时），如果是引用传递则不会调用<br>（3）当函数的<strong>返回值是类的对象或引用</strong>时</p></blockquote><p>代码示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="type">int</span> a;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">int</span> i):<span class="built_in">a</span>(i)</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">~<span class="built_in">Test</span>()</span><br><span class="line">&#123;</span><br><span class="line">cout&lt;&lt;a&lt;&lt;<span class="string">&#x27;\t&#x27;</span> &lt;&lt; <span class="string">&quot;析构函数&quot;</span>&lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(<span class="type">const</span> Test &amp;b)</span><br><span class="line">&#123;</span><br><span class="line">a = b.a;</span><br><span class="line">cout &lt;&lt; b.a &lt;&lt;<span class="string">&#x27;\t&#x27;</span>&lt;&lt; <span class="string">&quot;拷贝构造函数&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="function">Test <span class="title">a</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line"><span class="function">Test <span class="title">b</span><span class="params">(<span class="number">20</span>)</span></span>;</span><br><span class="line"><span class="built_in">Add</span>(a, b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="调用函数为值传递"><a href="#调用函数为值传递" class="headerlink" title="调用函数为值传递"></a>调用函数为值传递</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Test <span class="title">Add</span><span class="params">(Test a, Test b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">auto</span> res = <span class="built_in">Test</span>(a.a + b.a);</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出为</p><table><thead><tr><th>输出</th><th>解释</th></tr></thead><tbody><tr><td>10      构造函数</td><td></td></tr><tr><td>20      构造函数</td><td>a,b初始化</td></tr><tr><td>20      拷贝构造函数</td><td></td></tr><tr><td>10      拷贝构造函数</td><td>形参的拷贝构造，右到左</td></tr><tr><td>30      构造函数</td><td>Test(a.a + b.a)临时对象</td></tr><tr><td>30      拷贝构造函数</td><td>赋值给res</td></tr><tr><td>30      析构函数</td><td>Test(a.a + b.a)临时对象析构</td></tr><tr><td>10      析构函数</td><td></td></tr><tr><td>20      析构函数</td><td>形参析构</td></tr><tr><td>30      析构函数</td><td>返回值res的析构函数</td></tr><tr><td>20      析构函数</td><td></td></tr><tr><td>10      析构函数</td><td>a,b析构函数</td></tr></tbody></table><h4 id="参数为引用传递"><a href="#参数为引用传递" class="headerlink" title="参数为引用传递"></a>参数为引用传递</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Test <span class="title">Add</span><span class="params">(Test&amp; a, Test&amp; b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">auto</span> res = <span class="built_in">Test</span>(a.a + b.a);</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>输出</th><th>解释</th></tr></thead><tbody><tr><td>10      构造函数</td><td></td></tr><tr><td>20      构造函数</td><td>a,b 初始化</td></tr><tr><td>30      构造函数</td><td>Test(a.a + b.a)临时变量初始化</td></tr><tr><td>30      拷贝构造函数</td><td>res的复制构造（值传递）</td></tr><tr><td>30      析构函数</td><td>临时对象析构</td></tr><tr><td>30      析构函数</td><td>res析构</td></tr><tr><td>20      析构函数</td><td></td></tr><tr><td>10      析构函数</td><td>a,b析构</td></tr></tbody></table><h4 id="返回值为引用"><a href="#返回值为引用" class="headerlink" title="返回值为引用"></a>返回值为引用</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Test&amp; <span class="title">Add</span><span class="params">(Test&amp; a, Test&amp; b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">auto</span> res = <span class="built_in">Test</span>(a.a + b.a);</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>输出</th><th>解释</th></tr></thead><tbody><tr><td>10      构造函数</td><td></td></tr><tr><td>20      构造函数</td><td></td></tr><tr><td>30      构造函数</td><td>res 是 临时对象的引用，因此不会拷贝构造</td></tr><tr><td>30      析构函数</td><td></td></tr><tr><td>20      析构函数</td><td></td></tr><tr><td>10      析构函数</td><td></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>516.最长回文子序列</title>
      <link href="/Longest_Palindromic_Subsequence/"/>
      <url>/Longest_Palindromic_Subsequence/</url>
      
        <content type="html"><![CDATA[<p><strong>题目</strong></p><blockquote><p>给定字符串s，求其最长回文子序列（可以非连续）的长度</p></blockquote><hr><p><strong>DP</strong></p><blockquote><p>当已知一个序列是回文时，添加首尾元素后的序列存在两种情况，一种是首尾元素相等，则最长回文的长度加2，当首尾元素不相等，则最长回文序列为仅添加首元素时的最长回文与仅添加尾元素时的最长回文之间的最大值。我们可以用$dp[i][j]$表示$s[i…j]$中的最长回文序列，而状态转移方程则是 </p><ol><li>$i &gt; j，dp[i][j] &#x3D; 0；$ </li><li>$i &#x3D;&#x3D; j，dp[i][j] &#x3D; 1；$ </li><li>$i &lt; j且s[i] &#x3D;&#x3D; s[j]，dp[i][j] &#x3D; dp[i + 1][j - 1] + 2； $</li><li>$i &lt; j且s[i]！&#x3D; s[j]，dp[i][j] &#x3D; max(dp[i + 1][j]，dp[i][j - 1])；$</li></ol><p>从状态转移方程可以看出，计算$dp[i][j]$时需要用到$dp[i+1][j - 1]$和$dp[i + 1][j]$，所以对于$i$的遍历应该从尾部开始，最后返回$dp[0][s.length() - 1]$就行。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">longestPalindromeSubseq</span>(<span class="params">self, s</span>):</span><br><span class="line">memo = [[<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))]</span><br><span class="line"><span class="keyword">return</span> self.__shrink_recursion(s, <span class="number">0</span>, <span class="built_in">len</span>(s)-<span class="number">1</span>, memo)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__shrink_recursion</span>(<span class="params">self, s, left, right,  memo</span>):</span><br><span class="line"><span class="keyword">if</span> (memo[left][right] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line"><span class="keyword">return</span> memo[left][right]</span><br><span class="line"><span class="keyword">if</span> (left &gt; right):</span><br><span class="line">memo[left][right] = <span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">if</span> (left == right):</span><br><span class="line">memo[left][right] = <span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (s[left] == s[right]):</span><br><span class="line">memo[left][right] = self.__shrink(s, left+<span class="number">1</span>, right-<span class="number">1</span>, memo) + <span class="number">2</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">memo[left][right] = <span class="built_in">max</span>(self.__shrink_recursion(s, left+<span class="number">1</span>, right, memo),\</span><br><span class="line">                                    self.__shrink_recursion(s, left, right-<span class="number">1</span>, memo))</span><br><span class="line"><span class="comment"># print(memo)</span></span><br><span class="line"><span class="keyword">return</span> memo[left][right]</span><br><span class="line">   </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DP_iter</span>(<span class="params">self, s</span>):</span><br><span class="line">lens = <span class="built_in">len</span>(s)</span><br><span class="line">i = j = lens // <span class="number">2</span></span><br><span class="line">memo = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s))]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(lens-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">memo[i][i] = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, lens):</span><br><span class="line"><span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">memo[i][j] = memo[i+<span class="number">1</span>][j-<span class="number">1</span>]+<span class="number">2</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">memo[i][j] = <span class="built_in">max</span>(memo[i][j-<span class="number">1</span>], memo[i+<span class="number">1</span>][j])</span><br><span class="line"><span class="keyword">return</span> memo[<span class="number">0</span>][-<span class="number">1</span>]</span><br><span class="line">   </span><br></pre></td></tr></table></figure><p>时间复杂度是$O(n^2)$，空间复杂度是$O(n^2)$。</p><hr><p><strong>改进</strong></p><blockquote><p>上述的算法，从状态转移方程来看，计算$dp[i][x]$时，只用到了$dp[i][y]$和$dp[i + 1][z]$，即计算当前行时，只用到了当前行和下一行，因此可以对上一个算法进行改进，需要用两行空间存储就能完成计算。</p><p>用一个变量cur表示当前行的下标，cur的取值为0或1，1 - cur表示的就是另外一行，因此状态转移方程变成了： </p><ol><li>$i &gt; j，dp[cur][j] &#x3D; 0； $</li><li>$i &#x3D;&#x3D; j，dp[cur][j] &#x3D; 1； $</li><li>$i &lt; j且s[i] &#x3D;&#x3D; s[j]，dp[cur][j] &#x3D; dp[1 - cur][j - 1] + 2；$</li><li>$i &lt; j且s[i]！&#x3D; s[j]，dp[cur][j] &#x3D; max(dp[1 - cur][j]，dp[cur][j - 1])； $</li></ol><p>注意每次计算完一个$i$后需要更新$cur$的值，即$cur &#x3D; 1 - cur$。因为循环执行最后一次之后会多更新一次cur，所以返回的是$dp[1 - cur][s.length() - 1]$的值。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">longestPalindromeSubseq</span><span class="params">(string s)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> n = s.<span class="built_in">length</span>(), cur = <span class="number">0</span>;</span><br><span class="line">        vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">dp</span>(<span class="number">2</span>, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(n, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">            dp[cur][i] = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; n; j++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (s[i] == s[j]) &#123;</span><br><span class="line">                    dp[cur][j] = dp[<span class="number">1</span> - cur][j - <span class="number">1</span>] + <span class="number">2</span>;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[cur][j] = <span class="built_in">max</span>(dp[<span class="number">1</span> - cur][j], dp[cur][j - <span class="number">1</span>]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            cur = <span class="number">1</span> - cur;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">1</span> - cur][n - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>53.最大子序列和</title>
      <link href="/MaximumSubarray/"/>
      <url>/MaximumSubarray/</url>
      
        <content type="html"><![CDATA[<blockquote><p>对于给定序列，得到最大和的子序列</p><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;Input: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">&gt;Output: 6</span><br><span class="line">&gt;Explanation: [4,-1,2,1] has the largest sum = 6.</span><br></pre></td></tr></table></figure></blockquote><p><strong>brute force</strong></p><blockquote><p>遍历所有的可能答案，得到最大子序列和。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">nums</span>):</span><br><span class="line">    max_sum = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> L <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):<span class="comment"># 左边界</span></span><br><span class="line">        <span class="keyword">for</span> R <span class="keyword">in</span> <span class="built_in">range</span>(L,<span class="built_in">len</span>(nums)):<span class="comment"># 右边界</span></span><br><span class="line">            cur_sum = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L,R):</span><br><span class="line">                cur_sum+=nums[i]</span><br><span class="line">            <span class="keyword">if</span> cur_sum &gt; max_sum:</span><br><span class="line">                max_sum = cur_sum</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></table></figure><p>时间复杂度 $O(n^3)$</p><p><strong>改进版穷举</strong></p><blockquote><p>上面的方法，可以改进，去掉最内层的循环。以左边界为起点，记录连续的求和，只取最大的即可。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">nums</span>):</span><br><span class="line">    max_sum = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> L <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):<span class="comment"># 左边界</span></span><br><span class="line">        cur_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> R <span class="keyword">in</span> <span class="built_in">range</span>(L,<span class="built_in">len</span>(nums)):</span><br><span class="line">            cur_sum+=nums[R]</span><br><span class="line">            <span class="keyword">if</span> cur_sum &gt; max_sum:</span><br><span class="line">                max_sum = cur_sum</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></table></figure><p>此时时间复杂度 $O(n^2)$</p><p><strong>分治</strong></p><blockquote><p>这个问题可以递归求解，</p><p>在例子中，最大子序列的和只可能出现在3个地方：</p><ol><li>出现在输入数据的左半部分</li><li>出现在输入数据的右半部分</li><li>跨越输入数据的中部而位于左右两个部分</li></ol><p>前两种情况可以递归求解，第三种情况的最大和可以通过求出前半部分（包含前半部分的最后一个元素）的最大和以及后半部分（包括后半部分的第一个元素）的最大和，再将二者相加得到。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Devided_Conquer</span>(<span class="params">nums, left, right</span>):</span><br><span class="line">  <span class="keyword">if</span> left == right:</span><br><span class="line">    <span class="keyword">return</span> nums[left] <span class="comment">#if nums[left] &gt; 0 else 0</span></span><br><span class="line">  </span><br><span class="line">  center = (left+right) // <span class="number">2</span></span><br><span class="line">  max_left  = Devided_Conquer(nums, left, center)</span><br><span class="line">  max_right = Devided_Conquer(nums, center+<span class="number">1</span>, right)</span><br><span class="line">  </span><br><span class="line">  left_Sum = <span class="number">0</span></span><br><span class="line">  maxLeft_Sum = nums[center]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(center, left-<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">    left_Sum += nums[i]</span><br><span class="line">    <span class="keyword">if</span> left_Sum &gt; maxLeft_Sum:</span><br><span class="line">      maxLeft_Sum = left_Sum</span><br><span class="line">  </span><br><span class="line">  right_sum = <span class="number">0</span></span><br><span class="line">  max_right_sum = nums[center+<span class="number">1</span>] </span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(center+<span class="number">1</span>, right+<span class="number">1</span>):</span><br><span class="line">    right_sum += nums[i]</span><br><span class="line">    <span class="keyword">if</span> right_sum &gt; max_right_sum:</span><br><span class="line">      max_right_sum = right_sum</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">max</span>(max_left, max_right, maxLeft_Sum+max_right_sum)</span><br></pre></td></tr></table></figure><p>时间复杂度 $O (N\log N)$</p><p><strong>One-Pass</strong></p><blockquote><p>考虑</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">One_Pass</span>(<span class="params">nums</span>):</span><br><span class="line">    max_sum = nums[<span class="number">0</span>]</span><br><span class="line">    this_sum = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums[<span class="number">1</span>:]:</span><br><span class="line">        this_sum = <span class="built_in">max</span>(num, this_sum+num)</span><br><span class="line">        <span class="keyword">if</span> this_sum &gt; max_sum:</span><br><span class="line">            max_sum = this_sum</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>122.Best Time to Buy and Sell Stock II</title>
      <link href="/Best_Time_to_Buy_and_Sell_Stock_II/"/>
      <url>/Best_Time_to_Buy_and_Sell_Stock_II/</url>
      
        <content type="html"><![CDATA[<blockquote><p>与<a href="./Best_Time_to_Buy_and_Sell_Stock">121</a>不同的在于，121只能操作一次，而这个是可以操作任意次。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 7</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 3 (price = 5), profit = 5-1 = 4.Then buy on day 4 (price = 3) and sell on day 5 (price = 6), profit = 6-3 = 3.</span><br></pre></td></tr></table></figure></blockquote><hr><p><strong>brute force</strong></p><blockquote><p>暴力搜索，没什么好说的</p><p>当前位置 $i$ ，搜索其后所有的可能答案，取最大的</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">arr, start</span>):</span><br><span class="line">  <span class="keyword">if</span> start &gt;= <span class="built_in">len</span>(arr):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">  max_profit = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(arr)):</span><br><span class="line">    tmp_max_profit = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start+<span class="number">1</span>, <span class="built_in">len</span>(arr)):</span><br><span class="line">      <span class="keyword">if</span> arr[j] &gt; arr[i]:</span><br><span class="line">        profit = brute_force(arr, j+<span class="number">1</span>) + arr[j] - arr[i]</span><br><span class="line">        <span class="keyword">if</span> profit &gt; tmp_max_profit:</span><br><span class="line">          tmp_max_profit = profit</span><br><span class="line">    <span class="keyword">if</span> tmp_max_profit&gt;max_profit:</span><br><span class="line">      max_profit = tmp_max_profit</span><br><span class="line">  <span class="keyword">return</span> max_profit</span><br></pre></td></tr></table></figure><p>时间复杂度$O(n^n)$ ， 空间复杂度$O(n)$</p><hr><p> <strong>Peak Valley Approach</strong></p><blockquote><p>给定的<code>price</code>数组为$[7, 1, 5, 3, 6, 4]$. 绘制图片有（来自<a href="https://leetcode.com/media/original_images/122_maxprofit_1.PNG">leetcode</a>）</p><p><img src="https://leetcode.com/media/original_images/122_maxprofit_1.PNG"></p><p>显然有，最终的收益来自于所有的峰值减去谷值之和</p><p>$Total_Profit &#x3D; \sum_i(height(peak_i)-height(valley_i))$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Peak_Valley</span>(<span class="params">prices</span>):</span><br><span class="line">  valley = prices[<span class="number">0</span>]</span><br><span class="line">  peak  = prices[<span class="number">0</span>]</span><br><span class="line">  idx = <span class="number">0</span></span><br><span class="line">  max_profit = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(prices)-<span class="number">1</span>:</span><br><span class="line">    <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(prices)-<span class="number">1</span> <span class="keyword">and</span> prices[idx+<span class="number">1</span>] &lt;= prices[idx]:</span><br><span class="line">      idx+=<span class="number">1</span></span><br><span class="line">    vally = prices[idx]</span><br><span class="line">    <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(prices)-<span class="number">1</span> <span class="keyword">and</span> prices[idx+<span class="number">1</span>] &gt; prices[idx]:</span><br><span class="line">      idx+=<span class="number">1</span></span><br><span class="line">    peak = prices[idx]</span><br><span class="line">    max_profit += peak-vally</span><br><span class="line">  <span class="keyword">return</span> max_profit</span><br></pre></td></tr></table></figure><p>时间复杂度$O(n)$, 空间复杂度$O(1)$</p><hr><p><strong>Simple One Pass</strong></p><blockquote><p>跟上面略微不同的是，只要斜率是正的，就一直买入卖出就可以获得最大利润</p><p><img src="https://leetcode.com/media/original_images/122_maxprofit_2.PNG" alt="Profit Graph"></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">One_Pass</span>(<span class="params">prices</span>):</span><br><span class="line">  max_profit = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> idx, price <span class="keyword">in</span> <span class="built_in">enumerate</span>(prices):</span><br><span class="line">    <span class="keyword">if</span> idx &gt; <span class="number">0</span> <span class="keyword">and</span> price &gt; prices[idx-<span class="number">1</span>]:</span><br><span class="line">      max_profit += price-prices[idx-<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">return</span> max_profit</span><br><span class="line">        </span><br></pre></td></tr></table></figure><p>时间复杂度$O(n)$, 空间复杂度$O(1)$</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>70. climbing stairs</title>
      <link href="/ClimbingStairs/"/>
      <url>/ClimbingStairs/</url>
      
        <content type="html"><![CDATA[<p><strong>描述</strong></p><blockquote><p>n阶楼梯，每次一步或者两步，一共有多少种方法</p></blockquote><p>[Solutions](..&#x2F;prob_70_Climbing Stairs.py)</p><p><strong>brute_force</strong></p><blockquote><p>$f(n)&#x3D;f(n-1)+f(n-2)$</p><p>显然有，到第n阶楼梯有两种方法，从n-1过去，和n-2过去。即到n阶的方法等于这两种方法的和</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">brute_force</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span> <span class="keyword">or</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> brute_force(n-<span class="number">1</span>)+brute_force(n-<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>这种方法的时间复杂度为 $2^n$. 图片来自于<a href="https://leetcode.com/problems/climbing-stairs/solution/">leetcode</a><img src="https://leetcode.com/problems/climbing-stairs/Figures/70_Climbing_Stairs_rt.jpg" alt="Climbing_Stairs"></p><p><strong>带记忆的递归计算</strong></p><blockquote><p>在上面的计算中，显然有大量的重复计算，如果这个数值已经存下来了，就可以减小运算时间</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">memo = &#123;&#125;</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">recursion_memo</span>(<span class="params">n, memo</span>):</span><br><span class="line">    <span class="keyword">if</span> n==<span class="number">1</span> <span class="keyword">or</span> n ==<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">in</span> memo.keys():</span><br><span class="line">        <span class="keyword">return</span> memo[n]</span><br><span class="line">    value = recursion_memo(n-<span class="number">1</span>, memo) + recursion_memo(n-<span class="number">2</span>, memo)</span><br><span class="line">    memo.update(&#123;n:value&#125;)</span><br><span class="line">    <span class="keyword">return</span> memo[n]</span><br></pre></td></tr></table></figure><p><strong>动态规划</strong></p><blockquote><p>在暴力搜索里提到了，$f(n)&#x3D;f(n-1)+f(n-2)$</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dynamic</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span> <span class="keyword">or</span> n == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    result = [<span class="number">0</span>]*(n)</span><br><span class="line">    result[<span class="number">0</span>] =<span class="number">1</span></span><br><span class="line">    result[<span class="number">1</span>] =<span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n):</span><br><span class="line">        result[i] = result[i-<span class="number">1</span>]+result[i-<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> result[n-<span class="number">1</span>]</span><br><span class="line">        </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>121. Best Time to Buy and Sell Stock</title>
      <link href="/Best_Time_to_Buy_and_Sell_Stock%20/"/>
      <url>/Best_Time_to_Buy_and_Sell_Stock%20/</url>
      
        <content type="html"><![CDATA[<p>Say you have an array for which the $i^{th}$ element is the price of a given stock on day <em>i</em>.</p><p>If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit.</p><p>Note that you cannot sell a stock before you buy one.</p><p><strong>Example 1:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: [7,1,5,3,6,4]</span><br><span class="line">Output: 5</span><br><span class="line">Explanation: Buy on day 2 (price = 1) and sell on day 5 (price = 6), profit = 6-1 = 5.</span><br><span class="line">             Not 7-1 = 6, as selling price needs to be larger than buying price.</span><br></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [7,6,4,3,1]</span><br><span class="line">Output: 0</span><br><span class="line">Explanation: In this case, no transaction is done, i.e. max profit = 0.</span><br></pre></td></tr></table></figure><p>题目要求为，选择最佳的买入卖出时间，得到最大收益。</p><h2 id="暴力搜索"><a href="#暴力搜索" class="headerlink" title="暴力搜索"></a>暴力搜索</h2><p>复杂度$O(n^2)$ 确切一点是 $O(\frac{n(n+1)}{2})$</p><h2 id="one-pass"><a href="#one-pass" class="headerlink" title="one pass"></a>one pass</h2><p>对于$[7, 1, 5, 3, 6, 4] $</p><p><img src="https://leetcode.com/media/original_images/121_profit_graph.png" alt="Profit Graph"> </p><p>可以知道，我们感兴趣的是峰谷之间的差值。则我们只需要找到当前值与之前最小值的最大差值即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">maxProfit</span><span class="params">(<span class="type">int</span> prices[])</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">minprice</span> <span class="operator">=</span> Integer.MAX_VALUE;</span><br><span class="line">        <span class="type">int</span> <span class="variable">maxprofit</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; prices.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (prices[i] &lt; minprice)</span><br><span class="line">                minprice = prices[i];</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (prices[i] - minprice &gt; maxprofit)</span><br><span class="line">                maxprofit = prices[i] - minprice;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> maxprofit;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.longest substring without repeating</title>
      <link href="/%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/"/>
      <url>/%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/</url>
      
        <content type="html"><![CDATA[<p>Given a string, find the length of the <strong>longest substring</strong> without repeating characters.</p><p><strong>Examples:</strong></p><p>Given <code>&quot;abcabcbb&quot;</code>, the answer is <code>&quot;abc&quot;</code>, which the length is 3.</p><p>Given <code>&quot;bbbbb&quot;</code>, the answer is <code>&quot;b&quot;</code>, with the length of 1.</p><p>Given <code>&quot;pwwkew&quot;</code>, the answer is <code>&quot;wke&quot;</code>, with the length of 3. Note that the answer must be a <strong>substring</strong>, <code>&quot;pwke&quot;</code> is a <em>subsequence</em> and not a substring.</p><p>最长不重复子串</p><h2 id="Brute-Force"><a href="#Brute-Force" class="headerlink" title="Brute Force"></a>Brute Force</h2><p>有一个判断当前字符串为不重复的函数 <code>boolean allUnique(String substring)</code> .</p><p>然后两重循环求解，时间复杂度$O(n^3)$</p><h2 id="sliding-window"><a href="#sliding-window" class="headerlink" title="sliding window"></a>sliding window</h2><p>在暴力搜索中， <code>boolean allUnique(String substring)</code> 对于字串是从头开始搜索的。如果使用<code>set</code>的结构，可以将复杂度降到$O(n^2)$</p><h2 id="Sliding-Window-Optimized"><a href="#Sliding-Window-Optimized" class="headerlink" title="Sliding Window Optimized"></a>Sliding Window Optimized</h2><p>暴力搜索会产生很多不必要的操作，比如$s_{i,j}​$代表字符串 $i​$ 到 $j-1​$ 没有重复字串。则我们只需要判断第 $j​$ 个是否含于 $s_{i, j}​$ 即可。如果不包含，则 $s_{i,j}​$ 变为 $s_{i,j+1}​$ 。如果包含，则从包含的下标的下一位置开始(记录对应的位置)。</p><p>如下图所示：</p><img src="/%E6%9C%80%E9%95%BF%E4%B8%8D%E9%87%8D%E5%A4%8D%E5%AD%90%E4%B8%B2/1.jpg" class=""><p>下一坐标起始点即为2</p><p>这样就把时间复杂度降到了$O(n)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">self, s</span>):</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  :type s: str</span></span><br><span class="line"><span class="string">  :rtype: int</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line">  used = &#123;&#125;</span><br><span class="line">  max_length = start = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line">    <span class="keyword">if</span> c <span class="keyword">in</span> used <span class="keyword">and</span> start &lt;= used[c]:</span><br><span class="line">      start = used[c] + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:   </span><br><span class="line">      max_length = <span class="built_in">max</span>(max_length, i - start + <span class="number">1</span>)</span><br><span class="line">          </span><br><span class="line">    used[c] = i</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> max_length</span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>93. Restore IP Address</title>
      <link href="/Restore_IP_Addresses/"/>
      <url>/Restore_IP_Addresses/</url>
      
        <content type="html"><![CDATA[<p>Given a string containing only digits, restore it by returning all possible valid IP address combinations.</p><p><strong>Example:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;25525511135&quot;</span><br><span class="line">Output: [&quot;255.255.11.135&quot;, &quot;255.255.111.35&quot;]</span><br></pre></td></tr></table></figure><p>给定一个字符串，输出所有可能的IP地址</p><h2 id="四分法"><a href="#四分法" class="headerlink" title="四分法"></a>四分法</h2><blockquote><p>三个点将字符串分成四段，验证每一段是否是有效的。我们只要控制这三个分割点就行了，注意约束条件有两个，一个是一段字符串不超过3个字母，另一个是控制好每段字符串最远结束的位置，比如第一个字符串最多延伸到倒数第4个字母</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">restoreIpAddresses</span>(<span class="params">s, res</span>):</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i,i+<span class="number">4</span>):</span><br><span class="line">            <span class="keyword">if</span> j &gt;= <span class="built_in">len</span>(s):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(j, j+<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">if</span> k &gt;= <span class="built_in">len</span>(s):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                s1 = s[<span class="number">0</span>:i]</span><br><span class="line">                s2 = s[i:j]</span><br><span class="line">                s3 = s[j:k]</span><br><span class="line">                s4 = s[k:]</span><br><span class="line">                <span class="keyword">if</span> isValid(s1) <span class="keyword">and</span> isValid(s2) <span class="keyword">and</span> isVAlid(s3) <span class="keyword">and</span> isValid(s4):</span><br><span class="line">                    res.append(s1+<span class="string">&#x27;.&#x27;</span>+s2+<span class="string">&#x27;.&#x27;</span>+s3+<span class="string">&#x27;.&#x27;</span>+s4)</span><br><span class="line">     <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h2 id="递归求解"><a href="#递归求解" class="headerlink" title="递归求解"></a>递归求解</h2><blockquote><p>因为<code>ip</code>地址为4段，每段数值在<code>0-255</code>之间。从头开始，判断。只要满足每段在此范围内，即可进入下一段的操作。如果最后一段以后，长度为<code>0</code>。则此答案为正确答案。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">restoreIpAddresses</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :type s: str</span></span><br><span class="line"><span class="string">        :rtype: List[str]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># split the string into 4 part</span></span><br><span class="line">        res = []</span><br><span class="line">        self.getip(s, <span class="number">4</span>, <span class="string">&quot;&quot;</span>, res)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getip</span>(<span class="params">self, s, k, out, result</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        s ： input str</span></span><br><span class="line"><span class="string">        out : result str</span></span><br><span class="line"><span class="string">        result : all possible result</span></span><br><span class="line"><span class="string">        k      : k-th part</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(s) == <span class="number">0</span>:</span><br><span class="line">                result.append(out)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 0-3 每段最长3</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="comment"># len(s) &gt;= i 保证足够分的</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(s) &gt;= i <span class="keyword">and</span> self._isValid(s[<span class="number">0</span>:i]):</span><br><span class="line">                    <span class="comment"># k==1 即最后一段了</span></span><br><span class="line">                    <span class="keyword">if</span> k == <span class="number">1</span>:</span><br><span class="line">                        self.getip(s[i:], k-<span class="number">1</span>, out+s[<span class="number">0</span>:i], result)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        self.getip(s[i:], k-<span class="number">1</span>, out+s[<span class="number">0</span>:i]+<span class="string">&#x27;.&#x27;</span>, result)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_isValid</span>(<span class="params">self, s</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(s) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">len</span>(s) &gt; <span class="number">3</span> <span class="keyword">or</span> (<span class="built_in">len</span>(s)&gt;<span class="number">1</span> <span class="keyword">and</span> s[<span class="number">0</span>] ==<span class="string">&#x27;0&#x27;</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> &lt;= <span class="built_in">int</span>(s) &lt;= <span class="number">255</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>语义分割中的度量标准</title>
      <link href="/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%BA%A6%E9%87%8F/"/>
      <url>/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%BA%A6%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="pixel-accuracy-PA，像素精度"><a href="#pixel-accuracy-PA，像素精度" class="headerlink" title="pixel accuracy (PA，像素精度)"></a>pixel accuracy (PA，像素精度)</h1><p>标记正确的像素占总像素的比例</p><p>$$<br>PA&#x3D;\frac{\sum_{i&#x3D;1}^kp_{ii}}{\sum_{i&#x3D;0}^k\sum_{j&#x3D;0}^kp_{ij}}<br>$$</p><h1 id="mean-pixel-accuracy-MPA-均像素精度"><a href="#mean-pixel-accuracy-MPA-均像素精度" class="headerlink" title="mean pixel accuracy (MPA, 均像素精度)"></a>mean pixel accuracy (MPA, 均像素精度)</h1><p>计算每个类中被正确分类像素的比例，然后平均</p><p>$$<br>MPA&#x3D;\frac{1}{k+1}\sum_{i&#x3D;0}^{k}\frac{p_{ii}}{\sum_{j&#x3D;0}^kp_{ij}}<br>$$</p><h1 id="Mean-Intersection-over-Union-MIoU-均交并比"><a href="#Mean-Intersection-over-Union-MIoU-均交并比" class="headerlink" title="Mean Intersection over Union(MIoU, 均交并比)"></a>Mean Intersection over Union(MIoU, 均交并比)</h1><p>语义分割标准度量。计算两个集合的交集和并集之比。在semantic segmentation中，为真实值（ground truth）与预测值（predicted segmentation）的比值。这个比例变形为正真数（intersection）比上真正、假负、假正（并集）之和。在每个类上计算IoU，平均。</p><p>$$<br>MIoU&#x3D;\frac{1}{k+1}\sum_{i&#x3D;0}^k\frac{p_{ii}}{\sum_{j&#x3D;0}^k p_{ij}+\sum_{j&#x3D;0}^kp_{ji}-p_{ii}}<br>$$</p><img src="/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%BA%A6%E9%87%8F/1.jpg" class="" title="miou"><h1 id="Frequency-Weight-Intersection-over-Union-FWIoU-频权交并比"><a href="#Frequency-Weight-Intersection-over-Union-FWIoU-频权交并比" class="headerlink" title="Frequency Weight Intersection over Union(FWIoU, 频权交并比)"></a>Frequency Weight Intersection over Union(FWIoU, 频权交并比)</h1><p>MIoU的提升。根据每个类出现的频率设置权重</p><p>$$<br>FWIoU&#x3D;\frac{1}{\sum_{i&#x3D;0}^k\sum_{j&#x3D;0}^kp_{ij}}\sum_{i&#x3D;0}^k\frac{p_{ii}}{\sum_{j&#x3D;0}^k\sum_{j&#x3D;0}^kp_{ji}-p_{ii}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算label_true和label_pred对应相同的就在矩阵中对应坐标加1。a和b保存着各个像素的分的类别</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_fast_hist</span>(<span class="params">label_true, label_pred, n_class</span>):</span><br><span class="line">    <span class="comment">#过滤掉多余的分类</span></span><br><span class="line">    mask = (label_true &gt;= <span class="number">0</span>) &amp; (label_true &lt; n_class)</span><br><span class="line">    <span class="comment">#bincount用于统计在范围内出现的个数，即直方图，如果不够n^2个，</span></span><br><span class="line">    <span class="comment">#那就填充到n^2，这样可以reshpe为n*n的矩阵，正好表示分割图和正确标记图在相同</span></span><br><span class="line">    <span class="comment">#类别上像素出现的个数</span></span><br><span class="line">    hist = np.bincount(</span><br><span class="line">        n_class * label_true[mask].astype(<span class="built_in">int</span>) +</span><br><span class="line">        label_pred[mask], minlength=n_class ** <span class="number">2</span>).reshape(n_class, n_class)</span><br><span class="line">    <span class="keyword">return</span> hist</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">label_accuracy_score</span>(<span class="params">label_trues, label_preds, n_class</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns accuracy score evaluation result.</span></span><br><span class="line"><span class="string">      - overall accuracy</span></span><br><span class="line"><span class="string">      - mean accuracy</span></span><br><span class="line"><span class="string">      - mean IU</span></span><br><span class="line"><span class="string">      - fwavacc</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    hist = np.zeros((n_class, n_class))</span><br><span class="line">    <span class="keyword">for</span> lt, lp <span class="keyword">in</span> <span class="built_in">zip</span>(label_trues, label_preds):</span><br><span class="line">        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)</span><br><span class="line">    acc = np.diag(hist).<span class="built_in">sum</span>() / hist.<span class="built_in">sum</span>()</span><br><span class="line">    acc_cls = np.diag(hist) / hist.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">    acc_cls = np.nanmean(acc_cls)</span><br><span class="line">    iu = np.diag(hist) / (hist.<span class="built_in">sum</span>(axis=<span class="number">1</span>) + hist.<span class="built_in">sum</span>(axis=<span class="number">0</span>) - np.diag(hist))</span><br><span class="line">    mean_iu = np.nanmean(iu)</span><br><span class="line">    freq = hist.<span class="built_in">sum</span>(axis=<span class="number">1</span>) / hist.<span class="built_in">sum</span>()</span><br><span class="line">    fwavacc = (freq[freq &gt; <span class="number">0</span>] * iu[freq &gt; <span class="number">0</span>]).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> acc, acc_cls, mean_iu, fwavacc</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
